{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Victor Andres Aguirre Fernandez \u00b6 These are my notes, what I\u2019m learning to improve my Daily Job, a new Hobby or just something that I found interesting. I store all notes about the different categories, which are constantly changing, the idea is to have easy access to knowledge and use as a quick personal reference. NOTE: I use different resources and not all the content is mine, there are different authors and the credits/attributions are for them, in any moment I want to say the content is totally created by me, in some cases I use the same graphic the original Author use, just because they give a clear picture or useful information. PS: My native language is not English, so if someone other than me sees this notes, please be patient, this is personal notes, not a blog, thus, some topics are not finished yet (because I\u2019m learning it). I\u2019m using MkDocs to generate this \u201cdocumentation\u201d more information visit mkdocs.org .","title":"Home"},{"location":"index.html#victor_andres_aguirre_fernandez","text":"These are my notes, what I\u2019m learning to improve my Daily Job, a new Hobby or just something that I found interesting. I store all notes about the different categories, which are constantly changing, the idea is to have easy access to knowledge and use as a quick personal reference. NOTE: I use different resources and not all the content is mine, there are different authors and the credits/attributions are for them, in any moment I want to say the content is totally created by me, in some cases I use the same graphic the original Author use, just because they give a clear picture or useful information. PS: My native language is not English, so if someone other than me sees this notes, please be patient, this is personal notes, not a blog, thus, some topics are not finished yet (because I\u2019m learning it). I\u2019m using MkDocs to generate this \u201cdocumentation\u201d more information visit mkdocs.org .","title":"Victor Andres Aguirre Fernandez"},{"location":"Helpers.html","text":"Installing mkdocs \u00b6 Install using pip: 1 pip install mkdocs and to check the installation 1 2 $ mkdocs --version mkdocs, version 0.15.3 Installing Material for mkdocs \u00b6 Install using pip: 1 pip install mkdocs-material and int the project\u2019s mkdocs.yml we need to add: 1 2 theme: name: 'material' Useful MkDocs Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Useful commands \u00b6 ![Name of the image](/images/nameImage.png) - to add an image [text](URL) to add a link to a text Deploy the MKDocs \u00b6 The project pages site will be deploy in other branch called \u2018gh-deploy\u2019, run the following command: 1 mkdocs gh-deploy That\u2019s it! Behind the scenes, MkDocs will build your docs and use the ghp-import tool to commit them to the gh-pages branch and push the gh-pages branch to GitHub. Math \u00b6 To add inline math you can use $ and close it $ , like $...$ To add a block of math we use $$$ and close with $$$ In case that you need to add a white space inside this blocks you will need to escape it, example training\\ batch extra.css \u00b6 This is a file to customize the theme without modify the original files, I modify the following: 1. Center the image \u00b6 To center the images first, I added the markdown extension called attr_list , later create the following modification on the extra.css 1 2 3 4 5 6 7 8 9 10 /* /////////////////////// // To center images//// /////////////////////// */ . center { display : block ; margin : 0 auto ; } now to center the image I just need to add \u201c{: .center}\u201d at the end of the statement, for example: ![Name of the image](/images/nameImage.png).{: .center} 2. Modification to the dark theme \u00b6 TOC visited links \u00b6 I made a modification on the extra.css to overwrite dark_theme.css 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 /* ///////////////////////////// /// visited links on toc //// //////////////////////////// Modification by: Victor Fernandez Description: modification of the visited links on TOC to solve contrast issue */ . md-nav__link : visited { color : #808080 ; } . md-nav__link [ data-md-state = blur ] { color : #808080 ; } This modification wants to change the color of the visited links in the TOC, by default this visited link ( or permalinks) are black, this black color against black background present low contrast, thus it is difficult to read, therefore the modification. Blockquote \u00b6 block quote has the same problem with the contrast, like the visited link in toc, thus these changes in the extra.css to overwrite dark_theme.css file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /* //////////////////// /// Blockquote///// /////////////////// Modification by: Victor Fernandez Description: modification of the blockquote to solve contrast issue */ . md-typeset blockquote { color : #808080 ; border-left-width : 0.2 rem ; border-left-style : solid ; border-left-color : #808080 ; } Modifications to codehilite.css (extra.css) \u00b6 In order to change how the inline code ( this in line code ) and solve the contrast problems ( before was black) I made a change in extra.css that overwrite the original codehilite.css code, as follow: 1 2 3 4 5 6 7 8 9 10 /* ///////////////// // Inline Code // ///////////////// */ . md-typeset code { color : #ae81ff !important ; background-color : #313131 !important ; box-shadow : 0.29412 em 0 0 rgba ( 0 , 0 , 0 , .07 ), -0.29412 em 0 0 rgba ( 0 , 0 , 0 , .07 ); } Modification to footnote strings color \u00b6 To change the color of the footnotes i made a chang ein the css 1 2 3 . md-typeset div . footnote { color : #808080 ; } \u201cTo every man upon this earth, death cometh soon or late, And how can man die better, Than facing fearful odds, for the ashes of his fathers, And the temples of his gods.\u201d","title":"Helpers"},{"location":"Helpers.html#installing_mkdocs","text":"Install using pip: 1 pip install mkdocs and to check the installation 1 2 $ mkdocs --version mkdocs, version 0.15.3","title":"Installing mkdocs"},{"location":"Helpers.html#installing_material_for_mkdocs","text":"Install using pip: 1 pip install mkdocs-material and int the project\u2019s mkdocs.yml we need to add: 1 2 theme: name: 'material'","title":"Installing Material for mkdocs"},{"location":"Helpers.html#useful_mkdocs_commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Useful MkDocs Commands"},{"location":"Helpers.html#useful_commands","text":"![Name of the image](/images/nameImage.png) - to add an image [text](URL) to add a link to a text","title":"Useful commands"},{"location":"Helpers.html#deploy_the_mkdocs","text":"The project pages site will be deploy in other branch called \u2018gh-deploy\u2019, run the following command: 1 mkdocs gh-deploy That\u2019s it! Behind the scenes, MkDocs will build your docs and use the ghp-import tool to commit them to the gh-pages branch and push the gh-pages branch to GitHub.","title":"Deploy the MKDocs"},{"location":"Helpers.html#math","text":"To add inline math you can use $ and close it $ , like $...$ To add a block of math we use $$$ and close with $$$ In case that you need to add a white space inside this blocks you will need to escape it, example training\\ batch","title":"Math"},{"location":"Helpers.html#extracss","text":"This is a file to customize the theme without modify the original files, I modify the following:","title":"extra.css"},{"location":"Helpers.html#1_center_the_image","text":"To center the images first, I added the markdown extension called attr_list , later create the following modification on the extra.css 1 2 3 4 5 6 7 8 9 10 /* /////////////////////// // To center images//// /////////////////////// */ . center { display : block ; margin : 0 auto ; } now to center the image I just need to add \u201c{: .center}\u201d at the end of the statement, for example: ![Name of the image](/images/nameImage.png).{: .center}","title":"1. Center the image"},{"location":"Helpers.html#2_modification_to_the_dark_theme","text":"","title":"2. Modification to the dark theme"},{"location":"Helpers.html#toc_visited_links","text":"I made a modification on the extra.css to overwrite dark_theme.css 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 /* ///////////////////////////// /// visited links on toc //// //////////////////////////// Modification by: Victor Fernandez Description: modification of the visited links on TOC to solve contrast issue */ . md-nav__link : visited { color : #808080 ; } . md-nav__link [ data-md-state = blur ] { color : #808080 ; } This modification wants to change the color of the visited links in the TOC, by default this visited link ( or permalinks) are black, this black color against black background present low contrast, thus it is difficult to read, therefore the modification.","title":"TOC visited links"},{"location":"Helpers.html#blockquote","text":"block quote has the same problem with the contrast, like the visited link in toc, thus these changes in the extra.css to overwrite dark_theme.css file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /* //////////////////// /// Blockquote///// /////////////////// Modification by: Victor Fernandez Description: modification of the blockquote to solve contrast issue */ . md-typeset blockquote { color : #808080 ; border-left-width : 0.2 rem ; border-left-style : solid ; border-left-color : #808080 ; }","title":"Blockquote"},{"location":"Helpers.html#modifications_to_codehilitecss_extracss","text":"In order to change how the inline code ( this in line code ) and solve the contrast problems ( before was black) I made a change in extra.css that overwrite the original codehilite.css code, as follow: 1 2 3 4 5 6 7 8 9 10 /* ///////////////// // Inline Code // ///////////////// */ . md-typeset code { color : #ae81ff !important ; background-color : #313131 !important ; box-shadow : 0.29412 em 0 0 rgba ( 0 , 0 , 0 , .07 ), -0.29412 em 0 0 rgba ( 0 , 0 , 0 , .07 ); }","title":"Modifications to codehilite.css (extra.css)"},{"location":"Helpers.html#modification_to_footnote_strings_color","text":"To change the color of the footnotes i made a chang ein the css 1 2 3 . md-typeset div . footnote { color : #808080 ; } \u201cTo every man upon this earth, death cometh soon or late, And how can man die better, Than facing fearful odds, for the ashes of his fathers, And the temples of his gods.\u201d","title":"Modification to footnote strings color"},{"location":"about.html","text":"Leader and professionally focused Product development manager. Working with local and international development teams for over 4 years. Experience working as: Field Application Engineer. Product development manager. Automation test developer. Project manager in Web and mobile developer. Experienced Integrating test automation with Jenkins (Continuous integration tools), understanding of software development life cycle and agile methodologies (especially Scrum). I constantly study new technologies and innovative concepts to its rapid and efficient Implementation, I have the ability to perform effectively and efficiently in a team and individually, as well as adapt to international teams.","title":"About"},{"location":"3D Printing/Concepts.html","text":"Here some concepts related with 3D printing, this can be concepts from a specific board, firmware, etc The idea is have a place to refresh memory when reading other articles. Sensorless Homing \u00b6 Sensorless homing allows to home an axis without the need for a physical limit switch. Instead, the carriage on the axis is moved into the mechanical limit making the stepper motor lose steps. Then the firmware will trigger stop for the axis. SKR mini E3 v1.2 for this specific one we will need to use cables jumpers","title":"Concepts"},{"location":"3D Printing/Concepts.html#sensorless_homing","text":"Sensorless homing allows to home an axis without the need for a physical limit switch. Instead, the carriage on the axis is moved into the mechanical limit making the stepper motor lose steps. Then the firmware will trigger stop for the axis. SKR mini E3 v1.2 for this specific one we will need to use cables jumpers","title":"Sensorless Homing"},{"location":"3D Printing/about design.html","text":"This are some notes about design, mostly take it from the Sketch class in instructable Drawing 2D \u00b6 It is a good idea start with 2D and separate the object in \u201corthographic\u201d views. this will help to understand the proportions , scale and relationship that are difficult to see in 3D. For this we can start by drawing the Construction lines, sort of a cage around the object. Draw the lines that define the shape of the object, just the general shape, start but the horizontal or vertical lines and later make the curve lines. Later make construction lines for the other details This extra line can help to center the details and see what is the position of this extra details in the other faces Finally we can proceed to draw the upper and lower parts, using the construction lines to keep the size consistent. Using different line weight \u00b6 In order to give more details about the object we can use different line weights This is a direct copy of the information in this class, this describe where each line weight its been use 1. Extra Light (pencil or light weight pen): This weight is used for aspects that should either be secondary to the overall form of the object or almost disappear entirely. It\u2019s good for construction lines, surface features like textures or patterns, and seams between parts that don\u2019t have much depth (like the edge where a fender touches a hood on a car, for example). 2. Light (+/- 0.3mm): This weight is most often used for contour lines (lines that follow a surface to express its 3D shape) and planar corners (edges between two planes in which you can see both planes in the view). 3. Medium (+/- 0.5mm): This weight is generally used for spatial edges (the edges of surfaces where only one surface is seen in the view). 4. Heavy (0.7mm+): Heavy lines are reserved for the perimeter edges of an object and for the cut edges of a cross-section drawing. Drawing 3D \u00b6 Once we have clear the relationship of the different size or faces of the object, we can move to 3D in order to understand the object, after all we see in 3D, now, in this case we are no going to use perspective, we are going to use something call Isometric . We start in a similar way than 2D, we start making a cage for the object, we can start drawing 3 parallel lines, notice that line 1 and 2 from left to right represent how thick is the object, this can be different to different objects. Now we can draw the last vertical line, this will be the back each of the object, and will be (most of the time) be aware that the distance between this last line and line 3 should be same line 1 and 2, since the object has the same thickness. Now, we will use the angles of the bottom lines to emphasis the side, explained in other way will be, if the viewer is at the lower angle the focus will be in the side of the cube. if the viewer is looking from a higher place the focus will be the top. Angles that are more acute to the horizon will show less of the top of the object, emphasizing the sides. Angles that are more oblique to the horizon will show more of the top. Both faces the lower and the upper will be parallel to each other Similar what we did with the 2D we can continue making construction lines for the other features of the objects, for example in this case we need some lines for the circles on top of the object. In order to draw the circles we need to have an idea of how the look like at different angles According to the instructor in the class A Rule of thumb will be: A circle will hit the construction lines at the same points in 3D as they will in 2D. The curve along the obtuse corners of the box get longer and flatter. The curve along the acute corners of the box shorter and deeper. here will be a big jump in the instruction, but basically we need to follow similar instruction that we did until now, just that this time we will focus in the details in each step we increase the amount of details finally we use the different line weights Extra Light: surface details. Light: corner edges. Medium: edges with background surfaces. Heavy: perimeter outline. Shadows and Light \u00b6 Taken directly and literally from the class There are 5 basic parts of a shadow. 1. Cast Shadow: Just like your own shadow on the sidewalk on a sunny day, a cast shadow is a -distorted- projection of the shape of the object. The length of the shadow and its direction depend on the direction of the light. On the Copic Marker scale, this would be N8 . 2. Core Shadow: This is the shaded part on the opposite side of an object from the direction of the light. This part is especially useful in defining geometry. On the Copic Marker scale, this would be N6 . 3. Reflected Highlight: This is ambient light bounced off of the ground to brighten part of an object that would otherwise be in core shadow. It\u2019s often okay to leave this part out if you want to make a quick drawing. On the Copic Marker scale, this would be N4 . 4. Mid Tone: This is the part of the shadow that transitions from the core shadow to the highlight on a curved surface. Think of it as a blurred edge of the core shadow / highlight. The mid tone also shows up on flat faces that aren\u2019t directly in the line of the light direction. On the Copic Marker scale, this would be N2 . 5. Highlight: This is the part of the object that reflects the most light. The highlight is directly in line with the direction of the light. This will remain white. In the class the had a video where the instructor explain how to make the different shadows The suggestion is start with the cast shadows, since this will gave us the sense of what surface will have the level of shadows. Choose a light direction: imagine the rays of light are parallel, like sunlight, and project the top of the object onto the floor. The lines this projection follows should be parallel to each other. Notice that the Shadow is parallel to the top edge of the object","title":"About design"},{"location":"3D Printing/about design.html#drawing_2d","text":"It is a good idea start with 2D and separate the object in \u201corthographic\u201d views. this will help to understand the proportions , scale and relationship that are difficult to see in 3D. For this we can start by drawing the Construction lines, sort of a cage around the object. Draw the lines that define the shape of the object, just the general shape, start but the horizontal or vertical lines and later make the curve lines. Later make construction lines for the other details This extra line can help to center the details and see what is the position of this extra details in the other faces Finally we can proceed to draw the upper and lower parts, using the construction lines to keep the size consistent.","title":"Drawing 2D"},{"location":"3D Printing/about design.html#using_different_line_weight","text":"In order to give more details about the object we can use different line weights This is a direct copy of the information in this class, this describe where each line weight its been use 1. Extra Light (pencil or light weight pen): This weight is used for aspects that should either be secondary to the overall form of the object or almost disappear entirely. It\u2019s good for construction lines, surface features like textures or patterns, and seams between parts that don\u2019t have much depth (like the edge where a fender touches a hood on a car, for example). 2. Light (+/- 0.3mm): This weight is most often used for contour lines (lines that follow a surface to express its 3D shape) and planar corners (edges between two planes in which you can see both planes in the view). 3. Medium (+/- 0.5mm): This weight is generally used for spatial edges (the edges of surfaces where only one surface is seen in the view). 4. Heavy (0.7mm+): Heavy lines are reserved for the perimeter edges of an object and for the cut edges of a cross-section drawing.","title":"Using different line weight"},{"location":"3D Printing/about design.html#drawing_3d","text":"Once we have clear the relationship of the different size or faces of the object, we can move to 3D in order to understand the object, after all we see in 3D, now, in this case we are no going to use perspective, we are going to use something call Isometric . We start in a similar way than 2D, we start making a cage for the object, we can start drawing 3 parallel lines, notice that line 1 and 2 from left to right represent how thick is the object, this can be different to different objects. Now we can draw the last vertical line, this will be the back each of the object, and will be (most of the time) be aware that the distance between this last line and line 3 should be same line 1 and 2, since the object has the same thickness. Now, we will use the angles of the bottom lines to emphasis the side, explained in other way will be, if the viewer is at the lower angle the focus will be in the side of the cube. if the viewer is looking from a higher place the focus will be the top. Angles that are more acute to the horizon will show less of the top of the object, emphasizing the sides. Angles that are more oblique to the horizon will show more of the top. Both faces the lower and the upper will be parallel to each other Similar what we did with the 2D we can continue making construction lines for the other features of the objects, for example in this case we need some lines for the circles on top of the object. In order to draw the circles we need to have an idea of how the look like at different angles According to the instructor in the class A Rule of thumb will be: A circle will hit the construction lines at the same points in 3D as they will in 2D. The curve along the obtuse corners of the box get longer and flatter. The curve along the acute corners of the box shorter and deeper. here will be a big jump in the instruction, but basically we need to follow similar instruction that we did until now, just that this time we will focus in the details in each step we increase the amount of details finally we use the different line weights Extra Light: surface details. Light: corner edges. Medium: edges with background surfaces. Heavy: perimeter outline.","title":"Drawing 3D"},{"location":"3D Printing/about design.html#shadows_and_light","text":"Taken directly and literally from the class There are 5 basic parts of a shadow. 1. Cast Shadow: Just like your own shadow on the sidewalk on a sunny day, a cast shadow is a -distorted- projection of the shape of the object. The length of the shadow and its direction depend on the direction of the light. On the Copic Marker scale, this would be N8 . 2. Core Shadow: This is the shaded part on the opposite side of an object from the direction of the light. This part is especially useful in defining geometry. On the Copic Marker scale, this would be N6 . 3. Reflected Highlight: This is ambient light bounced off of the ground to brighten part of an object that would otherwise be in core shadow. It\u2019s often okay to leave this part out if you want to make a quick drawing. On the Copic Marker scale, this would be N4 . 4. Mid Tone: This is the part of the shadow that transitions from the core shadow to the highlight on a curved surface. Think of it as a blurred edge of the core shadow / highlight. The mid tone also shows up on flat faces that aren\u2019t directly in the line of the light direction. On the Copic Marker scale, this would be N2 . 5. Highlight: This is the part of the object that reflects the most light. The highlight is directly in line with the direction of the light. This will remain white. In the class the had a video where the instructor explain how to make the different shadows The suggestion is start with the cast shadows, since this will gave us the sense of what surface will have the level of shadows. Choose a light direction: imagine the rays of light are parallel, like sunlight, and project the top of the object onto the floor. The lines this projection follows should be parallel to each other. Notice that the Shadow is parallel to the top edge of the object","title":"Shadows and Light"},{"location":"3D Printing/gcode.html","text":"What are G-code Commands? \u00b6 G-code stands for \u201cGeometric code\u201d , it is a rudimentary programing language, a numerical control programing language, that lacks of data structure such as variable, control blocks as conditionals and loops, its main function is to provide instruction to the machine of how to move in the 3 geometrical dimension, how ever it can send non-geometric instruction, like heat the bed, extrude specific amount of material at a specific extrusion rate, etc. How to read G-code Commands? \u00b6 A typical G-code line will look like: 1 G1 X-10 Y4.5 Z0.5 F30000.0 E 0.0377 This line tell the machine to move in a straight line toward the coordinates X-10 , Y4.5 , Z0.5 at a feed rate of 3000.0 and extrude 0.0377mm of material while moving. But how to read it, well each line start with a specific code, in this case the code is G1 , which it means \u201cmove in a straight line in a controlled fashion\u201d. 1 G1 X-10 Y4.5 Z0.5 F30000.0 E 0.0377 The values after the code are the arguments, it start with a English letter and then the value: XYZ Are values for the Cartesian coordinates F is the Feed rate E is the Extrusion So the code: 1 G1 X2 Y4 Z0 F30000.0 E 0.02 Reads \u201c Move towards X=2,Y=4, Z=0 in a straight line in a control fashion at a feed rate 3000.00 while extruding 0.02mm of material \u201d. SOmething that help use to understand the code faster, is the fact that all g-code that start with G is code related with geometric commands. But a machine do more that geometric movements, there fore we have another type of commands, Non-geometroc and those command start with M. Each English letter in a G-code has an specific meaning here a reference table from reprap , for example G for Geometric commands, M for Non-geometric, X means x x coordinate, Y y y coordinate, Z z z coordinate, F means feed rate, E Extruder, etc G1 command means \u201cmove the nozzle in a controlled fashion in a straight line\u201d Following will be a description of the most common command, i will use the same graphic use in the document g-code tutorial 3D printer gcode-commands The most common G-code \u00b6 G00 \u201cRapid Motion\u201d \u00b6 This command ask the printer to move to the maximum speed possible from the current position to the coordinate given. With this command no material is extrude, and it is normally use to move the nozzle before start and after finished the print 1 G0 X7 Y18 G01 \u201cControlled motion\u201d \u00b6 This Command will move the nozzle or printed head to a specific speed from the current position to the coordinated specified, the speed is specify by the Feed rate F , the amount of material extruded will be define by the number after E . 1 G01 X7 Y18 F500 E0.02 G17/G18/19 \u201cset planes\u201d \u00b6 This is to set the plan where the nozzle should move, G17 will be X-Y, G18 Z-X and G19 Y-Z . G20/G21 \u201cSet units\u201d \u00b6 With G20 you set the units to mm and G21 to Inches G28 \u201cHoming\u201d \u00b6 Send the printing head to the home position G90 \u201cabsolute mode\u201d \u00b6 This mode tell the printer to move to the absolute coordinate. 1 2 G90 G0 X10 The machine will move the print head to the coordinate X=10 G91 \u201cRelative mode\u201d \u00b6 Tells the machine to interpreted the coordinate as relative, if the machine is currently at X=10 and we use this command 1 2 G91 G0 x10 the print head will move 10 units in the X direction, finishing at X=20. G2 and G3 \u201cclockwise or anticlockwise motion\u201d \u00b6 This commands tell the printer to move either clockwise G2 or andticlockwise G3 from the current point to the coordinate given, the center of the rotation is given by the parameter I and J, I denotate the X offset and J the Y offset 1 2 3 4 5 G21 G90 G17 G0 X6 Y18 G2 X18 Y6 I0 J-12 1 2 3 4 5 G21 G90 G17 G0 X-5 Y25 G3 X-25 Y5 I0 J-20 Comments ; \u00b6 Like any other programming languages, we can make comments in G-code, this is done using ; after the command, example 1 G0 X-25 Y5 ; rapid movement to X=-25 and Y=5 Example of the beginning of a G-code \u00b6 1 2 3 4 5 6 7 8 9 G90 M82 M106 S0 M140 S100 M190 S100 G90 Set the coordinate as absolute M82 Put the E axis into absolute mode independent of the other axes. M106 S0 turn fan on and set speed to 0 M140 S100 set Bed temperature as 100 degrees M190 S100 Wait until the temperature is 100 Now the printing phase 1 2 3 4 5 6 7 G1 X108.587 Y111.559 F525 ; controlled motion in X-Y plane G1 X108.553 Y111.504 F525 ; controlled motion in X-Y plane ... G1 Z0.345 F500 ; change layer There is nothing to explain here that is not already explained, there are some movements in X-Y and others in Z. After the print is complete we pass to the Reset printing phase 1 2 3 4 5 6 7 G28 ; bring the nozzle to home M104 S0 ; turn off heaters M140 S0 ; turn off bed M84 ; disable motors","title":"G-code"},{"location":"3D Printing/gcode.html#what_are_g-code_commands","text":"G-code stands for \u201cGeometric code\u201d , it is a rudimentary programing language, a numerical control programing language, that lacks of data structure such as variable, control blocks as conditionals and loops, its main function is to provide instruction to the machine of how to move in the 3 geometrical dimension, how ever it can send non-geometric instruction, like heat the bed, extrude specific amount of material at a specific extrusion rate, etc.","title":"What are G-code Commands?"},{"location":"3D Printing/gcode.html#how_to_read_g-code_commands","text":"A typical G-code line will look like: 1 G1 X-10 Y4.5 Z0.5 F30000.0 E 0.0377 This line tell the machine to move in a straight line toward the coordinates X-10 , Y4.5 , Z0.5 at a feed rate of 3000.0 and extrude 0.0377mm of material while moving. But how to read it, well each line start with a specific code, in this case the code is G1 , which it means \u201cmove in a straight line in a controlled fashion\u201d. 1 G1 X-10 Y4.5 Z0.5 F30000.0 E 0.0377 The values after the code are the arguments, it start with a English letter and then the value: XYZ Are values for the Cartesian coordinates F is the Feed rate E is the Extrusion So the code: 1 G1 X2 Y4 Z0 F30000.0 E 0.02 Reads \u201c Move towards X=2,Y=4, Z=0 in a straight line in a control fashion at a feed rate 3000.00 while extruding 0.02mm of material \u201d. SOmething that help use to understand the code faster, is the fact that all g-code that start with G is code related with geometric commands. But a machine do more that geometric movements, there fore we have another type of commands, Non-geometroc and those command start with M. Each English letter in a G-code has an specific meaning here a reference table from reprap , for example G for Geometric commands, M for Non-geometric, X means x x coordinate, Y y y coordinate, Z z z coordinate, F means feed rate, E Extruder, etc G1 command means \u201cmove the nozzle in a controlled fashion in a straight line\u201d Following will be a description of the most common command, i will use the same graphic use in the document g-code tutorial 3D printer gcode-commands","title":"How to read G-code Commands?"},{"location":"3D Printing/gcode.html#the_most_common_g-code","text":"","title":"The most common G-code"},{"location":"3D Printing/gcode.html#g00_rapid_motion","text":"This command ask the printer to move to the maximum speed possible from the current position to the coordinate given. With this command no material is extrude, and it is normally use to move the nozzle before start and after finished the print 1 G0 X7 Y18","title":"G00 \"Rapid Motion\""},{"location":"3D Printing/gcode.html#g01_controlled_motion","text":"This Command will move the nozzle or printed head to a specific speed from the current position to the coordinated specified, the speed is specify by the Feed rate F , the amount of material extruded will be define by the number after E . 1 G01 X7 Y18 F500 E0.02","title":"G01  \"Controlled motion\""},{"location":"3D Printing/gcode.html#g17g1819_set_planes","text":"This is to set the plan where the nozzle should move, G17 will be X-Y, G18 Z-X and G19 Y-Z .","title":"G17/G18/19 \"set planes\""},{"location":"3D Printing/gcode.html#g20g21_set_units","text":"With G20 you set the units to mm and G21 to Inches","title":"G20/G21 \"Set units\""},{"location":"3D Printing/gcode.html#g28_homing","text":"Send the printing head to the home position","title":"G28 \"Homing\""},{"location":"3D Printing/gcode.html#g90_absolute_mode","text":"This mode tell the printer to move to the absolute coordinate. 1 2 G90 G0 X10 The machine will move the print head to the coordinate X=10","title":"G90 \"absolute mode\""},{"location":"3D Printing/gcode.html#g91_relative_mode","text":"Tells the machine to interpreted the coordinate as relative, if the machine is currently at X=10 and we use this command 1 2 G91 G0 x10 the print head will move 10 units in the X direction, finishing at X=20.","title":"G91 \"Relative mode\""},{"location":"3D Printing/gcode.html#g2_and_g3_clockwise_or_anticlockwise_motion","text":"This commands tell the printer to move either clockwise G2 or andticlockwise G3 from the current point to the coordinate given, the center of the rotation is given by the parameter I and J, I denotate the X offset and J the Y offset 1 2 3 4 5 G21 G90 G17 G0 X6 Y18 G2 X18 Y6 I0 J-12 1 2 3 4 5 G21 G90 G17 G0 X-5 Y25 G3 X-25 Y5 I0 J-20","title":"G2 and G3 \"clockwise or anticlockwise motion\""},{"location":"3D Printing/gcode.html#comments","text":"Like any other programming languages, we can make comments in G-code, this is done using ; after the command, example 1 G0 X-25 Y5 ; rapid movement to X=-25 and Y=5","title":"Comments ;"},{"location":"3D Printing/gcode.html#example_of_the_beginning_of_a_g-code","text":"1 2 3 4 5 6 7 8 9 G90 M82 M106 S0 M140 S100 M190 S100 G90 Set the coordinate as absolute M82 Put the E axis into absolute mode independent of the other axes. M106 S0 turn fan on and set speed to 0 M140 S100 set Bed temperature as 100 degrees M190 S100 Wait until the temperature is 100 Now the printing phase 1 2 3 4 5 6 7 G1 X108.587 Y111.559 F525 ; controlled motion in X-Y plane G1 X108.553 Y111.504 F525 ; controlled motion in X-Y plane ... G1 Z0.345 F500 ; change layer There is nothing to explain here that is not already explained, there are some movements in X-Y and others in Z. After the print is complete we pass to the Reset printing phase 1 2 3 4 5 6 7 G28 ; bring the nozzle to home M104 S0 ; turn off heaters M140 S0 ; turn off bed M84 ; disable motors","title":"Example of the beginning of a G-code"},{"location":"3D Printing/Projects/Moving Electronics.html","text":"about how to connect the raspberry and power without the usb https://thepihut.com/blogs/raspberry-pi-tutorials/how-do-i-power-my-raspberry-pi about the AWG and connectors https://www.dronetrest.com/t/wires-connectors-and-current-what-you-need-to-know-as-a-drone-builder/1342 about wire https://www.dronetrest.com/t/wires-connectors-and-current-what-you-need-to-know-as-a-drone-builder/1342 about PID https://3daddict.com/pid-tune-3d-printer/ marlin guide: http://3daddict.com/beginner-guide-marlin-printer-firmware/","title":"Moving Electronics"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html","text":"After change the board or flash a new firmware it is necessary center and set the offset for the Z Z-Offset Instructions: \u00b6 Home 3D printer M851 Z0 - Reset Z0Offset M500 - Store setting to eeprom M501 - Set active parameters M503 - Display Active Parameters G28 Z - Home Z Axis G1 F60 Z0 - Move nozzle to true 0 offset M211 S0 - Switch off soft endstops Move nozzle towards bed slowly until the paper can barely move Take note of the Z on the printer display ( take that number and add the measurement of the calibration sheet or device used ) M851 Z X.XX (X.XX being your z offset achieved) M211 S1 - Enable Soft Endstops M500 - Save settings to EEprom M501 - Set Active Parameters M503 - display current settings base in https://www.youtube.com/watch?v=y_1Kg45APko&feature=youtu.be and https://3dprinting.stackexchange.com/questions/6375/how-to-center-my-prints-on-the-build-platform-re-calibrate-homing-offset/6376#6376 G-code used \u00b6 M851 - XYZ Probe Offset \u00b6 Not totally clear for me how to make the procedure explained, specially when they mentioned that you can use define Z_PROBE_OFFSET_FROM_EXTRUDER to define the offset at firmware level, although ti is useful in the calibration procedure. Usage M851 \u00b6 1 M851 [ X < linear > ] [ Y < linear > ] [ Z < linear > ] Example: M851 Z0 - Reset Z0Offset source: M851 M500 - Save settings \u00b6 Save all configuration on the EEPROM SKR Mini E3 doesnt have a EEPROM there is several ways to overcome this, that is what Reddit says, one will be with a Virtual EEPROM other using the SD card as EEPROM. It requires EEPROM_SETTINGS Usage M500 \u00b6 1 M500 source: M500 M501 - Restore Settings \u00b6 Load all saved settings from EEPROM It requires EEPROM_SETTINGS Usage M501 \u00b6 1 M501 source: M501 M503 - Report Settings \u00b6 Print a concise report of all current settings. Does not require EEPROM_SETTINGS Usage M503 \u00b6 1 M503 source: M503 G28 - Auto Home \u00b6 Auto-home one or more axis moving to the end-stop until triggered. G28 disable bed leveling. follow with M420 s to turn leveling on, or use RESTORE_LEVELING_AFTER_G28 Usage G28 \u00b6 1 G28 [O] [R] [X] [Y] [Z] [X] Flag to go back to the X axis origin [Y] Flag to go back to the Y axis origin [Z] Flag to go back to the Z axis origin Example: G28 Z home the Z axis source: G28 G1 - Linear move \u00b6 G0 and G1 suppose to be similar command, they generate a linear movement, but this command is queue and it is execute when there is a space in the queue, G0 it is use for movements that doesn\u2019t include the extrudor and G1 for those that does All the coordinates are given in millimeters by default (see G20 if you want to change to inch) Usage G1 \u00b6 1 2 G0 [E<pos>] [F<rate>] [X<pos>] [Y<pos>] [Z<pos>] G1 [E<pos>] [F<rate>] [X<pos>] [Y<pos>] [Z<pos>] [E<pos>] The length of filament to feed into the extruder between the start and end point [F<rate>] The maximum movement rate of the move between the start and end point. The feedrate set here applies to subsequent moves that omit this parameter. [X<pos>] A coordinate on the X axis [Y<pos>] A coordinate on the Y axis [Z<pos>] A coordinate on the Z axis source: G1 M211 - software Endstops \u00b6 Optionally enable and disabel software stop, this software stop prevent to go below 0 Usage M211 \u00b6 1 M211 [S<flag>] S<flag> flag 0 for disable and 1 for enable Example: 1 M211 S0 source: M211","title":"Center and Z offset"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#z-offset_instructions","text":"Home 3D printer M851 Z0 - Reset Z0Offset M500 - Store setting to eeprom M501 - Set active parameters M503 - Display Active Parameters G28 Z - Home Z Axis G1 F60 Z0 - Move nozzle to true 0 offset M211 S0 - Switch off soft endstops Move nozzle towards bed slowly until the paper can barely move Take note of the Z on the printer display ( take that number and add the measurement of the calibration sheet or device used ) M851 Z X.XX (X.XX being your z offset achieved) M211 S1 - Enable Soft Endstops M500 - Save settings to EEprom M501 - Set Active Parameters M503 - display current settings base in https://www.youtube.com/watch?v=y_1Kg45APko&feature=youtu.be and https://3dprinting.stackexchange.com/questions/6375/how-to-center-my-prints-on-the-build-platform-re-calibrate-homing-offset/6376#6376","title":"Z-Offset Instructions:"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#g-code_used","text":"","title":"G-code used"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#m851_-_xyz_probe_offset","text":"Not totally clear for me how to make the procedure explained, specially when they mentioned that you can use define Z_PROBE_OFFSET_FROM_EXTRUDER to define the offset at firmware level, although ti is useful in the calibration procedure.","title":"M851 - XYZ Probe Offset"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#usage_m851","text":"1 M851 [ X < linear > ] [ Y < linear > ] [ Z < linear > ] Example: M851 Z0 - Reset Z0Offset source: M851","title":"Usage M851"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#m500_-_save_settings","text":"Save all configuration on the EEPROM SKR Mini E3 doesnt have a EEPROM there is several ways to overcome this, that is what Reddit says, one will be with a Virtual EEPROM other using the SD card as EEPROM. It requires EEPROM_SETTINGS","title":"M500 - Save settings"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#usage_m500","text":"1 M500 source: M500","title":"Usage M500"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#m501_-_restore_settings","text":"Load all saved settings from EEPROM It requires EEPROM_SETTINGS","title":"M501 -  Restore Settings"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#usage_m501","text":"1 M501 source: M501","title":"Usage M501"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#m503_-_report_settings","text":"Print a concise report of all current settings. Does not require EEPROM_SETTINGS","title":"M503 - Report Settings"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#usage_m503","text":"1 M503 source: M503","title":"Usage M503"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#g28_-_auto_home","text":"Auto-home one or more axis moving to the end-stop until triggered. G28 disable bed leveling. follow with M420 s to turn leveling on, or use RESTORE_LEVELING_AFTER_G28","title":"G28 - Auto Home"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#usage_g28","text":"1 G28 [O] [R] [X] [Y] [Z] [X] Flag to go back to the X axis origin [Y] Flag to go back to the Y axis origin [Z] Flag to go back to the Z axis origin Example: G28 Z home the Z axis source: G28","title":"Usage G28"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#g1_-_linear_move","text":"G0 and G1 suppose to be similar command, they generate a linear movement, but this command is queue and it is execute when there is a space in the queue, G0 it is use for movements that doesn\u2019t include the extrudor and G1 for those that does All the coordinates are given in millimeters by default (see G20 if you want to change to inch)","title":"G1 - Linear move"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#usage_g1","text":"1 2 G0 [E<pos>] [F<rate>] [X<pos>] [Y<pos>] [Z<pos>] G1 [E<pos>] [F<rate>] [X<pos>] [Y<pos>] [Z<pos>] [E<pos>] The length of filament to feed into the extruder between the start and end point [F<rate>] The maximum movement rate of the move between the start and end point. The feedrate set here applies to subsequent moves that omit this parameter. [X<pos>] A coordinate on the X axis [Y<pos>] A coordinate on the Y axis [Z<pos>] A coordinate on the Z axis source: G1","title":"Usage G1"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#m211_-_software_endstops","text":"Optionally enable and disabel software stop, this software stop prevent to go below 0","title":"M211 - software Endstops"},{"location":"3D Printing/Setup and Calibration/Center and Z offset.html#usage_m211","text":"1 M211 [S<flag>] S<flag> flag 0 for disable and 1 for enable Example: 1 M211 S0 source: M211","title":"Usage M211"},{"location":"3D Printing/Setup and Calibration/Debuging and Logging.html","text":"Debug \u00b6 M111 - Debug Level \u00b6 There are several debug bits, to enable it is necessary add up the bits need it Mask Name Description 1 ECHO Echo all commands sent to the parser. 2 INFO Print extra informational messages. 4 ERRORS Print extra error messages. 8 DRYRUN Don\u2019t extrude, don\u2019t save leveling data, etc. 16 COMMUNICATION Not currently used. 32 LEVELING Detailed messages for homing, probing, and leveling. (Requires DEBUG_LEVELING_FEATURE .) 64 Reserved Reserved for future usage 128 Reserved Reserved for future usage Usage M111 \u00b6 1 M111 [S<flags>] [S<flags>] debug flag bits Example: Enable extra messages M111 S38 ; LEVELING, ERRORS, INFO Enable everything except dry-run mode M111 S247 ; 255 - 8 Disable previously set extra debugging output. M111 S0 source: M111 M280 - Servo Position \u00b6 Set or get the position of a servo. Using M280 \u00b6 1 M280 P<index> S<pos> P<index> Servo index to set or get S<pos> Servo position to set. Omit to read the current position. source: M280 Logging \u00b6 This are G-code to get the logs of the printer or the board. M928 - Start SD Logging \u00b6 Use this command to start logging all console and host input into the SD card use M29 to stop logging Usage M928 \u00b6 1 M928 filename filename File name of log file Example: M928 log.txt source: M928 M28 - Start SD Write \u00b6 This Commands start a file write, the firmware will log all commands, this command wont be execute until M29 closes the file. Required SDSUPPORT To write file while printing use M928 Usage M28 \u00b6 1 M28 filename filename File name of log file Example: M28 file.txt source: M28 M29 - Stop SD Write \u00b6 Stop Writing to file begun with M28 or M928 Require SDSUPPORT Usage M28 \u00b6 1 M29 source: M29","title":"Debuging and Logging"},{"location":"3D Printing/Setup and Calibration/Debuging and Logging.html#debug","text":"","title":"Debug"},{"location":"3D Printing/Setup and Calibration/Debuging and Logging.html#m111_-_debug_level","text":"There are several debug bits, to enable it is necessary add up the bits need it Mask Name Description 1 ECHO Echo all commands sent to the parser. 2 INFO Print extra informational messages. 4 ERRORS Print extra error messages. 8 DRYRUN Don\u2019t extrude, don\u2019t save leveling data, etc. 16 COMMUNICATION Not currently used. 32 LEVELING Detailed messages for homing, probing, and leveling. (Requires DEBUG_LEVELING_FEATURE .) 64 Reserved Reserved for future usage 128 Reserved Reserved for future usage","title":"M111 - Debug Level"},{"location":"3D Printing/Setup and Calibration/Debuging and Logging.html#usage_m111","text":"1 M111 [S<flags>] [S<flags>] debug flag bits Example: Enable extra messages M111 S38 ; LEVELING, ERRORS, INFO Enable everything except dry-run mode M111 S247 ; 255 - 8 Disable previously set extra debugging output. M111 S0 source: M111","title":"Usage M111"},{"location":"3D Printing/Setup and Calibration/Debuging and Logging.html#m280_-_servo_position","text":"Set or get the position of a servo.","title":"M280 - Servo Position"},{"location":"3D Printing/Setup and Calibration/Debuging and Logging.html#using_m280","text":"1 M280 P<index> S<pos> P<index> Servo index to set or get S<pos> Servo position to set. Omit to read the current position. source: M280","title":"Using M280"},{"location":"3D Printing/Setup and Calibration/Debuging and Logging.html#logging","text":"This are G-code to get the logs of the printer or the board.","title":"Logging"},{"location":"3D Printing/Setup and Calibration/Debuging and Logging.html#m928_-_start_sd_logging","text":"Use this command to start logging all console and host input into the SD card use M29 to stop logging","title":"M928 - Start SD Logging"},{"location":"3D Printing/Setup and Calibration/Debuging and Logging.html#usage_m928","text":"1 M928 filename filename File name of log file Example: M928 log.txt source: M928","title":"Usage M928"},{"location":"3D Printing/Setup and Calibration/Debuging and Logging.html#m28_-_start_sd_write","text":"This Commands start a file write, the firmware will log all commands, this command wont be execute until M29 closes the file. Required SDSUPPORT To write file while printing use M928","title":"M28 - Start SD Write"},{"location":"3D Printing/Setup and Calibration/Debuging and Logging.html#usage_m28","text":"1 M28 filename filename File name of log file Example: M28 file.txt source: M28","title":"Usage M28"},{"location":"3D Printing/Setup and Calibration/Debuging and Logging.html#m29_-_stop_sd_write","text":"Stop Writing to file begun with M28 or M928 Require SDSUPPORT","title":"M29 - Stop SD Write"},{"location":"3D Printing/Setup and Calibration/Debuging and Logging.html#usage_m28_1","text":"1 M29 source: M29","title":"Usage M28"},{"location":"3D Printing/Setup and Calibration/E-steps and Extrutor multiplie.html","text":"In order to have the best print quality, we need to have a correct calibration for the extruder this is achieve making a extruder steps and flow rate tune and calibrated for the printer. E-steps \u00b6 First we need to know what are the E-steps that the printer has at the moment, for that we use Octoprint (or any software that allow me to communicate with the printer by G-code) to send M503 that will give me as a result different values, between those value we have the steps for each axis and the extruder This is an example of a response with M503 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 G21 ; Units in mm M149 C ; Units in Celsius Filament settings: Disabled M200 D1.75 M200 D0 Steps per unit: M92 X100.00 Y100.00 Z398.70 E100.00 Maximum feedrates (units/s): M203 X400.00 Y400.00 Z8.00 E50.00 Maximum Acceleration (units/s2): M201 X1000 Y1000 Z100 E10000 Acceleration (units/s2): P R T M204 P400.00 R1000.00 T1000.00 Advanced: S T B X Z E M205 S0.00 T0.00 B20000 X10.00 Y10.00 Z0.30 E5.00 Home offset: M206 X0.00 Y0.00 Z0.00 Auto Bed Leveling: M420 S1 Z0.00 Material heatup parameters: M145 S0 H195 B55 F0 M145 S1 H205 B60 F0 PID settings: M301 P52.25 I5.60 D122.00 Z-Probe Offset (mm): M851 Z-2.41 Notice the lines 1 2 Steps per unit: M92 X100.00 Y100.00 Z398.70 E100.00 This means that currently the E-steps is 100 Calibrate the E-steps will be: \u00b6 Make a mark in the filament, already loaded in the printer, at 120mm. Connect to the printer to send G-code. Execute M83 ( this will set the extruder to relative mode) Make the printer extrude 100mm of filament using G1 E100 F100 (you can use the printer interface screen if it has one). wait until the printer finish the extrution measure how much filament is left before the extruder motor and the mark if the measurement is 20mm the e-steps are correct, if the number is different than that you need to make some changes in the E-steps Formula to find new E-steps \u00b6 Now with the old E-step value and number we got from the previous procedure we can find the new e-steps, for this example lets assume that the distance from the extruder gear/motor to the mar was 19mm, it means our printer extruded 101mm instead of the 100mm we ask for E-Steps(New) = old steps\\ x\\ (100/measured\\ distance\\ filament) E-Steps(New) = old steps\\ x\\ (100/measured\\ distance\\ filament) the old steps will be 100, the 100 is the amount of filament we asked to extrude and the \u201cmeasured distance filament\u201d will be 120-19=101 120-19=101 , so E-steps_n = 100\\ x\\ (100/101) E-steps_n = 100\\ x\\ (100/101) E-steps_n = 101 E-steps_n = 101 Save the New E-step Value \u00b6 To save this new value, you will need to use the following command 1 2 M92 E101.00 ; change E-steps for the new value M500 ; save new values to the EEPROM Calibrating the Extrusion multiplier or flow rate \u00b6 After calibrate the E-steps we need to make sure the machine is extruding the correct amount of filament comparing with the model on the slicer. For this we will print a calibration cube that will be a hollow object with 2 perimeter layers or 2 walls , by default the Extruder width will be 0.4mm the diameter of the nozzle, so the thickness of the walls ( 2 walls) should be 0.8mm, once we print the model we will measure the thickness with a caliper ( two measurements per wall). This example picture the there is just one wall Formula to calculate flow rate \u00b6 After measure all the walls we need to make the calculation Average_t = (measurement_1wall_1 + measurement_2wall_1 + measurement_1wall_2 + ... measurement_2wall_4) / 0.8mm Average_t = (measurement_1wall_1 + measurement_2wall_1 + measurement_1wall_2 + ... measurement_2wall_4) / 0.8mm now Flow-rate_n = Average_t\\ X\\ Current-Flow-rate Flow-rate_n = Average_t\\ X\\ Current-Flow-rate","title":"E-steps and Extrutor multiplier"},{"location":"3D Printing/Setup and Calibration/E-steps and Extrutor multiplie.html#e-steps","text":"First we need to know what are the E-steps that the printer has at the moment, for that we use Octoprint (or any software that allow me to communicate with the printer by G-code) to send M503 that will give me as a result different values, between those value we have the steps for each axis and the extruder This is an example of a response with M503 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 G21 ; Units in mm M149 C ; Units in Celsius Filament settings: Disabled M200 D1.75 M200 D0 Steps per unit: M92 X100.00 Y100.00 Z398.70 E100.00 Maximum feedrates (units/s): M203 X400.00 Y400.00 Z8.00 E50.00 Maximum Acceleration (units/s2): M201 X1000 Y1000 Z100 E10000 Acceleration (units/s2): P R T M204 P400.00 R1000.00 T1000.00 Advanced: S T B X Z E M205 S0.00 T0.00 B20000 X10.00 Y10.00 Z0.30 E5.00 Home offset: M206 X0.00 Y0.00 Z0.00 Auto Bed Leveling: M420 S1 Z0.00 Material heatup parameters: M145 S0 H195 B55 F0 M145 S1 H205 B60 F0 PID settings: M301 P52.25 I5.60 D122.00 Z-Probe Offset (mm): M851 Z-2.41 Notice the lines 1 2 Steps per unit: M92 X100.00 Y100.00 Z398.70 E100.00 This means that currently the E-steps is 100","title":"E-steps"},{"location":"3D Printing/Setup and Calibration/E-steps and Extrutor multiplie.html#calibrate_the_e-steps_will_be","text":"Make a mark in the filament, already loaded in the printer, at 120mm. Connect to the printer to send G-code. Execute M83 ( this will set the extruder to relative mode) Make the printer extrude 100mm of filament using G1 E100 F100 (you can use the printer interface screen if it has one). wait until the printer finish the extrution measure how much filament is left before the extruder motor and the mark if the measurement is 20mm the e-steps are correct, if the number is different than that you need to make some changes in the E-steps","title":"Calibrate the E-steps will be:"},{"location":"3D Printing/Setup and Calibration/E-steps and Extrutor multiplie.html#formula_to_find_new_e-steps","text":"Now with the old E-step value and number we got from the previous procedure we can find the new e-steps, for this example lets assume that the distance from the extruder gear/motor to the mar was 19mm, it means our printer extruded 101mm instead of the 100mm we ask for E-Steps(New) = old steps\\ x\\ (100/measured\\ distance\\ filament) E-Steps(New) = old steps\\ x\\ (100/measured\\ distance\\ filament) the old steps will be 100, the 100 is the amount of filament we asked to extrude and the \u201cmeasured distance filament\u201d will be 120-19=101 120-19=101 , so E-steps_n = 100\\ x\\ (100/101) E-steps_n = 100\\ x\\ (100/101) E-steps_n = 101 E-steps_n = 101","title":"Formula to find new E-steps"},{"location":"3D Printing/Setup and Calibration/E-steps and Extrutor multiplie.html#save_the_new_e-step_value","text":"To save this new value, you will need to use the following command 1 2 M92 E101.00 ; change E-steps for the new value M500 ; save new values to the EEPROM","title":"Save the New E-step Value"},{"location":"3D Printing/Setup and Calibration/E-steps and Extrutor multiplie.html#calibrating_the_extrusion_multiplier_or_flow_rate","text":"After calibrate the E-steps we need to make sure the machine is extruding the correct amount of filament comparing with the model on the slicer. For this we will print a calibration cube that will be a hollow object with 2 perimeter layers or 2 walls , by default the Extruder width will be 0.4mm the diameter of the nozzle, so the thickness of the walls ( 2 walls) should be 0.8mm, once we print the model we will measure the thickness with a caliper ( two measurements per wall). This example picture the there is just one wall","title":"Calibrating the Extrusion multiplier or flow rate"},{"location":"3D Printing/Setup and Calibration/E-steps and Extrutor multiplie.html#formula_to_calculate_flow_rate","text":"After measure all the walls we need to make the calculation Average_t = (measurement_1wall_1 + measurement_2wall_1 + measurement_1wall_2 + ... measurement_2wall_4) / 0.8mm Average_t = (measurement_1wall_1 + measurement_2wall_1 + measurement_1wall_2 + ... measurement_2wall_4) / 0.8mm now Flow-rate_n = Average_t\\ X\\ Current-Flow-rate Flow-rate_n = Average_t\\ X\\ Current-Flow-rate","title":"Formula to calculate flow rate"},{"location":"Coding/Arduino/Functions/Characters.html","text":"","title":"Characters"},{"location":"Coding/Arduino/Functions/Communication.html","text":"","title":"Communication"},{"location":"Coding/Arduino/Functions/Digital_and_Analog.html","text":"","title":"Digital I/O and Analog I/O"},{"location":"Coding/Arduino/Functions/Interrupts_and_External_Interruptions.html","text":"","title":"Interrupts and External Interruptions"},{"location":"Coding/Arduino/Functions/Math_and_Trigonometry.html","text":"","title":"Math and Trigonometry"},{"location":"Coding/Arduino/Functions/Random_Numbers.html","text":"","title":"Random Numbers"},{"location":"Coding/Arduino/Functions/Time.html","text":"","title":"Time"},{"location":"Coding/Arduino/Projects/Air_vent_servo_control.html","text":"This is part of the project for the Ikea Lack enclosure, more exact to the Air vent or the temperature control, the idea vents will be open and close with the servo, the decision to open or close the vents will be base in the temperature of the main enclosure. This is base in Servo automated iris / aperture for air flow control by AcE_Krystal First Sketch (Basic Control) \u00b6 First, we are going to create a sketch to control the servo, it will move 180 degrees right and later 180 left The code will be as follow: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #include <Servo.h> Servo servo_air_vent ; int servo_position = 0 ; void setup () { Servo_Air_vent . attach ( 9 ); } void loop () { for ( servo_position = 0 ; servo_position <= 180 ; servo_position += 1 ){ Servo_Air_vent . write ( servo_position ); delay ( 10 ); } for ( servo_position = 180 ; servo_position >= 0 ; servo_position -= 1 ){ Servo_Air_vent . write ( servo_position ); delay ( 10 ); } } We are going to import the library #include <Servo.h> we are going to give a name to the servo, in this case \u201cservo_air_vent\u201d Servo servo_air_vent; we define the initial position int servo_position = 0; in the setup block we tell arduino where the servo is connected Servo_Air_vent.attach (9); In the loop block we create 2 for that will move the servo to the right and later to the left","title":"Air Vent servo control"},{"location":"Coding/Arduino/Projects/Air_vent_servo_control.html#first_sketch_basic_control","text":"First, we are going to create a sketch to control the servo, it will move 180 degrees right and later 180 left The code will be as follow: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #include <Servo.h> Servo servo_air_vent ; int servo_position = 0 ; void setup () { Servo_Air_vent . attach ( 9 ); } void loop () { for ( servo_position = 0 ; servo_position <= 180 ; servo_position += 1 ){ Servo_Air_vent . write ( servo_position ); delay ( 10 ); } for ( servo_position = 180 ; servo_position >= 0 ; servo_position -= 1 ){ Servo_Air_vent . write ( servo_position ); delay ( 10 ); } } We are going to import the library #include <Servo.h> we are going to give a name to the servo, in this case \u201cservo_air_vent\u201d Servo servo_air_vent; we define the initial position int servo_position = 0; in the setup block we tell arduino where the servo is connected Servo_Air_vent.attach (9); In the loop block we create 2 for that will move the servo to the right and later to the left","title":"First Sketch (Basic Control)"},{"location":"Coding/Arduino/Structure/Control_Structure.html","text":"","title":"Control Structure"},{"location":"Coding/Arduino/Structure/Further_Syntax.html","text":"#define \u00b6 Description \u00b6 #define is a useful C++ component that allows the programmer to give a name to a constant value before the program is compiled. Defined constants in arduino don\u2019t take up any program memory space on the chip. The compiler will replace references to these constants with the defined value at compile time. Unwanted side effects: A constant name that had been #defined is included in some other constant or variable name. In that case the text would be replaced by the #defined number (or text). The const keyword is preferred for defining constants and should be used instead of #define . Syntax 1 #define constantName value Parameters: constantName: the name of the macro to define. value: the value to assign to the macro. Example Code \u00b6 1 #define ledPin 3","title":"Further Syntax"},{"location":"Coding/Arduino/Structure/Further_Syntax.html#define","text":"","title":"#define"},{"location":"Coding/Arduino/Structure/Further_Syntax.html#description","text":"#define is a useful C++ component that allows the programmer to give a name to a constant value before the program is compiled. Defined constants in arduino don\u2019t take up any program memory space on the chip. The compiler will replace references to these constants with the defined value at compile time. Unwanted side effects: A constant name that had been #defined is included in some other constant or variable name. In that case the text would be replaced by the #defined number (or text). The const keyword is preferred for defining constants and should be used instead of #define . Syntax 1 #define constantName value Parameters: constantName: the name of the macro to define. value: the value to assign to the macro.","title":"Description"},{"location":"Coding/Arduino/Structure/Further_Syntax.html#example_code","text":"1 #define ledPin 3","title":"Example Code"},{"location":"Coding/Arduino/Structure/Sketch.html","text":"The arduino code has two basic \u201cFunctions\u201d, these functions are a requirement to the sketch to run, other functions can be define and use inside this basic loops, these basic function are: Setup function. Loop function. setup() function \u00b6 The setup() function is called when a sketch starts. Use it to initialize variables, pin modes, start using libraries, etc. The setup() function will only run once, after each powerup or reset of the Arduino board. 1 2 3 void setup (){ } one of the statement that are going to be in this function, or that are commonly found, is the function to communicate with the serial port, Serial.begin(9600); this is standard and basically is feeding 960 characters per second over the serial 1 2 3 4 5 6 7 8 9 10 int buttonPin = 3 ; void setup () { Serial . begin ( 9600 ); pinMode ( buttonPin , INPUT ); } void loop () { // ... } loop() function \u00b6 After creating a setup() function, which initializes and sets the initial values, the loop() function does precisely what its name suggests, and loops consecutively, allowing your program to change and respond. Use it to actively control the Arduino board. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 int buttonPin = 3 ; // setup initializes serial and the button pin void setup () { Serial . begin ( 9600 ); pinMode ( buttonPin , INPUT ); } // loop checks the button pin each time, // and will send serial if it is pressed void loop () { if ( digitalRead ( buttonPin ) == HIGH ) { Serial . write ( 'H' ); } else { Serial . write ( 'L' ); } delay ( 1000 ); }","title":"Sketch"},{"location":"Coding/Arduino/Structure/Sketch.html#setup_function","text":"The setup() function is called when a sketch starts. Use it to initialize variables, pin modes, start using libraries, etc. The setup() function will only run once, after each powerup or reset of the Arduino board. 1 2 3 void setup (){ } one of the statement that are going to be in this function, or that are commonly found, is the function to communicate with the serial port, Serial.begin(9600); this is standard and basically is feeding 960 characters per second over the serial 1 2 3 4 5 6 7 8 9 10 int buttonPin = 3 ; void setup () { Serial . begin ( 9600 ); pinMode ( buttonPin , INPUT ); } void loop () { // ... }","title":"setup() function"},{"location":"Coding/Arduino/Structure/Sketch.html#loop_function","text":"After creating a setup() function, which initializes and sets the initial values, the loop() function does precisely what its name suggests, and loops consecutively, allowing your program to change and respond. Use it to actively control the Arduino board. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 int buttonPin = 3 ; // setup initializes serial and the button pin void setup () { Serial . begin ( 9600 ); pinMode ( buttonPin , INPUT ); } // loop checks the button pin each time, // and will send serial if it is pressed void loop () { if ( digitalRead ( buttonPin ) == HIGH ) { Serial . write ( 'H' ); } else { Serial . write ( 'L' ); } delay ( 1000 ); }","title":"loop() function"},{"location":"Coding/Arduino/Variables/Constants_and_Data_Types.html","text":"","title":"Constants and Data Types"},{"location":"Coding/Arduino/Variables/Conversion.html","text":"","title":"Conversion"},{"location":"Coding/Arduino/Variables/Variable_Scope_Qualifiers.html","text":"","title":"Variable Scope & Qualifiers"},{"location":"Coding/PowerLanguages/Plot.html","text":"To plot we use the function plot#() were # is a number that identify the plot, this number can be from 1 to 99 The basic structure of a plot statement for an indicator: PlotN: PlotN(numeric expression, \u201cplot name\u201d); //(where N = 1 to 99, no space) Usage Example1: 1 Plot1(Close, \"The Close\"); Usage Example2: 1 Plot1(High, \"The High\"); Plot2(Low, \"The Low\"); Modify structure of plot \u00b6 The complete structure of a plot statement for an indicator: PlotN: 1 PlotN(numeric expression, \"plot name\", foreground color, background color, width); Usage Example: 1 Plot1(Close, \"The Close\", Red, Default, 3); Note: It is generally more useful to set colors and width for an indicator conditionally based on some technical condition than to hard-code colors in the plot statement. There are three reserved words that can be used for this purpose: SetPlotColor , SetPlotBGColor , and SetPlotWidth . PlotPB \u00b6 The PlotPB statement is a specialized plot statement that is used in PaintBar studies. It instructs Multicharts where to draw on a bar so that the bar is painted a different color from the other bars based on some conditional criteria. The structure of a PlotPB statement for a PaintBar is: PlotPB: 1 PlotPB(Price Point, Price Point, \"plot name\"); Usage Example1 (paint full bar) : 1 2 3 4 if Close > Close[1] then PlotPB(High, Low, \"Up Bar\"); Usage Example2 (paint top half of the bar): if Close > Close[1] then PlotPB(High, High - Range * .5, \"Up Bar\"); In these examples the reserved word PlotPB is used to paint the bars, or a portion of the bar, a different color based on a specified condition. Usage Example3 (paint entire bar and set color): 1 2 if Close > Close[1] then PlotPB(High, Low, \"Up Bar\", Cyan); In these examples the reserved word PlotPB is used to paint the bar a different color and the color is specified in the PlotPB statement. NoPlot \u00b6 The NoPlot statement removes a specified drawn plot from the current bar. It is most often used to remove ShowMe or PaintBar plots that are no longer true on the current in-progress bar. If a ShowMe or PaintBar condition is true on the real-time in-progress bar, but during the same bar becomes false before the close of the bar, the drawn ShowMe or PaintBar can be removed with NoPlot. The structure of a NoPlot statement for an indicator is: NoPlot: 1 NoPlot(plot number); Usage Example: 1 2 3 4 if (High < Low[1]) Then Plot1(Low[1], \"GapDown\") else NoPlot(1) ; This ShowMe example marks the low price of a gap-down bar, but removes the ShowMe marker if the condition is no longer true on the real-time bar. Usage Example: 1 2 3 4 if Close > Average(Close,10) then PlotPB(High, Low, \"Up Bar\") else NoPlot(1); This PaintBar example paints the entire bar if the Close is greater than the average Close but removes the PaintBar if the condition is no longer true on the real-time bar. You may use number 1 to refer to a PlotPB statement in the NoPlot parameter. Displacing Plots \u00b6 Displacing plots allows you to visually move any analysis technique plots left or right on the chart by some number of specified bars. A positive number moves the plot to the left and a negative number moves the plot to the right. Space to the right of the last bar must be sufficient to accommodate the displaced plots or an error will occur. The structure of a NoPlot statement for an indicator is: Plot1[+/-N ] Square brackets after the Plot statement are used to indicate the number of bars to displace the plot left or right. Positive = left and Negative = right. Usage Example1 (displacing a plot into the future): 1 Plot1[-5](Average(Close,5), \u201cavg close\u201d); Usage Example2 (displacing a plot historically): 1 Plot1[5](Average(Close,5), \u201cavg close\u201d); These examples move the plot right and left, respectively, on the chart.","title":"Plot"},{"location":"Coding/PowerLanguages/Plot.html#modify_structure_of_plot","text":"The complete structure of a plot statement for an indicator: PlotN: 1 PlotN(numeric expression, \"plot name\", foreground color, background color, width); Usage Example: 1 Plot1(Close, \"The Close\", Red, Default, 3); Note: It is generally more useful to set colors and width for an indicator conditionally based on some technical condition than to hard-code colors in the plot statement. There are three reserved words that can be used for this purpose: SetPlotColor , SetPlotBGColor , and SetPlotWidth .","title":"Modify structure of plot"},{"location":"Coding/PowerLanguages/Plot.html#plotpb","text":"The PlotPB statement is a specialized plot statement that is used in PaintBar studies. It instructs Multicharts where to draw on a bar so that the bar is painted a different color from the other bars based on some conditional criteria. The structure of a PlotPB statement for a PaintBar is: PlotPB: 1 PlotPB(Price Point, Price Point, \"plot name\"); Usage Example1 (paint full bar) : 1 2 3 4 if Close > Close[1] then PlotPB(High, Low, \"Up Bar\"); Usage Example2 (paint top half of the bar): if Close > Close[1] then PlotPB(High, High - Range * .5, \"Up Bar\"); In these examples the reserved word PlotPB is used to paint the bars, or a portion of the bar, a different color based on a specified condition. Usage Example3 (paint entire bar and set color): 1 2 if Close > Close[1] then PlotPB(High, Low, \"Up Bar\", Cyan); In these examples the reserved word PlotPB is used to paint the bar a different color and the color is specified in the PlotPB statement.","title":"PlotPB"},{"location":"Coding/PowerLanguages/Plot.html#noplot","text":"The NoPlot statement removes a specified drawn plot from the current bar. It is most often used to remove ShowMe or PaintBar plots that are no longer true on the current in-progress bar. If a ShowMe or PaintBar condition is true on the real-time in-progress bar, but during the same bar becomes false before the close of the bar, the drawn ShowMe or PaintBar can be removed with NoPlot. The structure of a NoPlot statement for an indicator is: NoPlot: 1 NoPlot(plot number); Usage Example: 1 2 3 4 if (High < Low[1]) Then Plot1(Low[1], \"GapDown\") else NoPlot(1) ; This ShowMe example marks the low price of a gap-down bar, but removes the ShowMe marker if the condition is no longer true on the real-time bar. Usage Example: 1 2 3 4 if Close > Average(Close,10) then PlotPB(High, Low, \"Up Bar\") else NoPlot(1); This PaintBar example paints the entire bar if the Close is greater than the average Close but removes the PaintBar if the condition is no longer true on the real-time bar. You may use number 1 to refer to a PlotPB statement in the NoPlot parameter.","title":"NoPlot"},{"location":"Coding/PowerLanguages/Plot.html#displacing_plots","text":"Displacing plots allows you to visually move any analysis technique plots left or right on the chart by some number of specified bars. A positive number moves the plot to the left and a negative number moves the plot to the right. Space to the right of the last bar must be sufficient to accommodate the displaced plots or an error will occur. The structure of a NoPlot statement for an indicator is: Plot1[+/-N ] Square brackets after the Plot statement are used to indicate the number of bars to displace the plot left or right. Positive = left and Negative = right. Usage Example1 (displacing a plot into the future): 1 Plot1[-5](Average(Close,5), \u201cavg close\u201d); Usage Example2 (displacing a plot historically): 1 Plot1[5](Average(Close,5), \u201cavg close\u201d); These examples move the plot right and left, respectively, on the chart.","title":"Displacing Plots"},{"location":"Coding/PowerLanguages/PowerLanguages_001.html","text":"There are different type of data on Input and on Variables, these types are: a Numeric Value A String A Boolean (true or False) here an example: 001. Power languages tutorial 002. Free Forex API 003. Fixer API 004. Knowledge base Multicharts 005. Trading station Help page book for reference","title":"PowerLanguages 001"},{"location":"Coding/PowerLanguages/Reserved words.html","text":"Value1-99 \u00b6 Value is a reserved word that can store a value from a calcularion or an expresion, once is declared it can be reference later in the code.","title":"Reserved words"},{"location":"Coding/PowerLanguages/Reserved words.html#value1-99","text":"Value is a reserved word that can store a value from a calcularion or an expresion, once is declared it can be reference later in the code.","title":"Value1-99"},{"location":"Coding/PowerLanguages/functions.html","text":"Functions on easylanguage are similar to functions in other languages, not in syntax but in concepts, these are use to encapsulate logic to easy debugging and to avoid repetition. Making a Function \u00b6 to make a function we need to go to File -> New and select function: the next screen will display the options for the name of the function, the return type and the function storage The name \u00b6 like in other languages the function name should be descriptive and not contain special characters with the exception of underscore The return type \u00b6 the return type like in other languages specify the type of result that the function returns to the caller, if this return is a numeric value we need to select \u201c Numeric \u201d, if the function returns a boolean value, we will need to choose \u201c TrueFalse \u201d, finally if the function returns string we will choose \u201c String \u201c The function Storage \u00b6 In this case the best option will be \u201c Auto-detect \u201d, the reason is, that we will have lest problems if we decided to make significant changes to the function, the \u201c Simple \u201d it is use when inside the function we don\u2019t use previous bar\u2019s values, that means we don\u2019t use square baskets, if we decide to use square brackets the function storage option to select will be \u201c Series \u201c Functions Inputs \u00b6 In the indicators you need to specify the default value of the inputs, in the functions code, you have to specify the type of each input, this tell the program what input value to expect while the code is running. In most of the cases we will work with three type of input parameters: Numeric TrueFalse String Each of these types have one or two subtypes, series and simple , A simple way to see this will be; If a particular input parameter is constant it is Simple , for example ins a Moving average the length value use to calculate it will be a subtype Simple in other words NumericSimple , but the price, since this change or vary all the time, it will be define as subtype Series , that is why the price input will be NumericSeries . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 //Numeric Input Types //a constant input value, example 10 ConstantValue ( NumericSimple ), //a Input that can change its value, like Close PriceSeriesValue ( NumericSeries ), //TrueFalse Input Types //a Simple true/false switch for example used to enable a certain functionality CheckCondition ( TrueFalseSimple ), //Use for a true/false input that can change between true and false for a bar to bar EntryCondition ( TrueFalseSeries ), //String Input Type //a constant String - like email address EmailAddress ( StringSimple ), //use for text that change bar to bar EmailText ( StringSeries ), Function Outputs \u00b6 We can also use Inputs to receive values from the function, these \u201cin&out\u201d inputs can refer values to the caller, in this way we can create a function that not just give a return value, but also a series of values related with he operation, as an example, The stochastic, it return 4 values additional to the return value, this values are; Slow %K, Slow %D, Fast %K and Fast %D. to declare this reference input, we do it as a regular input with the difference that this will have \u201cRef\u201d at the end of the type, It is important to mention that the convention in easylanguease is to name this type of reference inputs with a \u201co\u201d at the beginning of the name, this will give more information and it will be easy to read. 1 2 3 4 5 6 7 8 //Output a numeric value - for example the result of a moving average oAverageValue ( NumericRef ), //Output a boolean - this could be the true/false result of a condition check for example oEntryCheck ( TrueFalseRef ), //output a String - for example a message that was created withing a function oMessage ( StringRef ), Return Value \u00b6 Each function will need to give a value, even if this is a dummy value, this done assigning the value to return to the name of the function. We can pass a reference array to and from the function, this is done with NumericArray , NumericArrayRef , TrueFalseArray , TrueFalseArrayRef , StringArray , StringArrayRef . An Example of a Function \u00b6 We are going to make a function that will calculate the moving average so for that we need The Inputs \u00b6 the inputs will be the Price and the AverageLenght 1 2 3 Inputs: Price ( NumericSeries ), AverageLength ( NumericSimple); The variables \u00b6 the variables to be use in the logic will be 1 2 3 4 variables: Counter ( 0 ), CloseValueSum ( 0 ), AverageValue ( 0 ); The logic \u00b6 now we use the counter to go through all the values in previous bars ( Averagelength ) all this sum will be store in CloseValueSum and later the Average will be calculated 1 2 3 4 5 6 7 8 9 10 11 // Reset the variable CloseValueSum = 0; for Counter = 0 to AverageLength - 1 begin CloseValueSum = CloseValueSum + Price[Counter]; end; // calculate the average if AverageLength <> 0 then AverageValue = CloseValueSum / AverageLength; The return \u00b6 Now we need to store the result in the name of the function so we can use it in an indicator 1 _victor_movingAverge = AverageValue; Using the Function \u00b6 now we can call this function from other indicator, and the code for this indicator will be 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Inputs: Price ( Close ), AverageLength ( 10 ); variables: MovAvgResult ( 0 ); //Call the function MovAvgResult = _victor_movingAverge( Price, AverageLength); //Plot Plot1(MovAvgResult, \"Average\");","title":"Functions"},{"location":"Coding/PowerLanguages/functions.html#making_a_function","text":"to make a function we need to go to File -> New and select function: the next screen will display the options for the name of the function, the return type and the function storage","title":"Making a Function"},{"location":"Coding/PowerLanguages/functions.html#the_name","text":"like in other languages the function name should be descriptive and not contain special characters with the exception of underscore","title":"The name"},{"location":"Coding/PowerLanguages/functions.html#the_return_type","text":"the return type like in other languages specify the type of result that the function returns to the caller, if this return is a numeric value we need to select \u201c Numeric \u201d, if the function returns a boolean value, we will need to choose \u201c TrueFalse \u201d, finally if the function returns string we will choose \u201c String \u201c","title":"The return type"},{"location":"Coding/PowerLanguages/functions.html#the_function_storage","text":"In this case the best option will be \u201c Auto-detect \u201d, the reason is, that we will have lest problems if we decided to make significant changes to the function, the \u201c Simple \u201d it is use when inside the function we don\u2019t use previous bar\u2019s values, that means we don\u2019t use square baskets, if we decide to use square brackets the function storage option to select will be \u201c Series \u201c","title":"The function Storage"},{"location":"Coding/PowerLanguages/functions.html#functions_inputs","text":"In the indicators you need to specify the default value of the inputs, in the functions code, you have to specify the type of each input, this tell the program what input value to expect while the code is running. In most of the cases we will work with three type of input parameters: Numeric TrueFalse String Each of these types have one or two subtypes, series and simple , A simple way to see this will be; If a particular input parameter is constant it is Simple , for example ins a Moving average the length value use to calculate it will be a subtype Simple in other words NumericSimple , but the price, since this change or vary all the time, it will be define as subtype Series , that is why the price input will be NumericSeries . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 //Numeric Input Types //a constant input value, example 10 ConstantValue ( NumericSimple ), //a Input that can change its value, like Close PriceSeriesValue ( NumericSeries ), //TrueFalse Input Types //a Simple true/false switch for example used to enable a certain functionality CheckCondition ( TrueFalseSimple ), //Use for a true/false input that can change between true and false for a bar to bar EntryCondition ( TrueFalseSeries ), //String Input Type //a constant String - like email address EmailAddress ( StringSimple ), //use for text that change bar to bar EmailText ( StringSeries ),","title":"Functions Inputs"},{"location":"Coding/PowerLanguages/functions.html#function_outputs","text":"We can also use Inputs to receive values from the function, these \u201cin&out\u201d inputs can refer values to the caller, in this way we can create a function that not just give a return value, but also a series of values related with he operation, as an example, The stochastic, it return 4 values additional to the return value, this values are; Slow %K, Slow %D, Fast %K and Fast %D. to declare this reference input, we do it as a regular input with the difference that this will have \u201cRef\u201d at the end of the type, It is important to mention that the convention in easylanguease is to name this type of reference inputs with a \u201co\u201d at the beginning of the name, this will give more information and it will be easy to read. 1 2 3 4 5 6 7 8 //Output a numeric value - for example the result of a moving average oAverageValue ( NumericRef ), //Output a boolean - this could be the true/false result of a condition check for example oEntryCheck ( TrueFalseRef ), //output a String - for example a message that was created withing a function oMessage ( StringRef ),","title":"Function Outputs"},{"location":"Coding/PowerLanguages/functions.html#return_value","text":"Each function will need to give a value, even if this is a dummy value, this done assigning the value to return to the name of the function. We can pass a reference array to and from the function, this is done with NumericArray , NumericArrayRef , TrueFalseArray , TrueFalseArrayRef , StringArray , StringArrayRef .","title":"Return Value"},{"location":"Coding/PowerLanguages/functions.html#an_example_of_a_function","text":"We are going to make a function that will calculate the moving average so for that we need","title":"An Example of a Function"},{"location":"Coding/PowerLanguages/functions.html#the_inputs","text":"the inputs will be the Price and the AverageLenght 1 2 3 Inputs: Price ( NumericSeries ), AverageLength ( NumericSimple);","title":"The Inputs"},{"location":"Coding/PowerLanguages/functions.html#the_variables","text":"the variables to be use in the logic will be 1 2 3 4 variables: Counter ( 0 ), CloseValueSum ( 0 ), AverageValue ( 0 );","title":"The variables"},{"location":"Coding/PowerLanguages/functions.html#the_logic","text":"now we use the counter to go through all the values in previous bars ( Averagelength ) all this sum will be store in CloseValueSum and later the Average will be calculated 1 2 3 4 5 6 7 8 9 10 11 // Reset the variable CloseValueSum = 0; for Counter = 0 to AverageLength - 1 begin CloseValueSum = CloseValueSum + Price[Counter]; end; // calculate the average if AverageLength <> 0 then AverageValue = CloseValueSum / AverageLength;","title":"The logic"},{"location":"Coding/PowerLanguages/functions.html#the_return","text":"Now we need to store the result in the name of the function so we can use it in an indicator 1 _victor_movingAverge = AverageValue;","title":"The return"},{"location":"Coding/PowerLanguages/functions.html#using_the_function","text":"now we can call this function from other indicator, and the code for this indicator will be 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Inputs: Price ( Close ), AverageLength ( 10 ); variables: MovAvgResult ( 0 ); //Call the function MovAvgResult = _victor_movingAverge( Price, AverageLength); //Plot Plot1(MovAvgResult, \"Average\");","title":"Using the Function"},{"location":"Coding/PowerLanguages/how_script_works.html","text":"Easylenguage/PowerLanguage studies operate on price data, organized as a series of data points, based on a defined interval and arranged in a chronological order. Each data point is a summary of a group of price points (ticks) that includes the price values of the first and of the last tick, as well as the range of price movement over the defined interval. Data points also include additional data, such as date and time of the last tick and trade volume. The most popular format for visually presenting a data point is a bar. References to bars in this guide actually refer to data points. Any other visual formats for presenting data points, such as candlesticks, points, lines, etc., can equally well be substituted. PowerLanguage studies are divided into two main types: Indicators and Signals . An Indicator is a visual technical analysis tool, used to analyze market conditions and identify and forecast trends and market patterns. An indicator is a visualization of a mathematical formula, and consists of one or more Plots \u2013 lines, histograms, series of points or crosses, highs and lows, left and right ticks, or a combination of the above, displayed on a chart A Signal is a mechanical technical analysis tool, used to systematically specify market entry or exit points according to a set of trading rules implemented in the signal\u2019s algorithm. The trade points are indicated on a chart by ticks and arrows. Strategies can easily be constructed by combining a number of signals. Market entry or exit points, specified by the signals, can be used to send orders electronically directly to a broker, fully automating the trading process. Indicators \u00b6 The purpose of indicators is to plot visualizations of mathematical formulas on a chart. The plots are created based on one or more price data series When applied to a chart, an indicator script first evaluates all the completed bars one-by-one, starting with the very first (oldest) bar on the chart. Once all the completed bars on the chart have been evaluated, an indicator script will proceed to evaluate the last bar on the chart on tick-by-tick basis, without waiting for the bar to be completed. Each time a new tick is received, the entire script will be executed for that bar, until the bar is completed and the next bar is started. Indicator scripts treat incomplete bars the same way as the bars that are completed, and can take action each time an incomplete bar is evaluated Update on Every Tick option can be turned off in the MultiCharts settings. Completed Bars (Indicators) \u00b6 A bar is considered completed when it is closed and no additional ticks can be added to it. For time-based charts, the bar is closed once the first tick with a time stamp past the bar\u2019s interval is received, or if no additional ticks are received for a period of three seconds. For tick-based charts, the bar is closed once the defined number of ticks has been reached. For range-based charts, the bar is closed once the tick with a price outside of the original bar\u2019s range has been received. For volume-based charts, the bar is closed once a tick, bringing the current bar\u2019s total to the defined number of contracts, has been received. For change-based charts, the bar is closed once a tick with a price, bringing the current bar\u2019s total number of price changes to the defined number, has been received. Signals \u00b6 Signals are the basic building blocks of strategies. Signals are substantially more complex than indicators and take in to account a far greater number of factors. When applied to a chart, a signal script first evaluates all the completed bars one-by-one, starting with the very first (oldest) bar on the chart. The entire script is executed once for each completed bar. On each bar, based on the results of the evaluation, a signal script may generate one or more trading orders. Orders are indicated by an arrow or a mark on the chart, by a visual or an audio alert, etc By default, once all the completed bars on a chart are evaluated, the execution of a signal script is paused until a new bar is completed (a bar is completed once an interval, defined for each bar, is over), and then the entire script is executed again for the new bar. Completed Bars (Signals) \u00b6 A bar is considered completed when it is closed and no additional ticks can be added to it. For time-based charts, the bar is closed once the first tick with a time stamp past the bar\u2019s interval is received, or if no additional ticks are received for a period of 300 seconds. For tick-based charts, the bar is closed once the defined number of ticks has been reached. For range-based charts, the bar is closed once the tick with a price outside of the original bar\u2019s range has been received. For volume-based charts, the bar is closed once a tick, bringing the current bar\u2019s total to the defined number of contracts, has been received. For change-based charts, the bar is closed once a tick with a price, bringing the current bar\u2019s total number of price changes to the defined number, has been received.","title":"How Scripts Work"},{"location":"Coding/PowerLanguages/how_script_works.html#indicators","text":"The purpose of indicators is to plot visualizations of mathematical formulas on a chart. The plots are created based on one or more price data series When applied to a chart, an indicator script first evaluates all the completed bars one-by-one, starting with the very first (oldest) bar on the chart. Once all the completed bars on the chart have been evaluated, an indicator script will proceed to evaluate the last bar on the chart on tick-by-tick basis, without waiting for the bar to be completed. Each time a new tick is received, the entire script will be executed for that bar, until the bar is completed and the next bar is started. Indicator scripts treat incomplete bars the same way as the bars that are completed, and can take action each time an incomplete bar is evaluated Update on Every Tick option can be turned off in the MultiCharts settings.","title":"Indicators"},{"location":"Coding/PowerLanguages/how_script_works.html#completed_bars_indicators","text":"A bar is considered completed when it is closed and no additional ticks can be added to it. For time-based charts, the bar is closed once the first tick with a time stamp past the bar\u2019s interval is received, or if no additional ticks are received for a period of three seconds. For tick-based charts, the bar is closed once the defined number of ticks has been reached. For range-based charts, the bar is closed once the tick with a price outside of the original bar\u2019s range has been received. For volume-based charts, the bar is closed once a tick, bringing the current bar\u2019s total to the defined number of contracts, has been received. For change-based charts, the bar is closed once a tick with a price, bringing the current bar\u2019s total number of price changes to the defined number, has been received.","title":"Completed Bars (Indicators)"},{"location":"Coding/PowerLanguages/how_script_works.html#signals","text":"Signals are the basic building blocks of strategies. Signals are substantially more complex than indicators and take in to account a far greater number of factors. When applied to a chart, a signal script first evaluates all the completed bars one-by-one, starting with the very first (oldest) bar on the chart. The entire script is executed once for each completed bar. On each bar, based on the results of the evaluation, a signal script may generate one or more trading orders. Orders are indicated by an arrow or a mark on the chart, by a visual or an audio alert, etc By default, once all the completed bars on a chart are evaluated, the execution of a signal script is paused until a new bar is completed (a bar is completed once an interval, defined for each bar, is over), and then the entire script is executed again for the new bar.","title":"Signals"},{"location":"Coding/PowerLanguages/how_script_works.html#completed_bars_signals","text":"A bar is considered completed when it is closed and no additional ticks can be added to it. For time-based charts, the bar is closed once the first tick with a time stamp past the bar\u2019s interval is received, or if no additional ticks are received for a period of 300 seconds. For tick-based charts, the bar is closed once the defined number of ticks has been reached. For range-based charts, the bar is closed once the tick with a price outside of the original bar\u2019s range has been received. For volume-based charts, the bar is closed once a tick, bringing the current bar\u2019s total to the defined number of contracts, has been received. For change-based charts, the bar is closed once a tick with a price, bringing the current bar\u2019s total number of price changes to the defined number, has been received.","title":"Completed Bars (Signals)"},{"location":"Coding/PowerLanguages/text_trendlines.html","text":"To demonstrate how to make lines and text in easylanguage(powerlanguage) we can create a study which goal is to track the daily extremes and to display them on the chart. We want to be able to see the current extremes for the day and also show yesterday\u2019s extremes on today\u2019s data. We need to be able to find the highest high and lowest low for each day The study should use trendlines to display yesterday\u2019s extremes We want to be able to change the appearance on the chart via inputs The study should display text on the chart that labels the lines Simple Program Logic \u00b6 Track daily high and low with a variable throughout the day Store the previous daily extremes on a new day and reset the tracking variables Draw text and trendlines for the previous extremes on today\u2019s data and update it with every new bar Add inputs to be able to conveniently change the text and trendline looks (color, size etc.) Trendlines \u00b6 Each Trendline has his own ID assigned automatically by Multicharts, to create a new trendline we need to use TL_New follow by 6 parameters, bellow we will create a variable to store the ID and we will name the different parameter, just to make it easier to read 1 TLID = TL_New(StartDate, StartTime, StartValue, EndDate, EndTime, EndValue); so a simple code that will draw a horizontal line will be 1 2 3 4 5 6 7 8 Variables: TLID (-1) // can be 0 but just for debugging we use -1 once begin // draw a trendline spacing over eleven bars TLID = TL_New(Date[10], Time[10], Close, Date, Time, Close); end; the result will be something like: there are some characteristics of the line that we can change, those will be: Color: TL_SetColor(TLID, Color); Size: TL_SetSize(TLID, size); size will be from 0 to 6 Style: TL_SetStyle(TLID, Style) for the style we have 5 different types, we can use reserve words or numbers as bellow so let see how the statement change after adding this settings 1 2 3 4 TLID = TL_New(Date[10], Time[10], Close, Date, Time, Close); TL_SetColor(TLID, red); TL_SetSize(TLID,2); TL_SetStyle(TLID,2); // 2 can be change for Tool_Dashed and the result will be similar to this: Text \u00b6 Similar to the trendlines, the text will need an anchor point, as well as the trendlines each text will have an ID and some parameter, thus the structure will be like: 1 TxtID = Text_New(TextDate, TextTime, TextPrice, TextString); so our script will be 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Variables: TLID (-1) // can be 0 but just for debugging we use -1 TxtID (-1) once begin // draw a trendline spacing over eleven bars TLID = TL_New(Date[10], Time[10], Close, Date, Time, Close); TL_SetColor(TLID, red); TL_SetSize(TLID,2); TL_SetStyle(TLID,2); // 2 can be change for Tool_Dashed TxtID = Text_New(Date, Time, Close, \"Hello World\"); end; then the result will be Like the trendlines we can modify the text style, position and color, we do this with: Color: Text_SetColor(TxtID, red) Size: Text_Setsize(TXtID, 12) Style: Text_SetStyle(TxtID, Horizontal_point, Vertical_point) The horizontal placement parameter can have three values: 0 \u2013 will place the text to the right of the bar 1 \u2013 will place the text to the left of the bar 2 \u2013 the text will be centered on the bar The vertical placement parameter can have three different values, too: 0 \u2013 will place the text under the specified price value 1 \u2013 the text will be above the price value 2 \u2013 will center the text on the price The code will be: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Variables: VertTxtP1 (2 ) HorizTxtP1 (0 ) TLID (-1) // can be 0 but just for debugging we use -1 TxtID (-1) once begin // draw a trendline spacing over eleven bars TLID = TL_New(Date[10], Time[10], Close, Date, Time, Close); TL_SetColor(TLID, red); TL_SetSize(TLID,2); TL_SetStyle(TLID,2); // 2 can be change for Tool_Dashed TxtID = Text_New(Date, Time, Close, \"Hello World\"); Text_SetColor(TxTID, red); Text_Setsize(TxtID, 12); Text_SetStyle(TxtID, HorizTxtP1, VertTxtP1); end; And we will have as a result: The study \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 Inputs: HighTLColor (darkgreen), LowTLColor (red), TLSize (1), TLStyle (1), HighTextColor (darkgreen), LowTextColor (red), TextSize (10); variable: VertTxtP1 (2), HorizTxtP1 (0), HaveTextAndLines (false), DayHigh (High), DayLow (Low), PrevDayHigh (0), PrevDayLow (0), HiTxt (-1), LoTxt (-1), HiTL (-1), LoTL (-1); //Reset on a date change if Date <> Date[1] ten begin // save previous extremes PrevDayHigh = DayHigh; PrevDayLow = DayLow; // get the high and low of the day DayHigh = High; DayLow = Low; // variable just to make sure the lines and text exist HaveTextAndLines = true; // Create the trendlines HiTL = TL_New(Date, Time, PrevDayHigh, Date, Time, PrevDayHigh); TL_SetColor(HiTL, HighTLColor); TL_SetSize(HiTL,TLSize); TL_SetStyle(HiTL, TLStyle); LowTL = TL_New(Date, Time, PrevDayLow, Date, Time, PrevDayLow); TL_SetColor(LoTL, LowTLColor); TL_SetSize(LoTL, TLSize); TL_SetStyle(LoTL, TLStyle); HiTxt = Text_New(Date, Time, PrevDayHigh, \"PrevHi\"); Text_SetColor(HiTxt, HighTextColor); Text_Setsize(HiTxt, TextSize); Text_SetStyle(HiTxt, HorizTxtP1, VertTxtP1); LowTxt = Text_New(Date, Time, PrevDayLow, \"PrevLow\"); Text_SetColor(LowTxt, LowTextColor); Text_Setsize(LowTxt, TextSize); Text_SetStyle(LowTxt, HorizTxtP1, VertTxtP1); end; Now we are going to use some reserve word to finish or end the Trendlines and set the text location for the Trendlines: 1 TL_SetEnd(TLID, TL_End_Date, TL_End_Time, TL_End_Price); and the text location 1 Text_SetLocation(TextID, Text_Bar_Date, Text_Bar_Time, Text_Bar_price) We will use the boolean variable \u201cHaveTextAndLines\u201d to make sure that we only try to update a text or trendline when it\u2019s ensured that at least one set of text and trendlines exists. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 if HaveTextAndLines then begin // update the trandline endpoint TL_SetEnd(HiTL, Date, TIme, PrevDayHigh); TL_SetEnd(LoTL, Date, Time, PrevDayLow); //Update text to the new location Text_SetLocation(HiTxt, Date, Time, PrevDayHigh); Text_SetLocation(LoTxt, Date, Time, PrevDayLow); end; // Update the variable tracking the higher high if High > DayHigh then DayHigh = High; // Update the Variable Tracking the lowest low if Low < DayLow then DayLow = Low;","title":"Text and Trendlines"},{"location":"Coding/PowerLanguages/text_trendlines.html#simple_program_logic","text":"Track daily high and low with a variable throughout the day Store the previous daily extremes on a new day and reset the tracking variables Draw text and trendlines for the previous extremes on today\u2019s data and update it with every new bar Add inputs to be able to conveniently change the text and trendline looks (color, size etc.)","title":"Simple Program Logic"},{"location":"Coding/PowerLanguages/text_trendlines.html#trendlines","text":"Each Trendline has his own ID assigned automatically by Multicharts, to create a new trendline we need to use TL_New follow by 6 parameters, bellow we will create a variable to store the ID and we will name the different parameter, just to make it easier to read 1 TLID = TL_New(StartDate, StartTime, StartValue, EndDate, EndTime, EndValue); so a simple code that will draw a horizontal line will be 1 2 3 4 5 6 7 8 Variables: TLID (-1) // can be 0 but just for debugging we use -1 once begin // draw a trendline spacing over eleven bars TLID = TL_New(Date[10], Time[10], Close, Date, Time, Close); end; the result will be something like: there are some characteristics of the line that we can change, those will be: Color: TL_SetColor(TLID, Color); Size: TL_SetSize(TLID, size); size will be from 0 to 6 Style: TL_SetStyle(TLID, Style) for the style we have 5 different types, we can use reserve words or numbers as bellow so let see how the statement change after adding this settings 1 2 3 4 TLID = TL_New(Date[10], Time[10], Close, Date, Time, Close); TL_SetColor(TLID, red); TL_SetSize(TLID,2); TL_SetStyle(TLID,2); // 2 can be change for Tool_Dashed and the result will be similar to this:","title":"Trendlines"},{"location":"Coding/PowerLanguages/text_trendlines.html#text","text":"Similar to the trendlines, the text will need an anchor point, as well as the trendlines each text will have an ID and some parameter, thus the structure will be like: 1 TxtID = Text_New(TextDate, TextTime, TextPrice, TextString); so our script will be 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Variables: TLID (-1) // can be 0 but just for debugging we use -1 TxtID (-1) once begin // draw a trendline spacing over eleven bars TLID = TL_New(Date[10], Time[10], Close, Date, Time, Close); TL_SetColor(TLID, red); TL_SetSize(TLID,2); TL_SetStyle(TLID,2); // 2 can be change for Tool_Dashed TxtID = Text_New(Date, Time, Close, \"Hello World\"); end; then the result will be Like the trendlines we can modify the text style, position and color, we do this with: Color: Text_SetColor(TxtID, red) Size: Text_Setsize(TXtID, 12) Style: Text_SetStyle(TxtID, Horizontal_point, Vertical_point) The horizontal placement parameter can have three values: 0 \u2013 will place the text to the right of the bar 1 \u2013 will place the text to the left of the bar 2 \u2013 the text will be centered on the bar The vertical placement parameter can have three different values, too: 0 \u2013 will place the text under the specified price value 1 \u2013 the text will be above the price value 2 \u2013 will center the text on the price The code will be: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Variables: VertTxtP1 (2 ) HorizTxtP1 (0 ) TLID (-1) // can be 0 but just for debugging we use -1 TxtID (-1) once begin // draw a trendline spacing over eleven bars TLID = TL_New(Date[10], Time[10], Close, Date, Time, Close); TL_SetColor(TLID, red); TL_SetSize(TLID,2); TL_SetStyle(TLID,2); // 2 can be change for Tool_Dashed TxtID = Text_New(Date, Time, Close, \"Hello World\"); Text_SetColor(TxTID, red); Text_Setsize(TxtID, 12); Text_SetStyle(TxtID, HorizTxtP1, VertTxtP1); end; And we will have as a result:","title":"Text"},{"location":"Coding/PowerLanguages/text_trendlines.html#the_study","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 Inputs: HighTLColor (darkgreen), LowTLColor (red), TLSize (1), TLStyle (1), HighTextColor (darkgreen), LowTextColor (red), TextSize (10); variable: VertTxtP1 (2), HorizTxtP1 (0), HaveTextAndLines (false), DayHigh (High), DayLow (Low), PrevDayHigh (0), PrevDayLow (0), HiTxt (-1), LoTxt (-1), HiTL (-1), LoTL (-1); //Reset on a date change if Date <> Date[1] ten begin // save previous extremes PrevDayHigh = DayHigh; PrevDayLow = DayLow; // get the high and low of the day DayHigh = High; DayLow = Low; // variable just to make sure the lines and text exist HaveTextAndLines = true; // Create the trendlines HiTL = TL_New(Date, Time, PrevDayHigh, Date, Time, PrevDayHigh); TL_SetColor(HiTL, HighTLColor); TL_SetSize(HiTL,TLSize); TL_SetStyle(HiTL, TLStyle); LowTL = TL_New(Date, Time, PrevDayLow, Date, Time, PrevDayLow); TL_SetColor(LoTL, LowTLColor); TL_SetSize(LoTL, TLSize); TL_SetStyle(LoTL, TLStyle); HiTxt = Text_New(Date, Time, PrevDayHigh, \"PrevHi\"); Text_SetColor(HiTxt, HighTextColor); Text_Setsize(HiTxt, TextSize); Text_SetStyle(HiTxt, HorizTxtP1, VertTxtP1); LowTxt = Text_New(Date, Time, PrevDayLow, \"PrevLow\"); Text_SetColor(LowTxt, LowTextColor); Text_Setsize(LowTxt, TextSize); Text_SetStyle(LowTxt, HorizTxtP1, VertTxtP1); end; Now we are going to use some reserve word to finish or end the Trendlines and set the text location for the Trendlines: 1 TL_SetEnd(TLID, TL_End_Date, TL_End_Time, TL_End_Price); and the text location 1 Text_SetLocation(TextID, Text_Bar_Date, Text_Bar_Time, Text_Bar_price) We will use the boolean variable \u201cHaveTextAndLines\u201d to make sure that we only try to update a text or trendline when it\u2019s ensured that at least one set of text and trendlines exists. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 if HaveTextAndLines then begin // update the trandline endpoint TL_SetEnd(HiTL, Date, TIme, PrevDayHigh); TL_SetEnd(LoTL, Date, Time, PrevDayLow); //Update text to the new location Text_SetLocation(HiTxt, Date, Time, PrevDayHigh); Text_SetLocation(LoTxt, Date, Time, PrevDayLow); end; // Update the variable tracking the higher high if High > DayHigh then DayHigh = High; // Update the Variable Tracking the lowest low if Low < DayLow then DayLow = Low;","title":"The study"},{"location":"Coding/Python/Data structures.html","text":"Related with data structures Data Structures \u00b6 Data structures are containers that organize and group data types together in different ways. Mutability and Order \u00b6 Mutability is about whether or not we can change an object once it has been created. If an object (like a list or string) can be changed (like a list can), then it is called mutable. However, if an object cannot be changed with creating a completely new object (like strings), then the object is considered immutable . Order is about whether the position of an element in the object can be used to access the element. Both strings and lists are ordered. We can use the order to access parts of a list and string. List \u00b6 A list is one of the most common and basic data structures in Python. List are Mutable and ordinated data structures 1 list_of_random_things = [ 1 , 3.4 , 'a string' , True ] Tuples \u00b6 Tuples are a data type for immutable ordered sequences of elements. They are often used to store related pieces of information, tuples are immutable - you can\u2019t add and remove items from tuples, or sort them in place 1 location = ( 13.4125 , 103.866667 ) Tuples can also be used to assign multiple variables in a compact way. In the example bellow, a tuple called dimensions is created, next, the content of this tuple is unpack, in three different variables, this is called tuple unpacking. 1 2 dimensions = 52 , 40 , 100 length , width , height = dimensions The parentheses are optional when defining tuples, and programmers frequently omit them if parentheses don\u2019t clarify the code. Set \u00b6 A set is a data type for mutable unordered collections of unique elements. One application of a set is to quickly remove duplicates from a list, it is an unordered data type, there fore if the method .pop() is use there is no way to know exactly with element will be eliminated. 1 2 3 numbers = [ 1 , 2 , 6 , 3 , 1 , 1 , 6 ] unique_nums = set ( numbers ) print ( unique_nums ) It will output: 1 {1, 2, 3, 6} Dictionaries \u00b6 A dictionary is a mutable data type that stores mappings of unique keys to values. 1 elements = { \"hydrogen\" : 1 , \"helium\" : 2 , \"carbon\" : 6 } Dictionaries can have keys of any immutable type, like integers or tuples, not just strings. It\u2019s not even necessary for every key to have the same type. If you expect lookups to sometimes fail, get might be a better tool than normal square bracket lookups because errors can crash your program. 1 2 print ( \"carbon\" in elements ) print ( elements . get ( \"dilithium\" )) this would output: 1 2 True None So, to recap Data Structure Ordered Mutable Constructor Example List Yes Yes [] or list() [5.7,4,'yesy',5.7] Tuple Yes No () or tuple() (5.7,4,'yes',5.7) Set No Yes {} 1 or set() {5.7,4,'yes'} Dictionary No No 2 {} or dict() {'jun':75.'jul':89} You can use curly braces to define a set like this: {1, 2, 3} . However, if you leave the curly braces empty like this: {} Python will instead create an empty dictionary. So to create an empty set, use set() . \u21a9 A dictionary itself is mutable, but each of its individual keys must be immutable. \u21a9","title":"Data Structures"},{"location":"Coding/Python/Data structures.html#data_structures","text":"Data structures are containers that organize and group data types together in different ways.","title":"Data Structures"},{"location":"Coding/Python/Data structures.html#mutability_and_order","text":"Mutability is about whether or not we can change an object once it has been created. If an object (like a list or string) can be changed (like a list can), then it is called mutable. However, if an object cannot be changed with creating a completely new object (like strings), then the object is considered immutable . Order is about whether the position of an element in the object can be used to access the element. Both strings and lists are ordered. We can use the order to access parts of a list and string.","title":"Mutability and Order"},{"location":"Coding/Python/Data structures.html#list","text":"A list is one of the most common and basic data structures in Python. List are Mutable and ordinated data structures 1 list_of_random_things = [ 1 , 3.4 , 'a string' , True ]","title":"List"},{"location":"Coding/Python/Data structures.html#tuples","text":"Tuples are a data type for immutable ordered sequences of elements. They are often used to store related pieces of information, tuples are immutable - you can\u2019t add and remove items from tuples, or sort them in place 1 location = ( 13.4125 , 103.866667 ) Tuples can also be used to assign multiple variables in a compact way. In the example bellow, a tuple called dimensions is created, next, the content of this tuple is unpack, in three different variables, this is called tuple unpacking. 1 2 dimensions = 52 , 40 , 100 length , width , height = dimensions The parentheses are optional when defining tuples, and programmers frequently omit them if parentheses don\u2019t clarify the code.","title":"Tuples"},{"location":"Coding/Python/Data structures.html#set","text":"A set is a data type for mutable unordered collections of unique elements. One application of a set is to quickly remove duplicates from a list, it is an unordered data type, there fore if the method .pop() is use there is no way to know exactly with element will be eliminated. 1 2 3 numbers = [ 1 , 2 , 6 , 3 , 1 , 1 , 6 ] unique_nums = set ( numbers ) print ( unique_nums ) It will output: 1 {1, 2, 3, 6}","title":"Set"},{"location":"Coding/Python/Data structures.html#dictionaries","text":"A dictionary is a mutable data type that stores mappings of unique keys to values. 1 elements = { \"hydrogen\" : 1 , \"helium\" : 2 , \"carbon\" : 6 } Dictionaries can have keys of any immutable type, like integers or tuples, not just strings. It\u2019s not even necessary for every key to have the same type. If you expect lookups to sometimes fail, get might be a better tool than normal square bracket lookups because errors can crash your program. 1 2 print ( \"carbon\" in elements ) print ( elements . get ( \"dilithium\" )) this would output: 1 2 True None So, to recap Data Structure Ordered Mutable Constructor Example List Yes Yes [] or list() [5.7,4,'yesy',5.7] Tuple Yes No () or tuple() (5.7,4,'yes',5.7) Set No Yes {} 1 or set() {5.7,4,'yes'} Dictionary No No 2 {} or dict() {'jun':75.'jul':89} You can use curly braces to define a set like this: {1, 2, 3} . However, if you leave the curly braces empty like this: {} Python will instead create an empty dictionary. So to create an empty set, use set() . \u21a9 A dictionary itself is mutable, but each of its individual keys must be immutable. \u21a9","title":"Dictionaries"},{"location":"Coding/Python/Databases_with_Python.html","text":"In this examples we are going to use SQLite as database. Import, Connect and Cursor \u00b6 To start to use SQLite with python we will need to import the library \u2018sqlite3\u2019, once imported we can start using it, first we will establish a connection with the database using sqlite3.connect() , later, to start the navigation we will need a cursor, for that we use .cursor() . 1 2 3 4 import sqlite3 conn = sqlite3 . connect ( 'name database' ) cur = conn . cursor () using as example chapter 15 of \u201cPython for everyone\u201d as an example we have 1 2 3 4 import sqlite3 conn = sqlite3 . connect ( 'rosterdb.sqlite' ) cur = conn . cursor () where rosterdb.sqlite is the name of the database executescript() and execute() \u00b6 there are two ways to execute SQL statements in python, one will be executescript() this will allow me to execute several SLQ statement art the same time, if this statement finish with \u201c;\u201d. The second option will be execute() this will be limited to one SQL statement. executescript() example \u00b6 In the following code we will execute several SLQ statement we start with executescript() , we are goin to use \u201c'''\u201c. 1 2 3 4 5 6 import sqlite3 conn = sqlite3 . connect ( 'rosterdb.sqlite' ) cur = conn . cursor () cur . executescript ( ''' SQL statements''' ) We will delete any existing tables with the names \u201cUser\u201d, \u201cMember\u201d, \u201cCourse\u201d 1 2 3 4 5 6 7 8 9 10 import sqlite3 conn = sqlite3 . connect ( 'rosterdb.sqlite' ) cur = conn . cursor () cur . executescript ( ''' DROP TABLE IF EXISTS User; DROP TABLE IF EXISTS Member; DROP TABLE IF EXISTS Course; ''' ) Now we will create 3 tables; \u201cUser\u201d, \u201cCourse\u201d, \u201cMember\u201d. Member table contain a primary key thajt take two parameters, this is a way to link to the other 2 tables and create the many to many relationship 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import sqlite3 conn = sqlite3 . connect ( 'rosterdb.sqlite' ) cur = conn . cursor () # Do some setup cur . executescript ( ''' DROP TABLE IF EXISTS User; DROP TABLE IF EXISTS Member; DROP TABLE IF EXISTS Course; CREATE TABLE User ( id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, name TEXT UNIQUE ); CREATE TABLE Course ( id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, title TEXT UNIQUE ); CREATE TABLE Member ( user_id INTEGER, course_id INTEGER, role INTEGER, PRIMARY KEY (user_id, course_id) ) ''' ) INSERT , IGNORE , REPLACE and SELECT \u00b6 In previous steps we create the shema of the database, now we need to populate this database with information, in this case we are goin to extract information from a JSON file and use it to feed the datase. Getting the data from JSON \u00b6 First we need to import json 1 2 import json [ ... ] second we need to get the information of a JSON file, in this case we are going to use one of the JSON file examples from the book \u201cPython for everyone\u201d 1 2 3 4 5 6 7 8 9 10 11 12 [ ... ] fname = input ( 'Enter file name: ' ) if len ( fname ) < 1 : fname = 'roster_data_sample.json' # [ # [ \"Charley\", \"si110\", 1 ], # [ \"Mea\", \"si110\", 0 ], str_data = open ( fname ) . read () json_data = json . loads ( str_data ) [ ... ] Now all the information is in the variable json_data (a list) we will need though this list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [ ... ] fname = input ( 'Enter file name: ' ) if len ( fname ) < 1 : fname = 'roster_data_sample.json' # [ # [ \"Charley\", \"si110\", 1 ], # [ \"Mea\", \"si110\", 0 ], str_data = open ( fname ) . read () json_data = json . loads ( str_data ) print ( type ( json_data )) for entry in json_data : name = entry [ 0 ] title = entry [ 1 ] role = entry [ 2 ] print (( name , title , role )) INSERT OR IGNORE \u00b6 Now we are goin to insert the information in the different tables, for that we will use INSERT and we will add IGNORE to avoid those cases when a error appears 1 2 cur . execute ( '''INSERT OR IGNORE INTO User (name) VALUES ( ? )''' , ( name , ) ) in the previous statement execute() will execute an INSERT instruction to the \u201cUser\u201d table, to the column \u201cname\u201d, the \u201c?\u201d is a placeholder and the \u201c (name,) \u201d is a tuple that indicade that the information on the variable \u201cname\u201d is going to be place in the \u201c?\u201d, and this information will be insert in the table \u201cUser\u201d. Bellow the example of the other statement, the only remark will be the usage of REPLACE which will replace the value of what ever is in that column(s). 1 2 3 4 5 6 7 8 9 10 11 cur . execute ( '''INSERT OR IGNORE INTO User (name) VALUES ( ? )''' , ( name , ) ) cur . execute ( '''INSERT OR IGNORE INTO Course (title) VALUES ( ? )''' , ( title , ) ) cur . execute ( '''INSERT OR REPLACE INTO Member (user_id, course_id, role) VALUES ( ?, ?, ? )''' , ( user_id , course_id , role ) ) SELECT and fetchone()[0] \u00b6 now we are going to select one of the records in the database and store the first row, for this we will execute the SELECT and later use fetchone[0] to store the first record 1 2 cur . execute ( 'SELECT id FROM User WHERE name = ? ' , ( name , )) user_id = cur . fetchone ()[ 0 ] we use \u201c[0]\u201d to be sure that we will get just the first record ( fetchone() will get back just one record, to get more you can use fetchall() ) commit \u00b6 Finally to commit this changes or this addition to the database we can use the function commit() , this will commit the changes to the database and wait until is done, that is one of the reason in some case the commit is done after several changes and not after each change, since this will make the execution of the script slower. 1 conn . commit () the full script \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 import json import sqlite3 conn = sqlite3 . connect ( 'rosterdb.sqlite' ) cur = conn . cursor () # Do some setup cur . executescript ( ''' DROP TABLE IF EXISTS User; DROP TABLE IF EXISTS Member; DROP TABLE IF EXISTS Course; CREATE TABLE User ( id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, name TEXT UNIQUE ); CREATE TABLE Course ( id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, title TEXT UNIQUE ); CREATE TABLE Member ( user_id INTEGER, course_id INTEGER, role INTEGER, PRIMARY KEY (user_id, course_id) ) ''' ) fname = input ( 'Enter file name: ' ) if len ( fname ) < 1 : fname = 'roster_data_sample.json' # [ # [ \"Charley\", \"si110\", 1 ], # [ \"Mea\", \"si110\", 0 ], str_data = open ( fname ) . read () json_data = json . loads ( str_data ) print ( type ( json_data )) for entry in json_data : name = entry [ 0 ] title = entry [ 1 ] role = entry [ 2 ] print (( name , title , role )) cur . execute ( '''INSERT OR IGNORE INTO User (name) VALUES ( ? )''' , ( name , ) ) cur . execute ( 'SELECT id FROM User WHERE name = ? ' , ( name , )) user_id = cur . fetchone ()[ 0 ] cur . execute ( '''INSERT OR IGNORE INTO Course (title) VALUES ( ? )''' , ( title , ) ) cur . execute ( 'SELECT id FROM Course WHERE title = ? ' , ( title , )) course_id = cur . fetchone ()[ 0 ] cur . execute ( '''INSERT OR REPLACE INTO Member (user_id, course_id, role) VALUES ( ?, ?, ? )''' , ( user_id , course_id , role ) ) conn . commit () and an example of the JSON file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 [ [ \"Alekzander\" , \"si110\" , 1 ], [ \"Tione\" , \"si110\" , 0 ], [ \"Samy\" , \"si110\" , 0 ], [ \"Alicia\" , \"si110\" , 0 ], [ \"Kruz\" , \"si110\" , 0 ], [ \"Dillan\" , \"si110\" , 0 ], [ \"Badr\" , \"si110\" , 0 ], [ \"Murry\" , \"si110\" , 0 ], [ \"Ruslan\" , \"si110\" , 0 ], [ \"Aliesha\" , \"si110\" , 0 ], [ \"Gracielynn\" , \"si110\" , 0 ], [ \"Markus\" , \"si110\" , 0 ] ]","title":"Data bases with Python"},{"location":"Coding/Python/Databases_with_Python.html#import_connect_and_cursor","text":"To start to use SQLite with python we will need to import the library \u2018sqlite3\u2019, once imported we can start using it, first we will establish a connection with the database using sqlite3.connect() , later, to start the navigation we will need a cursor, for that we use .cursor() . 1 2 3 4 import sqlite3 conn = sqlite3 . connect ( 'name database' ) cur = conn . cursor () using as example chapter 15 of \u201cPython for everyone\u201d as an example we have 1 2 3 4 import sqlite3 conn = sqlite3 . connect ( 'rosterdb.sqlite' ) cur = conn . cursor () where rosterdb.sqlite is the name of the database","title":"Import, Connect and Cursor"},{"location":"Coding/Python/Databases_with_Python.html#executescript_and_execute","text":"there are two ways to execute SQL statements in python, one will be executescript() this will allow me to execute several SLQ statement art the same time, if this statement finish with \u201c;\u201d. The second option will be execute() this will be limited to one SQL statement.","title":"executescript() and execute()"},{"location":"Coding/Python/Databases_with_Python.html#executescript_example","text":"In the following code we will execute several SLQ statement we start with executescript() , we are goin to use \u201c'''\u201c. 1 2 3 4 5 6 import sqlite3 conn = sqlite3 . connect ( 'rosterdb.sqlite' ) cur = conn . cursor () cur . executescript ( ''' SQL statements''' ) We will delete any existing tables with the names \u201cUser\u201d, \u201cMember\u201d, \u201cCourse\u201d 1 2 3 4 5 6 7 8 9 10 import sqlite3 conn = sqlite3 . connect ( 'rosterdb.sqlite' ) cur = conn . cursor () cur . executescript ( ''' DROP TABLE IF EXISTS User; DROP TABLE IF EXISTS Member; DROP TABLE IF EXISTS Course; ''' ) Now we will create 3 tables; \u201cUser\u201d, \u201cCourse\u201d, \u201cMember\u201d. Member table contain a primary key thajt take two parameters, this is a way to link to the other 2 tables and create the many to many relationship 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import sqlite3 conn = sqlite3 . connect ( 'rosterdb.sqlite' ) cur = conn . cursor () # Do some setup cur . executescript ( ''' DROP TABLE IF EXISTS User; DROP TABLE IF EXISTS Member; DROP TABLE IF EXISTS Course; CREATE TABLE User ( id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, name TEXT UNIQUE ); CREATE TABLE Course ( id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, title TEXT UNIQUE ); CREATE TABLE Member ( user_id INTEGER, course_id INTEGER, role INTEGER, PRIMARY KEY (user_id, course_id) ) ''' )","title":"executescript() example"},{"location":"Coding/Python/Databases_with_Python.html#insert_ignore_replace_and_select","text":"In previous steps we create the shema of the database, now we need to populate this database with information, in this case we are goin to extract information from a JSON file and use it to feed the datase.","title":"INSERT, IGNORE, REPLACE and SELECT"},{"location":"Coding/Python/Databases_with_Python.html#getting_the_data_from_json","text":"First we need to import json 1 2 import json [ ... ] second we need to get the information of a JSON file, in this case we are going to use one of the JSON file examples from the book \u201cPython for everyone\u201d 1 2 3 4 5 6 7 8 9 10 11 12 [ ... ] fname = input ( 'Enter file name: ' ) if len ( fname ) < 1 : fname = 'roster_data_sample.json' # [ # [ \"Charley\", \"si110\", 1 ], # [ \"Mea\", \"si110\", 0 ], str_data = open ( fname ) . read () json_data = json . loads ( str_data ) [ ... ] Now all the information is in the variable json_data (a list) we will need though this list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [ ... ] fname = input ( 'Enter file name: ' ) if len ( fname ) < 1 : fname = 'roster_data_sample.json' # [ # [ \"Charley\", \"si110\", 1 ], # [ \"Mea\", \"si110\", 0 ], str_data = open ( fname ) . read () json_data = json . loads ( str_data ) print ( type ( json_data )) for entry in json_data : name = entry [ 0 ] title = entry [ 1 ] role = entry [ 2 ] print (( name , title , role ))","title":"Getting the data from JSON"},{"location":"Coding/Python/Databases_with_Python.html#insert_or_ignore","text":"Now we are goin to insert the information in the different tables, for that we will use INSERT and we will add IGNORE to avoid those cases when a error appears 1 2 cur . execute ( '''INSERT OR IGNORE INTO User (name) VALUES ( ? )''' , ( name , ) ) in the previous statement execute() will execute an INSERT instruction to the \u201cUser\u201d table, to the column \u201cname\u201d, the \u201c?\u201d is a placeholder and the \u201c (name,) \u201d is a tuple that indicade that the information on the variable \u201cname\u201d is going to be place in the \u201c?\u201d, and this information will be insert in the table \u201cUser\u201d. Bellow the example of the other statement, the only remark will be the usage of REPLACE which will replace the value of what ever is in that column(s). 1 2 3 4 5 6 7 8 9 10 11 cur . execute ( '''INSERT OR IGNORE INTO User (name) VALUES ( ? )''' , ( name , ) ) cur . execute ( '''INSERT OR IGNORE INTO Course (title) VALUES ( ? )''' , ( title , ) ) cur . execute ( '''INSERT OR REPLACE INTO Member (user_id, course_id, role) VALUES ( ?, ?, ? )''' , ( user_id , course_id , role ) )","title":"INSERT OR IGNORE"},{"location":"Coding/Python/Databases_with_Python.html#select_and_fetchone0","text":"now we are going to select one of the records in the database and store the first row, for this we will execute the SELECT and later use fetchone[0] to store the first record 1 2 cur . execute ( 'SELECT id FROM User WHERE name = ? ' , ( name , )) user_id = cur . fetchone ()[ 0 ] we use \u201c[0]\u201d to be sure that we will get just the first record ( fetchone() will get back just one record, to get more you can use fetchall() )","title":"SELECT and fetchone()[0]"},{"location":"Coding/Python/Databases_with_Python.html#commit","text":"Finally to commit this changes or this addition to the database we can use the function commit() , this will commit the changes to the database and wait until is done, that is one of the reason in some case the commit is done after several changes and not after each change, since this will make the execution of the script slower. 1 conn . commit ()","title":"commit"},{"location":"Coding/Python/Databases_with_Python.html#the_full_script","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 import json import sqlite3 conn = sqlite3 . connect ( 'rosterdb.sqlite' ) cur = conn . cursor () # Do some setup cur . executescript ( ''' DROP TABLE IF EXISTS User; DROP TABLE IF EXISTS Member; DROP TABLE IF EXISTS Course; CREATE TABLE User ( id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, name TEXT UNIQUE ); CREATE TABLE Course ( id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, title TEXT UNIQUE ); CREATE TABLE Member ( user_id INTEGER, course_id INTEGER, role INTEGER, PRIMARY KEY (user_id, course_id) ) ''' ) fname = input ( 'Enter file name: ' ) if len ( fname ) < 1 : fname = 'roster_data_sample.json' # [ # [ \"Charley\", \"si110\", 1 ], # [ \"Mea\", \"si110\", 0 ], str_data = open ( fname ) . read () json_data = json . loads ( str_data ) print ( type ( json_data )) for entry in json_data : name = entry [ 0 ] title = entry [ 1 ] role = entry [ 2 ] print (( name , title , role )) cur . execute ( '''INSERT OR IGNORE INTO User (name) VALUES ( ? )''' , ( name , ) ) cur . execute ( 'SELECT id FROM User WHERE name = ? ' , ( name , )) user_id = cur . fetchone ()[ 0 ] cur . execute ( '''INSERT OR IGNORE INTO Course (title) VALUES ( ? )''' , ( title , ) ) cur . execute ( 'SELECT id FROM Course WHERE title = ? ' , ( title , )) course_id = cur . fetchone ()[ 0 ] cur . execute ( '''INSERT OR REPLACE INTO Member (user_id, course_id, role) VALUES ( ?, ?, ? )''' , ( user_id , course_id , role ) ) conn . commit () and an example of the JSON file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 [ [ \"Alekzander\" , \"si110\" , 1 ], [ \"Tione\" , \"si110\" , 0 ], [ \"Samy\" , \"si110\" , 0 ], [ \"Alicia\" , \"si110\" , 0 ], [ \"Kruz\" , \"si110\" , 0 ], [ \"Dillan\" , \"si110\" , 0 ], [ \"Badr\" , \"si110\" , 0 ], [ \"Murry\" , \"si110\" , 0 ], [ \"Ruslan\" , \"si110\" , 0 ], [ \"Aliesha\" , \"si110\" , 0 ], [ \"Gracielynn\" , \"si110\" , 0 ], [ \"Markus\" , \"si110\" , 0 ] ]","title":"the full script"},{"location":"Coding/Python/Error and Exceptions.html","text":"Error and Exceptions \u00b6 Syntax errors occur when Python can\u2019t interpret our code, since we didn\u2019t follow the correct syntax for Python. These are errors you\u2019re likely to get when you make a typo, or you\u2019re first starting to learn Python. Exceptions occur when unexpected things happen during execution of a program, even if the code is syntactically correct. There are different types of built-in exceptions in Python, and you can see which exception is thrown in the error message. try statement \u00b6 We can use try statement to catch exceptions and define how to handle it, the try statement has for parts: try : this is the only mandatory clause in a try statement. this is the first block python will run (and is where we suspect can be and error). except : if Python runs into an exception while running the try block, it will jump to the except block that handle the exception ( they are different type of exception that i will mention down). else : if Python runs into no exception while running the try block, it will run the code in this block after running the try . finally : before Python leave the try block, it will run the code in the finally block. an example of the simple try will be: 1 2 3 4 5 6 7 8 while True : try : x = int ( input ( \"Enter a number: \" )) break except : print ( \"Please enter a number\" ) finally : print ( \"Attempt input\" ) so, in the previous code if the user enter other value that a valid int the except block will be run and after that the code in finally now, if the user try to stop the execution using the ctrl + c in the terminal the except block will be run, to avoid this we can select specific exceptions to be capture. Specifying Exceptions \u00b6 the exception to handle can be specify in the code, for example: 1 2 3 4 try : # some code except valueError : #some code Now, in this case the except block will catch the exception ValueErrror but not other exception. We can catch the exception KeyboardInterrupt at the same time ( this apply to other exception not this two) 1 2 3 4 try : # some code except ( ValueError , KeyboardInterrupt ): # some code in the previous case the exception are going to be handle in the same way, but if we required different response to a different exception, it can be done in the following way: 1 2 3 4 5 6 try : #some code except ValueError : #some code except KeyboardInterrupt : # some code Accessing Error Messages \u00b6 Previously we saw how to handle the error, basically how to avoid the program crash when a error appears, but we don\u2019t get information about the error, but, there is a way to display this errors, and it is as follow: 1 2 3 4 5 try : # some code except ZeroDivisionError as e : #some code print ( \"ZeroDivisionErro occurred: {}\" . format ( e )) this would print something like this: 1 ZeroDivisionError occurred: integer division or modulo by zero so in that way you can handle errors, preventing the program for crashing and at the same time get information about the error. In those case where there are not specific errors to handle, you can use a general form to access to those messages: 1 2 3 4 5 try : #some code except Exception as e : #some code print ( \"Exception occurred: {} \" . format ( e )) for more information about exception check this link","title":"Error and Exceptions"},{"location":"Coding/Python/Error and Exceptions.html#error_and_exceptions","text":"Syntax errors occur when Python can\u2019t interpret our code, since we didn\u2019t follow the correct syntax for Python. These are errors you\u2019re likely to get when you make a typo, or you\u2019re first starting to learn Python. Exceptions occur when unexpected things happen during execution of a program, even if the code is syntactically correct. There are different types of built-in exceptions in Python, and you can see which exception is thrown in the error message.","title":"Error and Exceptions"},{"location":"Coding/Python/Error and Exceptions.html#try_statement","text":"We can use try statement to catch exceptions and define how to handle it, the try statement has for parts: try : this is the only mandatory clause in a try statement. this is the first block python will run (and is where we suspect can be and error). except : if Python runs into an exception while running the try block, it will jump to the except block that handle the exception ( they are different type of exception that i will mention down). else : if Python runs into no exception while running the try block, it will run the code in this block after running the try . finally : before Python leave the try block, it will run the code in the finally block. an example of the simple try will be: 1 2 3 4 5 6 7 8 while True : try : x = int ( input ( \"Enter a number: \" )) break except : print ( \"Please enter a number\" ) finally : print ( \"Attempt input\" ) so, in the previous code if the user enter other value that a valid int the except block will be run and after that the code in finally now, if the user try to stop the execution using the ctrl + c in the terminal the except block will be run, to avoid this we can select specific exceptions to be capture.","title":"try statement"},{"location":"Coding/Python/Error and Exceptions.html#specifying_exceptions","text":"the exception to handle can be specify in the code, for example: 1 2 3 4 try : # some code except valueError : #some code Now, in this case the except block will catch the exception ValueErrror but not other exception. We can catch the exception KeyboardInterrupt at the same time ( this apply to other exception not this two) 1 2 3 4 try : # some code except ( ValueError , KeyboardInterrupt ): # some code in the previous case the exception are going to be handle in the same way, but if we required different response to a different exception, it can be done in the following way: 1 2 3 4 5 6 try : #some code except ValueError : #some code except KeyboardInterrupt : # some code","title":"Specifying Exceptions"},{"location":"Coding/Python/Error and Exceptions.html#accessing_error_messages","text":"Previously we saw how to handle the error, basically how to avoid the program crash when a error appears, but we don\u2019t get information about the error, but, there is a way to display this errors, and it is as follow: 1 2 3 4 5 try : # some code except ZeroDivisionError as e : #some code print ( \"ZeroDivisionErro occurred: {}\" . format ( e )) this would print something like this: 1 ZeroDivisionError occurred: integer division or modulo by zero so in that way you can handle errors, preventing the program for crashing and at the same time get information about the error. In those case where there are not specific errors to handle, you can use a general form to access to those messages: 1 2 3 4 5 try : #some code except Exception as e : #some code print ( \"Exception occurred: {} \" . format ( e )) for more information about exception check this link","title":"Accessing Error Messages"},{"location":"Coding/Python/Generator and Iterator.html","text":"Iterators And Generators \u00b6 Iterables are objects that can return one of their elements at a time, such as a list. Many of the built-in functions we\u2019ve used so far, like \u2018enumerate,\u2019 return an iterator. An iterator is an object that represents a stream of data. This is different from a list, which is also an iterable, but is not an iterator because it is not a stream of data. Generators are a simple way to create iterators using functions. You can also define iterators using classes, here documentation about it Here is an example of a generator function called my_range, which produces an iterator that is a stream of numbers from 0 to (x - 1). 1 2 3 4 5 def my_range ( x ): i = 0 while i < x : yield i i += 1 Notice that instead of using the return keyword, it uses yield. This allows the function to return values one at a time, and start where it left off each time it\u2019s called. This yield keyword is what differentiates a generator from a typical function. we can use the for loop to iterate over the iterator. 1 2 for x in my_range ( 5 ): print ( x ) outputs: 1 2 3 4 5 0 1 2 3 4 Another example will be the implementation of the built-in function enumerate having this: 1 2 3 4 lessons = [ \"Why Python Programming\" , \"Data Types and Operators\" , \"Control Flow\" , \"Functions\" , \"Scripting\" ] for i , lesson in my_enumerate ( lessons , 1 ): print ( \"Lesson {}: {}\" . format ( i , lesson )) We need to output: 1 2 3 4 5 Lesson 1: Why Python Programming Lesson 2: Data Types and Operators Lesson 3: Control Flow Lesson 4: Functions Lesson 5: Scripting so the code will be: 1 2 3 4 5 6 7 8 9 10 11 lessons = [ \"Why Python Programming\" , \"Data Types and Operators\" , \"Control Flow\" , \"Functions\" , \"Scripting\" ] def my_enumerate ( iterable , start = 0 ): # Implement your generator function here count = start for item in iterable : yield count , item count += 1 for i , lesson in my_enumerate ( lessons , 1 ): print ( \"Lesson {}: {}\" . format ( i , lesson )) Chunker \u00b6 If you have an iterable that is too large to fit in memory in full (e.g., when dealing with large files), being able to take and use chunks of it at a time can be very valuable. Implement a generator function, chunker , that takes in an iterable and yields a chunk of a specified size at a time. 1 2 3 4 5 6 7 def chunker ( iterable , size ): \"\"\"Yield successive chunks from iterable of length size.\"\"\" for i in range ( 0 , len ( iterable ), size ): yield iterable [ i : i + size ] for chunk in chunker ( range ( 25 ), 4 ): print ( list ( chunk )) Output: 1 2 3 4 5 6 7 [0, 1, 2, 3] [4, 5, 6, 7] [8, 9, 10, 11] [12, 13, 14, 15] [16, 17, 18, 19] [20, 21, 22, 23] [24] Why Generators? \u00b6 Generators are a lazy way to build iterables. They are useful when the fully realized list would not fit in memory, or when the cost to calculate each list element is high and you want to do it as late as possible. But they can only be iterated over once. Generator Expressions \u00b6 Here\u2019s a cool concept that combines generators and list comprehensions! You can actually create a generator in the same way you\u2019d normally write a list comprehension, except with parentheses instead of square brackets. For example: 1 2 3 sq_list = [ x ** 2 for x in range ( 10 )] # this produces a list of squares sq_iterator = ( x ** 2 for x in range ( 10 )) # this produces an iterator of squares","title":"Generator and Iterators"},{"location":"Coding/Python/Generator and Iterator.html#iterators_and_generators","text":"Iterables are objects that can return one of their elements at a time, such as a list. Many of the built-in functions we\u2019ve used so far, like \u2018enumerate,\u2019 return an iterator. An iterator is an object that represents a stream of data. This is different from a list, which is also an iterable, but is not an iterator because it is not a stream of data. Generators are a simple way to create iterators using functions. You can also define iterators using classes, here documentation about it Here is an example of a generator function called my_range, which produces an iterator that is a stream of numbers from 0 to (x - 1). 1 2 3 4 5 def my_range ( x ): i = 0 while i < x : yield i i += 1 Notice that instead of using the return keyword, it uses yield. This allows the function to return values one at a time, and start where it left off each time it\u2019s called. This yield keyword is what differentiates a generator from a typical function. we can use the for loop to iterate over the iterator. 1 2 for x in my_range ( 5 ): print ( x ) outputs: 1 2 3 4 5 0 1 2 3 4 Another example will be the implementation of the built-in function enumerate having this: 1 2 3 4 lessons = [ \"Why Python Programming\" , \"Data Types and Operators\" , \"Control Flow\" , \"Functions\" , \"Scripting\" ] for i , lesson in my_enumerate ( lessons , 1 ): print ( \"Lesson {}: {}\" . format ( i , lesson )) We need to output: 1 2 3 4 5 Lesson 1: Why Python Programming Lesson 2: Data Types and Operators Lesson 3: Control Flow Lesson 4: Functions Lesson 5: Scripting so the code will be: 1 2 3 4 5 6 7 8 9 10 11 lessons = [ \"Why Python Programming\" , \"Data Types and Operators\" , \"Control Flow\" , \"Functions\" , \"Scripting\" ] def my_enumerate ( iterable , start = 0 ): # Implement your generator function here count = start for item in iterable : yield count , item count += 1 for i , lesson in my_enumerate ( lessons , 1 ): print ( \"Lesson {}: {}\" . format ( i , lesson ))","title":"Iterators And Generators"},{"location":"Coding/Python/Generator and Iterator.html#chunker","text":"If you have an iterable that is too large to fit in memory in full (e.g., when dealing with large files), being able to take and use chunks of it at a time can be very valuable. Implement a generator function, chunker , that takes in an iterable and yields a chunk of a specified size at a time. 1 2 3 4 5 6 7 def chunker ( iterable , size ): \"\"\"Yield successive chunks from iterable of length size.\"\"\" for i in range ( 0 , len ( iterable ), size ): yield iterable [ i : i + size ] for chunk in chunker ( range ( 25 ), 4 ): print ( list ( chunk )) Output: 1 2 3 4 5 6 7 [0, 1, 2, 3] [4, 5, 6, 7] [8, 9, 10, 11] [12, 13, 14, 15] [16, 17, 18, 19] [20, 21, 22, 23] [24]","title":"Chunker"},{"location":"Coding/Python/Generator and Iterator.html#why_generators","text":"Generators are a lazy way to build iterables. They are useful when the fully realized list would not fit in memory, or when the cost to calculate each list element is high and you want to do it as late as possible. But they can only be iterated over once.","title":"Why Generators?"},{"location":"Coding/Python/Generator and Iterator.html#generator_expressions","text":"Here\u2019s a cool concept that combines generators and list comprehensions! You can actually create a generator in the same way you\u2019d normally write a list comprehension, except with parentheses instead of square brackets. For example: 1 2 3 sq_list = [ x ** 2 for x in range ( 10 )] # this produces a list of squares sq_iterator = ( x ** 2 for x in range ( 10 )) # this produces an iterator of squares","title":"Generator Expressions"},{"location":"Coding/Python/Import Local Scripts.html","text":"Import Local Scripts \u00b6 Import is helpful when working on bigger projects where you want to organize the code into multiple files and reuse the code in those files, if the python script is in the same directory of the current script we can use import follow by the name of the file , without the .py extension. 1 import useful_functions to make it easier to use we can create aliases for the different script imported 1 2 import usefull_functions as uf uf . add_five ([ 1 , 2 , 3 , 4 ]) now, in many occasions the is code in this scripts that is not useful, or not useful in a script that is calling them, so in this case we use the \u201cmain\u201d block Using a main block \u00b6 To avoid running executable statements in a script when it\u2019s imported as a module in another script, include these lines in an if __name__ == \"__main__\" block. Or alternatively, include them in a function called main() and call this in the if main block. The code inside the block main will be execute just when we are running the script in specific and not when we are colling it as module in a script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # demo.py import useful_functions as uf scores = [ 88 , 92 , 79 , 93 , 85 ] mean = uf . mean ( scores ) curved = uf . add_five ( scores ) mean_c = uf . mean ( curved ) print ( \"Scores:\" , scores ) print ( \"Original Mean:\" , mean , \" New Mean:\" , mean_c ) print ( __name__ ) print ( uf . __name__ ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # useful_functions.py def mean ( num_list ): return sum ( num_list ) / len ( num_list ) def add_five ( num_list ): return [ n + 5 for n in num_list ] def main (): print ( \"Testing mean function\" ) n_list = [ 34 , 44 , 23 , 46 , 12 , 24 ] correct_mean = 30.5 assert ( mean ( n_list ) == correct_mean ) print ( \"Testing add_five function\" ) correct_list = [ 39 , 49 , 28 , 51 , 17 , 29 ] assert ( add_five ( n_list ) == correct_list ) print ( \"All tests passed!\" ) if __name__ == '__main__' : main () Most common use Python Standard Library modules \u00b6 The Python Standard Library has a lot of modules, here are a selection of our Python Standard Library modules and why we use them! csv : very convenient for reading and writing csv files collections : useful extensions of the usual data types including OrderedDict, defaultdict and namedtuple random : generates pseudo-random numbers, shuffles sequences randomly and chooses random items string : more functions on strings. This module also contains useful collections of letters like string.digits (a string containing all characters which are valid digits). re : pattern-matching in strings via regular expressions math : some standard mathematical functions os : interacting with operating systems os.path : submodule of os for manipulating path names sys : work directly with the Python interpreter json : good for reading and writing json files (good for web work) Techniques for Importing Modules \u00b6 there are some variants of import that are useful in different situation. To import an individual function or class: 1 from module_name import object_name To import multiple individual object from a module: 1 from module_name import firts_object , second_object To rename a module: 1 import module_name as new_name To import an object from a module and rename it: 1 from module_name import object_name as new_name To import everything 1 import module_name Modules, packages, and Names \u00b6 In order to manage the coder better, the modules of python are contain in package , this package is just a container for the modules, to call the sub-module from one package we use the dot notation: 1 import package_name.submodule_name Third-party packages \u00b6 normally in a project you will find a document call requirements.txt which is a list that contain all the modules, packages and its version us in the specific script, the requirements.txt file looks like: 1 2 3 4 beautifulsoup4==4.5.1 bs4==0.0.1 pytz==2016.7 requests==2.11.1 to install all the packages on this file we can use pip, the command will be: 1 pip install -r requirements.txt Some of the popular third-party packages are: IPython - A better interactive Python interpreter requests - Provides easy to use methods to make web requests. Useful for accessing web APIs. Flask - a lightweight framework for making web applications and APIs. Django - A more featureful framework for making web applications. Django is particularly good for designing complex, content heavy, web applications. Beautiful Soup - Used to parse HTML and extract information from it. Great for web scraping. pytest - extends Python\u2019s builtin assertions and unittest module. PyYAML - For reading and writing YAML files. NumPy - The fundamental package for scientific computing with Python. It contains among other things a powerful N-dimensional array object and useful linear algebra capabilities. pandas - A library containing high-performance, data structures and data analysis tools. In particular, pandas provides dataframes! matplotlib - a 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments. ggplot - Another 2D plotting library, based on R\u2019s ggplot2 library. Pillow - The Python Imaging Library adds image processing capabilities to your Python interpreter. pyglet - A cross-platform application framework intended for game development. Pygame - A set of Python modules designed for writing games. pytz - World Timezone Definitions for Python","title":"Import Local Scripts"},{"location":"Coding/Python/Import Local Scripts.html#import_local_scripts","text":"Import is helpful when working on bigger projects where you want to organize the code into multiple files and reuse the code in those files, if the python script is in the same directory of the current script we can use import follow by the name of the file , without the .py extension. 1 import useful_functions to make it easier to use we can create aliases for the different script imported 1 2 import usefull_functions as uf uf . add_five ([ 1 , 2 , 3 , 4 ]) now, in many occasions the is code in this scripts that is not useful, or not useful in a script that is calling them, so in this case we use the \u201cmain\u201d block","title":"Import Local Scripts"},{"location":"Coding/Python/Import Local Scripts.html#using_a_main_block","text":"To avoid running executable statements in a script when it\u2019s imported as a module in another script, include these lines in an if __name__ == \"__main__\" block. Or alternatively, include them in a function called main() and call this in the if main block. The code inside the block main will be execute just when we are running the script in specific and not when we are colling it as module in a script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # demo.py import useful_functions as uf scores = [ 88 , 92 , 79 , 93 , 85 ] mean = uf . mean ( scores ) curved = uf . add_five ( scores ) mean_c = uf . mean ( curved ) print ( \"Scores:\" , scores ) print ( \"Original Mean:\" , mean , \" New Mean:\" , mean_c ) print ( __name__ ) print ( uf . __name__ ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # useful_functions.py def mean ( num_list ): return sum ( num_list ) / len ( num_list ) def add_five ( num_list ): return [ n + 5 for n in num_list ] def main (): print ( \"Testing mean function\" ) n_list = [ 34 , 44 , 23 , 46 , 12 , 24 ] correct_mean = 30.5 assert ( mean ( n_list ) == correct_mean ) print ( \"Testing add_five function\" ) correct_list = [ 39 , 49 , 28 , 51 , 17 , 29 ] assert ( add_five ( n_list ) == correct_list ) print ( \"All tests passed!\" ) if __name__ == '__main__' : main ()","title":"Using a main block"},{"location":"Coding/Python/Import Local Scripts.html#most_common_use_python_standard_library_modules","text":"The Python Standard Library has a lot of modules, here are a selection of our Python Standard Library modules and why we use them! csv : very convenient for reading and writing csv files collections : useful extensions of the usual data types including OrderedDict, defaultdict and namedtuple random : generates pseudo-random numbers, shuffles sequences randomly and chooses random items string : more functions on strings. This module also contains useful collections of letters like string.digits (a string containing all characters which are valid digits). re : pattern-matching in strings via regular expressions math : some standard mathematical functions os : interacting with operating systems os.path : submodule of os for manipulating path names sys : work directly with the Python interpreter json : good for reading and writing json files (good for web work)","title":"Most common use Python Standard Library modules"},{"location":"Coding/Python/Import Local Scripts.html#techniques_for_importing_modules","text":"there are some variants of import that are useful in different situation. To import an individual function or class: 1 from module_name import object_name To import multiple individual object from a module: 1 from module_name import firts_object , second_object To rename a module: 1 import module_name as new_name To import an object from a module and rename it: 1 from module_name import object_name as new_name To import everything 1 import module_name","title":"Techniques for Importing Modules"},{"location":"Coding/Python/Import Local Scripts.html#modules_packages_and_names","text":"In order to manage the coder better, the modules of python are contain in package , this package is just a container for the modules, to call the sub-module from one package we use the dot notation: 1 import package_name.submodule_name","title":"Modules, packages, and Names"},{"location":"Coding/Python/Import Local Scripts.html#third-party_packages","text":"normally in a project you will find a document call requirements.txt which is a list that contain all the modules, packages and its version us in the specific script, the requirements.txt file looks like: 1 2 3 4 beautifulsoup4==4.5.1 bs4==0.0.1 pytz==2016.7 requests==2.11.1 to install all the packages on this file we can use pip, the command will be: 1 pip install -r requirements.txt Some of the popular third-party packages are: IPython - A better interactive Python interpreter requests - Provides easy to use methods to make web requests. Useful for accessing web APIs. Flask - a lightweight framework for making web applications and APIs. Django - A more featureful framework for making web applications. Django is particularly good for designing complex, content heavy, web applications. Beautiful Soup - Used to parse HTML and extract information from it. Great for web scraping. pytest - extends Python\u2019s builtin assertions and unittest module. PyYAML - For reading and writing YAML files. NumPy - The fundamental package for scientific computing with Python. It contains among other things a powerful N-dimensional array object and useful linear algebra capabilities. pandas - A library containing high-performance, data structures and data analysis tools. In particular, pandas provides dataframes! matplotlib - a 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments. ggplot - Another 2D plotting library, based on R\u2019s ggplot2 library. Pillow - The Python Imaging Library adds image processing capabilities to your Python interpreter. pyglet - A cross-platform application framework intended for game development. Pygame - A set of Python modules designed for writing games. pytz - World Timezone Definitions for Python","title":"Third-party packages"},{"location":"Coding/Python/Reading and Writing Files.html","text":"Reading and Writing Files \u00b6 Python has the ability to open, create, write and \u201cmodify\u201d files, write and modify has some point to take in count that we will address later. Reading a File \u00b6 To read a file you will need to: Open the file with the built-in function open , for this we will need a string with the location or path to this file, this will return an object file, this object is what Python use to interact with the document. There are optional parameters you can specify in the open function. One is the mode in which we open the file. if we use r ( which is the value by default) the mode will be read only. Use the read method to access the contents from the file object. this read method takes the text content in the file and puts into a string. ( IMPORTANT ) when finished with the file, use the close method to free up any system resources taken up by the file. the code will be something like: 1 2 3 f = open ( 'my_path/my_file.txt' , 'r' ) file_data = f . read () f . close () Writing to File \u00b6 Now, to write we need to the open the file in write mode, but it is important to know that this will remove any previous content in the document. Open the file in writing (\u2018w\u2019) mode. if the file does not exist, Python will create it for you, if the file exist all the content will be delete, if the intention is to add information to this document we will need to open it in append mode (\u2018a\u2019). Use the write method to add text to the file. Close the file when finished. so the code will look like: 1 2 3 f = open ( 'my_path/my_file.txt' , 'w' ) f . write ( \"Hello there!\" ) f . close () Now it is important to close the files after use them, but in some case can be easy to forget to close the files, in that case we can use the block with , we will be able to access the file within this block and after we finish the file will be close automatically, example: 1 2 with open ( 'my_path/my_file.txt' , 'r' ) as f : file_data = f . data () so in this case the as f is equal to f = open('my_path/my_file.txt','r') we can access the file using f within the indented with block and we don\u2019t need to use f.close() . About Reading the Files \u00b6 By default the read() method has no argument and it gives back the content of the file, but, we can use integer argument, this will give back the number of characters pass as argument and leave the window open, this will be easy to understand in a example: let assume the document camelot.txt contain: 1 2 We're the knights of the round table We dance whenever we're able and we have the following code: 1 2 3 4 with open ( \"camelot.txt\" ) as song : print ( song . read ( 2 )) print ( song . read ( 8 )) print ( song . read ()) the output will be: 1 2 3 4 We 're the knights of the round table We dance whenever we're able this will be really difficult to know in which position are we after each read() statement. Now, python is smart enough to understand that \\n which is a character that means newline means that the sentences finish and there is a new line. For this cases we can use readline() . although there is a way to loop over the lines of the file, this can be done with the syntax for line in file and using the .strip() we can remove the \\n . 1 2 3 4 5 6 camelot_lines = [] with open ( \"camelot.txt\" ) as f : for line in f : camelot_lines . append ( line . strip ()) print ( camelot_lines ) the output will be: 1 [\"We're the knights of the round table\", \"We dance whenever we're able\"]","title":"Reading and Writing Files"},{"location":"Coding/Python/Reading and Writing Files.html#reading_and_writing_files","text":"Python has the ability to open, create, write and \u201cmodify\u201d files, write and modify has some point to take in count that we will address later.","title":"Reading and Writing Files"},{"location":"Coding/Python/Reading and Writing Files.html#reading_a_file","text":"To read a file you will need to: Open the file with the built-in function open , for this we will need a string with the location or path to this file, this will return an object file, this object is what Python use to interact with the document. There are optional parameters you can specify in the open function. One is the mode in which we open the file. if we use r ( which is the value by default) the mode will be read only. Use the read method to access the contents from the file object. this read method takes the text content in the file and puts into a string. ( IMPORTANT ) when finished with the file, use the close method to free up any system resources taken up by the file. the code will be something like: 1 2 3 f = open ( 'my_path/my_file.txt' , 'r' ) file_data = f . read () f . close ()","title":"Reading a File"},{"location":"Coding/Python/Reading and Writing Files.html#writing_to_file","text":"Now, to write we need to the open the file in write mode, but it is important to know that this will remove any previous content in the document. Open the file in writing (\u2018w\u2019) mode. if the file does not exist, Python will create it for you, if the file exist all the content will be delete, if the intention is to add information to this document we will need to open it in append mode (\u2018a\u2019). Use the write method to add text to the file. Close the file when finished. so the code will look like: 1 2 3 f = open ( 'my_path/my_file.txt' , 'w' ) f . write ( \"Hello there!\" ) f . close () Now it is important to close the files after use them, but in some case can be easy to forget to close the files, in that case we can use the block with , we will be able to access the file within this block and after we finish the file will be close automatically, example: 1 2 with open ( 'my_path/my_file.txt' , 'r' ) as f : file_data = f . data () so in this case the as f is equal to f = open('my_path/my_file.txt','r') we can access the file using f within the indented with block and we don\u2019t need to use f.close() .","title":"Writing to File"},{"location":"Coding/Python/Reading and Writing Files.html#about_reading_the_files","text":"By default the read() method has no argument and it gives back the content of the file, but, we can use integer argument, this will give back the number of characters pass as argument and leave the window open, this will be easy to understand in a example: let assume the document camelot.txt contain: 1 2 We're the knights of the round table We dance whenever we're able and we have the following code: 1 2 3 4 with open ( \"camelot.txt\" ) as song : print ( song . read ( 2 )) print ( song . read ( 8 )) print ( song . read ()) the output will be: 1 2 3 4 We 're the knights of the round table We dance whenever we're able this will be really difficult to know in which position are we after each read() statement. Now, python is smart enough to understand that \\n which is a character that means newline means that the sentences finish and there is a new line. For this cases we can use readline() . although there is a way to loop over the lines of the file, this can be done with the syntax for line in file and using the .strip() we can remove the \\n . 1 2 3 4 5 6 camelot_lines = [] with open ( \"camelot.txt\" ) as f : for line in f : camelot_lines . append ( line . strip ()) print ( camelot_lines ) the output will be: 1 [\"We're the knights of the round table\", \"We dance whenever we're able\"]","title":"About Reading the Files"},{"location":"Coding/Python/Tricks_tips_better_code_python.html","text":"This notes are base in the Medium post called \u201cfive Python tricks you need to learn today\u201d TIp 1: Clean - Powerful One-liners \u00b6 Conditional statements \u00b6 A normal If conditional will look like this: 1 2 3 4 5 6 if alpha > 7 : beta = 999 elif alpha == 7 : beta = 99 else : beta = 0 but this can be one line, can be simplify int his way: 1 beta == 999 if alpha > 7 else 99 if alpha == 7 else 0 for loops \u00b6 for example, doubling a list of integers in four lines: 1 2 3 4 lst = [ 1 , 3 , 5 ] doubled = [] for num in lst : doubled . append ( num * 2 ) and it can be simplify to just one line: 1 double = [ num * 2 for num in lst ] Tip 2: String Manipulation \u00b6 Reverse a string \u00b6 we can use ::-1 to reverse a string, like this: 1 2 3 a = \"ilovemyjob\" print a [:: - 1 ] #bojymevoli join strings \u00b6 we can print the result of join different strings, or item of a list together: let say we have this: 1 2 3 str1 = \"Totally\" str2 = \"Awesome\" lst3 = [ \"Omg\" , \"You\" , \"Are\" ] so we can use join() method to create the outcome: 1 2 3 4 print ' ' . join ( lst3 ) #Omg You Are print ' ' . join ( lst3 ) + ' ' + str1 + ' ' + str2 #Omg You Are Totally Awesome","title":"Trick and tips for better code in python"},{"location":"Coding/Python/Tricks_tips_better_code_python.html#tip_1_clean_-_powerful_one-liners","text":"","title":"TIp 1: Clean - Powerful One-liners"},{"location":"Coding/Python/Tricks_tips_better_code_python.html#conditional_statements","text":"A normal If conditional will look like this: 1 2 3 4 5 6 if alpha > 7 : beta = 999 elif alpha == 7 : beta = 99 else : beta = 0 but this can be one line, can be simplify int his way: 1 beta == 999 if alpha > 7 else 99 if alpha == 7 else 0","title":"Conditional statements"},{"location":"Coding/Python/Tricks_tips_better_code_python.html#for_loops","text":"for example, doubling a list of integers in four lines: 1 2 3 4 lst = [ 1 , 3 , 5 ] doubled = [] for num in lst : doubled . append ( num * 2 ) and it can be simplify to just one line: 1 double = [ num * 2 for num in lst ]","title":"for loops"},{"location":"Coding/Python/Tricks_tips_better_code_python.html#tip_2_string_manipulation","text":"","title":"Tip 2: String Manipulation"},{"location":"Coding/Python/Tricks_tips_better_code_python.html#reverse_a_string","text":"we can use ::-1 to reverse a string, like this: 1 2 3 a = \"ilovemyjob\" print a [:: - 1 ] #bojymevoli","title":"Reverse a string"},{"location":"Coding/Python/Tricks_tips_better_code_python.html#join_strings","text":"we can print the result of join different strings, or item of a list together: let say we have this: 1 2 3 str1 = \"Totally\" str2 = \"Awesome\" lst3 = [ \"Omg\" , \"You\" , \"Are\" ] so we can use join() method to create the outcome: 1 2 3 4 print ' ' . join ( lst3 ) #Omg You Are print ' ' . join ( lst3 ) + ' ' + str1 + ' ' + str2 #Omg You Are Totally Awesome","title":"join strings"},{"location":"Coding/Python/Useful or researched built-in functions.html","text":"Useful functions or Statements \u00b6 reduce() from functools \u00b6 The reduce(fun,seq) function is used to apply a particular function passed in its argument to all of the list elements mentioned in the sequence passed along. This function is defined in \u201cfunctools\u201d module. Working : First two elements of sequence are picked and the result is obtained. Next step is to apply the same function to the previously attained result and the number just succeeding the second element and the result is again stored. This process continues till no more elements are left in the container. The final returned result is returned and printed on console. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import sys from math import gcd from functools import reduce n , m = input () . strip () . split ( ' ' ) n , m = [ int ( n ), int ( m )] A = map ( int , input () . strip () . split ( ' ' )) B = map ( int , input () . strip () . split ( ' ' )) def LCM ( a , b ): return ( a * b ) // gcd ( a , b ) lcm = reduce ( LCM , A , 1 ) gcd = reduce ( gcd , B ) lcm_copy = lcm count = 0 while lcm <= gcd : if ( gcd % lcm ) == 0 : count += 1 lcm = lcm + lcm_copy print ( count ) in the previous code the idea to solve the problem Between Two Sets here more information about GCD and LCM GCD() from math \u00b6 The Highest Common Factor (HCF) , also called gcd , can be computed in python using a single function offered by math module and hence can make tasks easier in many situations. 1 2 3 4 5 6 7 8 # Python code to demonstrate gcd() # method to compute gcd import math # prints 12 print ( \"The gcd of 60 and 48 is : \" , end = \"\" ) print ( math . gcd ( 60 , 48 )) Counter() from collection \u00b6 A Counter is a dict subclass for counting hashable objects. It is a collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. for example: assume an array arr=[1,1,2,2,3] and you are asked to find the number of occurrence of each integer, in this case you can use Counter whihc will return a dictionary using the value as key and the count as value, like this: 1 2 3 4 5 6 from collection import Counter arr = [ 1 , 1 , 2 , 2 , 3 ] dict_arr = Counter ( arr ) print ( dict_arr ) the result will be: 1 {1:2, 2:2, 3:1} iter() from the build-in functions \u00b6 Before start to talk about iter() better start by remembering what is a Iterator and what is the difference with iterable Python Iterators \u00b6 An iterator is an object that contains a countable number of values, and it can be iterated upon, meaning that you can traverse through all the values. Technically, in Python, an iterator is an object which implements the iterator protocol, which consist of the methods iter () and next (). Iterator vs Iterable \u00b6 Lists, tuples, dictionaries, and sets are all iterable objects. They are iterable containers which you can get an iterator from. All these objects have a iter() method which is used to get an iterator: 1 2 3 4 5 6 mytuple = ( \"apple\" , \"banana\" , \"cherry\" ) myit = iter ( mytuple ) print ( next ( myit )) print ( next ( myit )) print ( next ( myit )) it will output: 1 2 3 apple banana cherry setdefault() method from Dictionaries \u00b6 The setdefault() method returns the value of the item with the specified key. If the key does not exist, insert the key, with the specified value, see example below Syntax 1 dictionary.setdefault(keyname,value) Example 1 2 3 4 5 6 7 8 9 car = { \"brand\" : \"Ford\" , \"model\" : \"Mustang\" , \"year\" : 1964 } x = car . setdefault ( \"color\" , \"white\" ) print ( x . color ) the output will be: 1 White map() Built-in function \u00b6 Basic Syntax 1 map ( functions_object , iterable1 , iterable2 , ... ) map functions expects a function object and any number of iterables like list, dictionary, etc. It executes the function_object for each element in the sequence and returns a list of the elements modified by the function object. Example: 1 2 3 4 def multiply2 ( x ): return x * 2 map ( multiply2 , [ 1 , 2 , 3 , 4 ]) # Output [2, 4, 6, 8] In the above example, map executes multiply2 function for each element in the list i.e. 1, 2, 3, 4 and returns [2, 4, 6, 8] Let\u2019s see how we can write the above code using map and lambda. 1 map ( lambda x : x * 2 , [ 1 , 2 , 3 , 4 ]) #Output [2, 4, 6, 8] We can pass multiple sequences to the map functions as shown below: 1 2 3 4 list_a = [ 1 , 2 , 3 ] list_b = [ 10 , 20 , 30 ] map ( lambda x , y : x + y , list_a , list_b ) # Output: [11, 22, 33] Neither we can access the elements of the map object with index nor we can use len() to find the length of the map object We can force convert the map output i.e. the map object to list as shown below: 1 2 3 4 5 6 map_output = map ( lambda x : x * 2 , [ 1 , 2 , 3 , 4 ]) print ( map_output ) # Output: map object: <map object at 0x04D6BAB0> list_map_output = list ( map_output ) print ( list_map_output ) # Output: [2, 4, 6, 8] filter() Built-in function \u00b6 Basic Syntax 1 filter ( function_object , iterable ) filter function expects two arguments, function_object and an iterable. function_object returns a boolean value. function_object is called for each element of the iterable and filter returns only those element for which the function_object returns true. Like map function, filter function also returns a list of element. Unlike map function filter function can only have one iterable as input. Example: Even number using filter function 1 2 a = [ 1 , 2 , 3 , 4 , 5 , 6 ] filter ( lambda x : x % 2 == 0 , a ) # Output: [2, 4, 6] Similar to map, filter function in Python3 returns a filter object or the iterator which gets lazily evaluated. Neither we can access the elements of the filter object with index nor we can use len() to find the length of the filter object. 1 2 3 4 5 6 7 list_a = [ 1 , 2 , 3 , 4 , 5 ] filter_obj = filter ( lambda x : x % 2 == 0 , list_a ) # filter object <filter at 0x4e45890> even_num = list ( filter_obj ) # Converts the filer obj to a list print ( even_num ) # Output: [2, 4] pass Statement \u00b6 The pass statement does nothing. It can be used when a statement is required syntactically but the program requires no action. For example: 1 2 while True : pass # Busy-wait for keyboard interrupt (Ctrl+C) This is commonly used for creating minimal classes: 1 2 class MyEmptyClass : pass Another place pass can be used is as a place-holder for a function or conditional body when you are working on new code, allowing you to keep thinking at a more abstract level. The pass is silently ignored: 1 2 def initlog ( * args ): pass # Remember to implement this!","title":"Useful or researched built-in functions"},{"location":"Coding/Python/Useful or researched built-in functions.html#useful_functions_or_statements","text":"","title":"Useful functions or Statements"},{"location":"Coding/Python/Useful or researched built-in functions.html#reduce_from_functools","text":"The reduce(fun,seq) function is used to apply a particular function passed in its argument to all of the list elements mentioned in the sequence passed along. This function is defined in \u201cfunctools\u201d module. Working : First two elements of sequence are picked and the result is obtained. Next step is to apply the same function to the previously attained result and the number just succeeding the second element and the result is again stored. This process continues till no more elements are left in the container. The final returned result is returned and printed on console. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import sys from math import gcd from functools import reduce n , m = input () . strip () . split ( ' ' ) n , m = [ int ( n ), int ( m )] A = map ( int , input () . strip () . split ( ' ' )) B = map ( int , input () . strip () . split ( ' ' )) def LCM ( a , b ): return ( a * b ) // gcd ( a , b ) lcm = reduce ( LCM , A , 1 ) gcd = reduce ( gcd , B ) lcm_copy = lcm count = 0 while lcm <= gcd : if ( gcd % lcm ) == 0 : count += 1 lcm = lcm + lcm_copy print ( count ) in the previous code the idea to solve the problem Between Two Sets here more information about GCD and LCM","title":"reduce() from functools"},{"location":"Coding/Python/Useful or researched built-in functions.html#gcd_from_math","text":"The Highest Common Factor (HCF) , also called gcd , can be computed in python using a single function offered by math module and hence can make tasks easier in many situations. 1 2 3 4 5 6 7 8 # Python code to demonstrate gcd() # method to compute gcd import math # prints 12 print ( \"The gcd of 60 and 48 is : \" , end = \"\" ) print ( math . gcd ( 60 , 48 ))","title":"GCD() from math"},{"location":"Coding/Python/Useful or researched built-in functions.html#counter_from_collection","text":"A Counter is a dict subclass for counting hashable objects. It is a collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. for example: assume an array arr=[1,1,2,2,3] and you are asked to find the number of occurrence of each integer, in this case you can use Counter whihc will return a dictionary using the value as key and the count as value, like this: 1 2 3 4 5 6 from collection import Counter arr = [ 1 , 1 , 2 , 2 , 3 ] dict_arr = Counter ( arr ) print ( dict_arr ) the result will be: 1 {1:2, 2:2, 3:1}","title":"Counter() from collection"},{"location":"Coding/Python/Useful or researched built-in functions.html#iter_from_the_build-in_functions","text":"Before start to talk about iter() better start by remembering what is a Iterator and what is the difference with iterable","title":"iter() from the build-in functions"},{"location":"Coding/Python/Useful or researched built-in functions.html#python_iterators","text":"An iterator is an object that contains a countable number of values, and it can be iterated upon, meaning that you can traverse through all the values. Technically, in Python, an iterator is an object which implements the iterator protocol, which consist of the methods iter () and next ().","title":"Python Iterators"},{"location":"Coding/Python/Useful or researched built-in functions.html#iterator_vs_iterable","text":"Lists, tuples, dictionaries, and sets are all iterable objects. They are iterable containers which you can get an iterator from. All these objects have a iter() method which is used to get an iterator: 1 2 3 4 5 6 mytuple = ( \"apple\" , \"banana\" , \"cherry\" ) myit = iter ( mytuple ) print ( next ( myit )) print ( next ( myit )) print ( next ( myit )) it will output: 1 2 3 apple banana cherry","title":"Iterator vs Iterable"},{"location":"Coding/Python/Useful or researched built-in functions.html#setdefault_method_from_dictionaries","text":"The setdefault() method returns the value of the item with the specified key. If the key does not exist, insert the key, with the specified value, see example below Syntax 1 dictionary.setdefault(keyname,value) Example 1 2 3 4 5 6 7 8 9 car = { \"brand\" : \"Ford\" , \"model\" : \"Mustang\" , \"year\" : 1964 } x = car . setdefault ( \"color\" , \"white\" ) print ( x . color ) the output will be: 1 White","title":"setdefault() method from Dictionaries"},{"location":"Coding/Python/Useful or researched built-in functions.html#map_built-in_function","text":"Basic Syntax 1 map ( functions_object , iterable1 , iterable2 , ... ) map functions expects a function object and any number of iterables like list, dictionary, etc. It executes the function_object for each element in the sequence and returns a list of the elements modified by the function object. Example: 1 2 3 4 def multiply2 ( x ): return x * 2 map ( multiply2 , [ 1 , 2 , 3 , 4 ]) # Output [2, 4, 6, 8] In the above example, map executes multiply2 function for each element in the list i.e. 1, 2, 3, 4 and returns [2, 4, 6, 8] Let\u2019s see how we can write the above code using map and lambda. 1 map ( lambda x : x * 2 , [ 1 , 2 , 3 , 4 ]) #Output [2, 4, 6, 8] We can pass multiple sequences to the map functions as shown below: 1 2 3 4 list_a = [ 1 , 2 , 3 ] list_b = [ 10 , 20 , 30 ] map ( lambda x , y : x + y , list_a , list_b ) # Output: [11, 22, 33] Neither we can access the elements of the map object with index nor we can use len() to find the length of the map object We can force convert the map output i.e. the map object to list as shown below: 1 2 3 4 5 6 map_output = map ( lambda x : x * 2 , [ 1 , 2 , 3 , 4 ]) print ( map_output ) # Output: map object: <map object at 0x04D6BAB0> list_map_output = list ( map_output ) print ( list_map_output ) # Output: [2, 4, 6, 8]","title":"map() Built-in function"},{"location":"Coding/Python/Useful or researched built-in functions.html#filter_built-in_function","text":"Basic Syntax 1 filter ( function_object , iterable ) filter function expects two arguments, function_object and an iterable. function_object returns a boolean value. function_object is called for each element of the iterable and filter returns only those element for which the function_object returns true. Like map function, filter function also returns a list of element. Unlike map function filter function can only have one iterable as input. Example: Even number using filter function 1 2 a = [ 1 , 2 , 3 , 4 , 5 , 6 ] filter ( lambda x : x % 2 == 0 , a ) # Output: [2, 4, 6] Similar to map, filter function in Python3 returns a filter object or the iterator which gets lazily evaluated. Neither we can access the elements of the filter object with index nor we can use len() to find the length of the filter object. 1 2 3 4 5 6 7 list_a = [ 1 , 2 , 3 , 4 , 5 ] filter_obj = filter ( lambda x : x % 2 == 0 , list_a ) # filter object <filter at 0x4e45890> even_num = list ( filter_obj ) # Converts the filer obj to a list print ( even_num ) # Output: [2, 4]","title":"filter() Built-in function"},{"location":"Coding/Python/Useful or researched built-in functions.html#pass_statement","text":"The pass statement does nothing. It can be used when a statement is required syntactically but the program requires no action. For example: 1 2 while True : pass # Busy-wait for keyboard interrupt (Ctrl+C) This is commonly used for creating minimal classes: 1 2 class MyEmptyClass : pass Another place pass can be used is as a place-holder for a function or conditional body when you are working on new code, allowing you to keep thinking at a more abstract level. The pass is silently ignored: 1 2 def initlog ( * args ): pass # Remember to implement this!","title":"pass Statement"},{"location":"Coding/Python/Useful_snippets.html","text":"1. All Unique \u00b6 This method check if there is any duplicate element 1 2 3 4 5 6 7 def all_unique ( lst ): return len ( str ) == len ( set ( lst )) x = [ 1 , 1 , 2 , 2 , 3 , 2 , 3 , 4 , 5 , 6 ] y = [ 1 , 2 , 3 , 4 , 5 ] all_unique ( x ) # False all_unique ( y ) # True 2. Anagrams \u00b6 Check if a string is an anagram 1 2 3 4 5 6 from collections import Counter def anagram ( first , second ): return Counter ( first ) == Counter ( second ) anagram ( \"abcd3\" , \"3acdb\" ) #True 3. Memory \u00b6 This is use to check the memory usage of an object 1 2 3 4 import sys variable = 30 print ( sys . getsizeof ( variable )) # 24 4. Byte size \u00b6 This Method return the length of a string in bytes 1 2 3 4 5 def byte_size ( string ): return ( len ( String . encode ( 'utf-8' ))) byte_size ( '\ud83d\ude00' ) # 4 byte_size ( 'Hello World' ) # 11 5. Print a String N times \u00b6 This will print a string N times 1 2 3 4 n = 2 ; s = 'programing' print ( s * n ) # ProgrammingProgramming 6. Capitalize first letters \u00b6 1 2 3 s = \"programming is awesome\" print ( s . title ()) # Programming Is Awesome 7. Chunk \u00b6 This method chunk a list in a smaller list of specific size 1 2 3 4 5 6 from math import ceil def chunk ( lst , size ): return list ( map ( lambda x : lst [ x * size : x * size + size ], list ( range ( 0 , ceil ( len ( lst ) / size ))))) chunk ([ 1 , 2 , 3 , 4 , 5 ], 2 ) # [[1,2],[3,4],5] 8. Compact \u00b6 This method remove the \u201cFalsy\u201d values, in other words, False, 0, None,\u201d\u201c. 1 2 3 4 def compact ( lst ): return list ( filter ( bool , lst )) compact ([ 0 , 1 , False , 2 , '' , 3 , 'a' , 's' , 34 ]) # [ 1, 2, 3, 'a', 's', 34 ] 9. Count by \u00b6 This use ZIP() method to transpose a 2D array 1 2 3 array = [[ 'a' . 'b' ],[ 'c' , 'd' ],[ 'e' . 'f' ]] transposed = zip ( * array ) print ( transposed ) # [('a', 'c', 'e'), ('b', 'd', 'f')] 10. chained comparison \u00b6 multiple comparison in a single line 1 2 3 a = 3 print ( 2 < a < 8 ) # True print ( 1 == a < 2 ) # False 11. Comma-separated \u00b6 This snippet use the method .join() turn a list of strings into a single string with each element from the list separated by commas. 1 2 3 hobbies = [ \"basketball\" , \"football\" , \"swimming\" ] print ( \"My hobbies are: \" + \", \" . join ( hobbies )) # My hobbies are: basketball, football, swimming 12. Count vowels \u00b6 This method counts the number of vowels (\u2018a\u2019, \u2018e\u2019, \u2018i\u2019, \u2018o\u2019, \u2018u\u2019) found in a string, using regular expressions. 1 2 3 4 5 6 7 8 import re def count_vowels ( str ): return len ( len ( re . findall ( r '[aeiou]' , str , re . IGNORECASE ))) count_vowels ( 'foobar' ) # 3 count_vowels ( 'gym' ) # 0 13. Decapitalize \u00b6 This method can be used to turn the first letter of the given string into lowercase. 1 2 3 4 5 6 def decapitalize ( string ): return str [: 1 ] . lower () + str [ 1 :] decapitalize ( 'FooBar' ) # 'fooBar' decapitalize ( 'FooBar' ) # 'fooBar' 14. Flatten \u00b6 The following methods flatten a potentially deep list using recursion. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def spread ( arg ): ret = [] for i in arg : if isinstance ( i , list ): ret . extend ( i ) else : ret . append ( i ) return ret def deep_flatten ( lst ): result = [] result . extend ( spread ( list ( map ( lambda x : deep_flatten ( x ) if type ( x ) == list else x , lst )))) return result deep_flatten ([ 1 , [ 2 ], [[ 3 ], 4 ], 5 ]) # [1,2,3,4,5] 15. Difference \u00b6 This method finds the difference between two iterables and display just the different number in the first iterable 1 2 3 4 5 6 7 8 def difference ( a , b ): set_a = set ( a ) set_b = set ( b ) comparison = set_a . difference ( set_b ) return list ( comparison ) difference ([ 1 , 2 , 3 ], [ 1 , 2 , 4 ]) # [3] 16. Difference by \u00b6 The following method returns the difference between two lists after applying a given function to each element of both lists. 1 2 3 4 5 6 7 8 def difference_by ( a , b , fn ): b = set ( map ( fn , b )) return [ item for item in a if fn ( item ) not in b ] from math import floor difference_by ([ 2.1 , 1.2 ], [ 2.3 , 3.4 ], floor ) # [1.2] difference_by ([{ 'x' : 2 }, { 'x' : 1 }], [{ 'x' : 1 }], lambda v : v [ 'x' ]) # [ { x: 2 } ] 17. Chained function call \u00b6 You can call multiple functions inside a single line. 1 2 3 4 5 6 7 8 def add ( a , b ): return a + b def subtract ( a , b ): return a - b a , b = 4 , 5 print (( subtract if a > b else add )( a , b )) # 9 18. Has duplicates \u00b6 The following method checks whether a list has duplicate values by using the fact that set() contains only unique elements. 1 2 3 4 5 6 7 8 def has_duplicates ( lst ): return len ( lst ) != len ( set ( lst )) x = [ 1 , 2 , 3 , 4 , 5 , 5 ] y = [ 1 , 2 , 3 , 4 , 5 ] has_duplicates ( x ) # True has_duplicates ( y ) # False 19. Merge two dictionaries \u00b6 The following method can be used to merge two dictionaries. 1 2 3 4 5 6 7 def merge_dictionaries ( a , b ) return { ** a , ** b } a = { 'x' : 1 , 'y' : 2 } b = { 'y' : 3 , 'z' : 4 } print ( merge_dictionaries ( a , b )) # {'y': 3, 'x': 1, 'z': 4} 20. Convert two lists into a dictionary \u00b6 The following method can be used to convert two lists into a dictionary. 1 2 3 4 5 6 7 def to_dictionary ( keys , values ): return dict ( zip ( keys , values )) keys = [ \"a\" , \"b\" , \"c\" ] values = [ 2 , 3 , 4 ] print ( to_dictionary ( keys , values )) # {'a': 2, 'c': 4, 'b': 3} 21. Use enumerate \u00b6 This method gets a dictionary as an input and then returns only the keys that are in this dictionary. 1 2 3 4 5 6 7 list = [ \"a\" , \"b\" , \"c\" , \"d\" ] for index , element in enumerate ( list ): print ( \"Value\" , element , \"Index \" , index , ) # ('Value', 'a', 'Index ', 0) # ('Value', 'b', 'Index ', 1) #('Value', 'c', 'Index ', 2) # ('Value', 'd', 'Index ', 3) 22. Time spent \u00b6 This snippet can be used to calculate the time it takes to execute a particular code. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import time start_time = time . time () a = 1 b = 2 c = a + b print ( c ) #3 end_time = time . time () total_time = end_time - start_time print ( \"Time: \" , total_time ) # ('Time: ', 1.1205673217773438e-05) 23. Try else \u00b6 You can have an else clause as part of a try/except block, which is executed if no exception is thrown. 1 2 3 4 5 6 7 8 try : 2 * 3 except TypeError : print ( \"An exception was raised\" ) else : print ( \"Thank God, no exceptions were raised.\" ) #Thank God, no exceptions were raised. 24. Most frequent \u00b6 This method returns the most frequent element that appears in a list. 1 2 3 4 5 6 def most_frequent ( list ): return max ( set ( list ), key = list . count ) list = [ 1 , 2 , 1 , 2 , 3 , 2 , 1 , 4 , 2 ] most_frequent ( list ) 25. Palindrome \u00b6 This method checks whether a given string is a palindrome. It initially converts the string into lower case, then removes non-alphanumeric characters from it. In the end, it compares the new string with the reversed version. 1 2 3 4 5 6 7 def palindrome ( string ): from re import sub s = sub ( '[\\W_]' , '' , string . lower ()) return s == s [:: - 1 ] palindrome ( 'taco cat' ) # True 26. Calculator without if-else \u00b6 The following snippet shows how you can write a simple calculator without the need to use if-else conditions. 1 2 3 4 5 6 7 8 9 import operator action = { \"+\" : operator . add , \"-\" : operator . sub , \"/\" : operator . truediv , \"*\" : operator . mul , \"**\" : pow } print ( action [ '-' ]( 50 , 25 )) # 25 27. Shuffle \u00b6 This algorithm randomizes the order of the elements in a list by implementing the Fisher-Yates algorithm to do the ordering in the new list. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from copy import deepcopy from random import randint def shuffle ( lst ): temp_lst = deepcopy ( lst ) m = len ( temp_lst ) while ( m ): m -= 1 i = randint ( 0 , m ) temp_lst [ m ], temp_lst [ i ] = temp_lst [ i ], temp_lst [ m ] return temp_lst foo = [ 1 , 2 , 3 ] shuffle ( foo ) # [2,3,1] , foo = [1,2,3] 28. Spread \u00b6 This method flattens a list similarly like [].concat(\u2026arr) in JavaScript. 1 2 3 4 5 6 7 8 9 10 11 def spread ( arg ): ret = [] for i in arg : if isinstance ( i , list ): ret . extend ( i ) else : ret . append ( i ) return ret spread ([ 1 , 2 , 3 ,[ 4 , 5 , 6 ],[ 7 ], 8 , 9 ]) # [1,2,3,4,5,6,7,8,9] 29. Swap values \u00b6 A really quick way for swapping two variables without having to use an additional one. 1 2 3 4 5 def swap ( a , b ): return b , a a , b = - 1 , 14 swap ( a , b ) # (14, -1) 30. Get default value for missing keys \u00b6 This snippet shows how you can get a default value in case a key you are looking for is not included in the dictionary. 1 2 3 d = { 'a' : 1 , 'b' : 2 } print ( d . get ( 'c' , 3 )) # 3","title":"Useful Snippets"},{"location":"Coding/Python/Useful_snippets.html#1_all_unique","text":"This method check if there is any duplicate element 1 2 3 4 5 6 7 def all_unique ( lst ): return len ( str ) == len ( set ( lst )) x = [ 1 , 1 , 2 , 2 , 3 , 2 , 3 , 4 , 5 , 6 ] y = [ 1 , 2 , 3 , 4 , 5 ] all_unique ( x ) # False all_unique ( y ) # True","title":"1. All Unique"},{"location":"Coding/Python/Useful_snippets.html#2_anagrams","text":"Check if a string is an anagram 1 2 3 4 5 6 from collections import Counter def anagram ( first , second ): return Counter ( first ) == Counter ( second ) anagram ( \"abcd3\" , \"3acdb\" ) #True","title":"2. Anagrams"},{"location":"Coding/Python/Useful_snippets.html#3_memory","text":"This is use to check the memory usage of an object 1 2 3 4 import sys variable = 30 print ( sys . getsizeof ( variable )) # 24","title":"3. Memory"},{"location":"Coding/Python/Useful_snippets.html#4_byte_size","text":"This Method return the length of a string in bytes 1 2 3 4 5 def byte_size ( string ): return ( len ( String . encode ( 'utf-8' ))) byte_size ( '\ud83d\ude00' ) # 4 byte_size ( 'Hello World' ) # 11","title":"4. Byte size"},{"location":"Coding/Python/Useful_snippets.html#5_print_a_string_n_times","text":"This will print a string N times 1 2 3 4 n = 2 ; s = 'programing' print ( s * n ) # ProgrammingProgramming","title":"5. Print a String N times"},{"location":"Coding/Python/Useful_snippets.html#6_capitalize_first_letters","text":"1 2 3 s = \"programming is awesome\" print ( s . title ()) # Programming Is Awesome","title":"6. Capitalize first letters"},{"location":"Coding/Python/Useful_snippets.html#7_chunk","text":"This method chunk a list in a smaller list of specific size 1 2 3 4 5 6 from math import ceil def chunk ( lst , size ): return list ( map ( lambda x : lst [ x * size : x * size + size ], list ( range ( 0 , ceil ( len ( lst ) / size ))))) chunk ([ 1 , 2 , 3 , 4 , 5 ], 2 ) # [[1,2],[3,4],5]","title":"7. Chunk"},{"location":"Coding/Python/Useful_snippets.html#8_compact","text":"This method remove the \u201cFalsy\u201d values, in other words, False, 0, None,\u201d\u201c. 1 2 3 4 def compact ( lst ): return list ( filter ( bool , lst )) compact ([ 0 , 1 , False , 2 , '' , 3 , 'a' , 's' , 34 ]) # [ 1, 2, 3, 'a', 's', 34 ]","title":"8. Compact"},{"location":"Coding/Python/Useful_snippets.html#9_count_by","text":"This use ZIP() method to transpose a 2D array 1 2 3 array = [[ 'a' . 'b' ],[ 'c' , 'd' ],[ 'e' . 'f' ]] transposed = zip ( * array ) print ( transposed ) # [('a', 'c', 'e'), ('b', 'd', 'f')]","title":"9. Count by"},{"location":"Coding/Python/Useful_snippets.html#10_chained_comparison","text":"multiple comparison in a single line 1 2 3 a = 3 print ( 2 < a < 8 ) # True print ( 1 == a < 2 ) # False","title":"10. chained comparison"},{"location":"Coding/Python/Useful_snippets.html#11_comma-separated","text":"This snippet use the method .join() turn a list of strings into a single string with each element from the list separated by commas. 1 2 3 hobbies = [ \"basketball\" , \"football\" , \"swimming\" ] print ( \"My hobbies are: \" + \", \" . join ( hobbies )) # My hobbies are: basketball, football, swimming","title":"11. Comma-separated"},{"location":"Coding/Python/Useful_snippets.html#12_count_vowels","text":"This method counts the number of vowels (\u2018a\u2019, \u2018e\u2019, \u2018i\u2019, \u2018o\u2019, \u2018u\u2019) found in a string, using regular expressions. 1 2 3 4 5 6 7 8 import re def count_vowels ( str ): return len ( len ( re . findall ( r '[aeiou]' , str , re . IGNORECASE ))) count_vowels ( 'foobar' ) # 3 count_vowels ( 'gym' ) # 0","title":"12. Count vowels"},{"location":"Coding/Python/Useful_snippets.html#13_decapitalize","text":"This method can be used to turn the first letter of the given string into lowercase. 1 2 3 4 5 6 def decapitalize ( string ): return str [: 1 ] . lower () + str [ 1 :] decapitalize ( 'FooBar' ) # 'fooBar' decapitalize ( 'FooBar' ) # 'fooBar'","title":"13. Decapitalize"},{"location":"Coding/Python/Useful_snippets.html#14_flatten","text":"The following methods flatten a potentially deep list using recursion. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def spread ( arg ): ret = [] for i in arg : if isinstance ( i , list ): ret . extend ( i ) else : ret . append ( i ) return ret def deep_flatten ( lst ): result = [] result . extend ( spread ( list ( map ( lambda x : deep_flatten ( x ) if type ( x ) == list else x , lst )))) return result deep_flatten ([ 1 , [ 2 ], [[ 3 ], 4 ], 5 ]) # [1,2,3,4,5]","title":"14. Flatten"},{"location":"Coding/Python/Useful_snippets.html#15_difference","text":"This method finds the difference between two iterables and display just the different number in the first iterable 1 2 3 4 5 6 7 8 def difference ( a , b ): set_a = set ( a ) set_b = set ( b ) comparison = set_a . difference ( set_b ) return list ( comparison ) difference ([ 1 , 2 , 3 ], [ 1 , 2 , 4 ]) # [3]","title":"15. Difference"},{"location":"Coding/Python/Useful_snippets.html#16_difference_by","text":"The following method returns the difference between two lists after applying a given function to each element of both lists. 1 2 3 4 5 6 7 8 def difference_by ( a , b , fn ): b = set ( map ( fn , b )) return [ item for item in a if fn ( item ) not in b ] from math import floor difference_by ([ 2.1 , 1.2 ], [ 2.3 , 3.4 ], floor ) # [1.2] difference_by ([{ 'x' : 2 }, { 'x' : 1 }], [{ 'x' : 1 }], lambda v : v [ 'x' ]) # [ { x: 2 } ]","title":"16. Difference by"},{"location":"Coding/Python/Useful_snippets.html#17_chained_function_call","text":"You can call multiple functions inside a single line. 1 2 3 4 5 6 7 8 def add ( a , b ): return a + b def subtract ( a , b ): return a - b a , b = 4 , 5 print (( subtract if a > b else add )( a , b )) # 9","title":"17. Chained function call"},{"location":"Coding/Python/Useful_snippets.html#18_has_duplicates","text":"The following method checks whether a list has duplicate values by using the fact that set() contains only unique elements. 1 2 3 4 5 6 7 8 def has_duplicates ( lst ): return len ( lst ) != len ( set ( lst )) x = [ 1 , 2 , 3 , 4 , 5 , 5 ] y = [ 1 , 2 , 3 , 4 , 5 ] has_duplicates ( x ) # True has_duplicates ( y ) # False","title":"18. Has duplicates"},{"location":"Coding/Python/Useful_snippets.html#19_merge_two_dictionaries","text":"The following method can be used to merge two dictionaries. 1 2 3 4 5 6 7 def merge_dictionaries ( a , b ) return { ** a , ** b } a = { 'x' : 1 , 'y' : 2 } b = { 'y' : 3 , 'z' : 4 } print ( merge_dictionaries ( a , b )) # {'y': 3, 'x': 1, 'z': 4}","title":"19. Merge two dictionaries"},{"location":"Coding/Python/Useful_snippets.html#20_convert_two_lists_into_a_dictionary","text":"The following method can be used to convert two lists into a dictionary. 1 2 3 4 5 6 7 def to_dictionary ( keys , values ): return dict ( zip ( keys , values )) keys = [ \"a\" , \"b\" , \"c\" ] values = [ 2 , 3 , 4 ] print ( to_dictionary ( keys , values )) # {'a': 2, 'c': 4, 'b': 3}","title":"20. Convert two lists into a dictionary"},{"location":"Coding/Python/Useful_snippets.html#21_use_enumerate","text":"This method gets a dictionary as an input and then returns only the keys that are in this dictionary. 1 2 3 4 5 6 7 list = [ \"a\" , \"b\" , \"c\" , \"d\" ] for index , element in enumerate ( list ): print ( \"Value\" , element , \"Index \" , index , ) # ('Value', 'a', 'Index ', 0) # ('Value', 'b', 'Index ', 1) #('Value', 'c', 'Index ', 2) # ('Value', 'd', 'Index ', 3)","title":"21. Use enumerate"},{"location":"Coding/Python/Useful_snippets.html#22_time_spent","text":"This snippet can be used to calculate the time it takes to execute a particular code. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import time start_time = time . time () a = 1 b = 2 c = a + b print ( c ) #3 end_time = time . time () total_time = end_time - start_time print ( \"Time: \" , total_time ) # ('Time: ', 1.1205673217773438e-05)","title":"22. Time spent"},{"location":"Coding/Python/Useful_snippets.html#23_try_else","text":"You can have an else clause as part of a try/except block, which is executed if no exception is thrown. 1 2 3 4 5 6 7 8 try : 2 * 3 except TypeError : print ( \"An exception was raised\" ) else : print ( \"Thank God, no exceptions were raised.\" ) #Thank God, no exceptions were raised.","title":"23. Try else"},{"location":"Coding/Python/Useful_snippets.html#24_most_frequent","text":"This method returns the most frequent element that appears in a list. 1 2 3 4 5 6 def most_frequent ( list ): return max ( set ( list ), key = list . count ) list = [ 1 , 2 , 1 , 2 , 3 , 2 , 1 , 4 , 2 ] most_frequent ( list )","title":"24. Most frequent"},{"location":"Coding/Python/Useful_snippets.html#25_palindrome","text":"This method checks whether a given string is a palindrome. It initially converts the string into lower case, then removes non-alphanumeric characters from it. In the end, it compares the new string with the reversed version. 1 2 3 4 5 6 7 def palindrome ( string ): from re import sub s = sub ( '[\\W_]' , '' , string . lower ()) return s == s [:: - 1 ] palindrome ( 'taco cat' ) # True","title":"25. Palindrome"},{"location":"Coding/Python/Useful_snippets.html#26_calculator_without_if-else","text":"The following snippet shows how you can write a simple calculator without the need to use if-else conditions. 1 2 3 4 5 6 7 8 9 import operator action = { \"+\" : operator . add , \"-\" : operator . sub , \"/\" : operator . truediv , \"*\" : operator . mul , \"**\" : pow } print ( action [ '-' ]( 50 , 25 )) # 25","title":"26. Calculator without if-else"},{"location":"Coding/Python/Useful_snippets.html#27_shuffle","text":"This algorithm randomizes the order of the elements in a list by implementing the Fisher-Yates algorithm to do the ordering in the new list. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from copy import deepcopy from random import randint def shuffle ( lst ): temp_lst = deepcopy ( lst ) m = len ( temp_lst ) while ( m ): m -= 1 i = randint ( 0 , m ) temp_lst [ m ], temp_lst [ i ] = temp_lst [ i ], temp_lst [ m ] return temp_lst foo = [ 1 , 2 , 3 ] shuffle ( foo ) # [2,3,1] , foo = [1,2,3]","title":"27. Shuffle"},{"location":"Coding/Python/Useful_snippets.html#28_spread","text":"This method flattens a list similarly like [].concat(\u2026arr) in JavaScript. 1 2 3 4 5 6 7 8 9 10 11 def spread ( arg ): ret = [] for i in arg : if isinstance ( i , list ): ret . extend ( i ) else : ret . append ( i ) return ret spread ([ 1 , 2 , 3 ,[ 4 , 5 , 6 ],[ 7 ], 8 , 9 ]) # [1,2,3,4,5,6,7,8,9]","title":"28. Spread"},{"location":"Coding/Python/Useful_snippets.html#29_swap_values","text":"A really quick way for swapping two variables without having to use an additional one. 1 2 3 4 5 def swap ( a , b ): return b , a a , b = - 1 , 14 swap ( a , b ) # (14, -1)","title":"29. Swap values"},{"location":"Coding/Python/Useful_snippets.html#30_get_default_value_for_missing_keys","text":"This snippet shows how you can get a default value in case a key you are looking for is not included in the dictionary. 1 2 3 d = { 'a' : 1 , 'b' : 2 } print ( d . get ( 'c' , 3 )) # 3","title":"30. Get default value for missing keys"},{"location":"Coding/Python/control flow.html","text":"Iterating through Dictionaries with For loops \u00b6 The iteration through Dictionaries in pyhton is a different than Swift, in this case to iterate and get back both key and value you need to use a built-in method item . For example: 1 2 3 4 5 6 7 8 9 cast = { \"Jerry Seinfeld\" : \"Jerry Seinfeld\" , \"Julia Louis-Dreyfus\" : \"Elaine Benes\" , \"Jason Alexander\" : \"George Costanza\" , \"Michael Richards\" : \"Cosmo Kramer\" } for key , value in cast . items (): print ( \"Actor: {} Role: {}\" . format ( key , value )) which will result in something like: 1 2 3 4 Actor: Jerry Seinfeld Role: Jerry Seinfeld Actor: Julia Louis-Dreyfus Role: Elaine Benes Actor: Jason Alexander Role: George Costanza Actor: Michael Richards Role: Cosmo Kramer for Loops vs. while Loops \u00b6 for loops are idea when the number of iteration are known or finite . Examples: When you have an iterable collection (list, string, set, tuple, dictionary): 1 for name in names : When you want to iterate through a loop for a definite number of times, using range() 1 for i in range ( 5 ): while loop are ideal when the iterations are to continue until a condition is met Examples: When you want to use comparison operations: 1 while count <= 100 : When you want to loop base on receiving specific user input 1 while user_input == 'y' the following are required to build a correct while loop: The condition for existing the while loop should be included. Check if the iteration conditions is met. Body of the loop should change the value of condition variables. Break and continue. \u00b6 break terminates a loop. continue skips one iteration of a loop. Zip and Enumerate \u00b6 zip and enumerate are build-in functions that can come handy when dealing with loops. Zip zip will return an iterator that combine multiples iterables into one sequence of tuples, this will be more clear with an example: 1 list ( zip ([ 'a' , 'b' , 'c' ], [ 1 , 2 , 3 ])) which out put will be: 1 [( 'a' , 1 ),( 'b' , 2 ),( 'c' , 3 )] In this case we can see that zip create a iterator that combine the two provided iterables and each iterator is a tuple with items in the position of the original iterable. the reverse or Unzip process can be done using the * 1 2 3 4 5 6 7 8 letters = [ 'a' , 'b' , 'c' ] nums = [ 1 , 2 , 3 ] for letter , num in zip ( letters , nums ): print ( \"{}:{}\" . format ( letter , num )) some_list = [( 'a' , 1 ),( 'b' , 2 ),( 'c' , 3 )] letters , nums = zip ( * some_list ) In Python3, zip methods returns a zip object instead of a list. This zip object is an iterator. Iterators are lazily evaluated. Lazy evaluation, or call-by-need is an evaluation strategy which delays the evaluation of an expression until its value is needed and which also avoids repeated evaluations Iterators returns only element at a time. len function cannot be used with iterators. We can loop over the zip object or the iterator to get the actual list Consider the below example: 1 2 3 4 5 6 7 8 9 10 11 12 list_a = [ 1 , 2 , 3 ] list_b = [ 4 , 5 , 6 ] zipped = zip ( a , b ) # Output: Zip Object. <zip at 0x4c10a30> len ( zipped ) # TypeError: object of type 'zip' has no len() zipped [ 0 ] # TypeError: 'zip' object is not subscriptable list_c = list ( zipped ) #Output: [(1, 4), (2, 5), (3, 6)] list_d = list ( zipped ) # Output []... Output is empty list becuase by the above statement zip got exhausted. Enumerate enumerate is a built in function that return an iterator of tuples containing indexes and values of a list, example: 1 2 3 4 letters = [ 'a' , 'b' , 'c' , 'd' , 'e' ] for i , letter in enumerate ( letters ): print ( i , letter ) this will be the output: 1 2 3 4 5 0 a 1 b 2 c 3 d 4 e an example of enumerate , here: 1 2 3 4 5 6 7 cast = [ \"Barney Stinson\" , \"Robin Scherbatsky\" , \"Ted Mosby\" , \"Lily Aldrin\" , \"Marshall Eriksen\" ] heights = [ 72 , 68 , 72 , 66 , 76 ] for i , character in enumerate ( cast ): cast [ i ] = character + \" \" + str ( heights [ i ]) print ( cast ) It will take 2 list and it will out put a single list of items that include a both original list as a single string 1 ['Barney Stinson 72', 'Robin Scherbatsky 68', 'Ted Mosby 72', 'Lily Aldrin 66', 'Marshall Eriksen 76'] List Comprehensions \u00b6 List comprehensions are just present in python and not in other languages, this are normally use to create a list in a quickly and concisely way, for example: 1 2 3 capitalized_cities = [] for city in cities : capitalized_cities . append ( city . title ()) can be reduce to: 1 capitalized_cities = [ city . title () for city in cities ] Conditional can be added to this list comprehensions (listcomps). be aware that if the conditional has a else statement the syntax will be a bit different. Lets start with a simply conditional. 1 squares = [ x ** 2 for x in range ( 9 ) if x % 2 == 0 ] this will create a list with the power of the even numbers if you want to add a else , you will get a syntax error 1 squares = [ x ** 2 for x in range ( 9 ) if x % 2 == 0 else x + 3 ] # this will produce a syntax error in this case, it is necessary move all the block to the beginning. 1 squares = [ x ** 2 if x % 2 == 0 else x + 3 for x in range ( 9 )] Exercise using the dictionaries and for loops \u00b6 Provide a list with the name(s) of the director(s) with the most Oscar wins. We are asking for a list because there could be more than 1 director tied for the most Oscar wins. 1 winners = { 1931 : [ 'Norman Taurog' ], 1932 : [ 'Frank Borzage' ], 1933 : [ 'Frank Lloyd' ], 1934 : [ 'Frank Capra' ], 1935 : [ 'John Ford' ], 1936 : [ 'Frank Capra' ], 1937 : [ 'Leo McCarey' ], 1938 : [ 'Frank Capra' ], 1939 : [ 'Victor Fleming' ], 1940 : [ 'John Ford' ], 1941 : [ 'John Ford' ], 1942 : [ 'William Wyler' ], 1943 : [ 'Michael Curtiz' ], 1944 : [ 'Leo McCarey' ], 1945 : [ 'Billy Wilder' ], 1946 : [ 'William Wyler' ], 1947 : [ 'Elia Kazan' ], 1948 : [ 'John Huston' ], 1949 : [ 'Joseph L. Mankiewicz' ], 1950 : [ 'Joseph L. Mankiewicz' ], 1951 : [ 'George Stevens' ], 1952 : [ 'John Ford' ], 1953 : [ 'Fred Zinnemann' ], 1954 : [ 'Elia Kazan' ], 1955 : [ 'Delbert Mann' ], 1956 : [ 'George Stevens' ], 1957 : [ 'David Lean' ], 1958 : [ 'Vincente Minnelli' ], 1959 : [ 'William Wyler' ], 1960 : [ 'Billy Wilder' ], 1961 : [ 'Jerome Robbins' , 'Robert Wise' ], 1962 : [ 'David Lean' ], 1963 : [ 'Tony Richardson' ], 1964 : [ 'George Cukor' ], 1965 : [ 'Robert Wise' ], 1966 : [ 'Fred Zinnemann' ], 1967 : [ 'Mike Nichols' ], 1968 : [ 'Carol Reed' ], 1969 : [ 'John Schlesinger' ], 1970 : [ 'Franklin J. Schaffner' ], 1971 : [ 'William Friedkin' ], 1972 : [ 'Bob Fosse' ], 1973 : [ 'George Roy Hill' ], 1974 : [ 'Francis Ford Coppola' ], 1975 : [ 'Milos Forman' ], 1976 : [ 'John G. Avildsen' ], 1977 : [ 'Woody Allen' ], 1978 : [ 'Michael Cimino' ], 1979 : [ 'Robert Benton' ], 1980 : [ 'Robert Redford' ], 1981 : [ 'Warren Beatty' ], 1982 : [ 'Richard Attenborough' ], 1983 : [ 'James L. Brooks' ], 1984 : [ 'Milos Forman' ], 1985 : [ 'Sydney Pollack' ], 1986 : [ 'Oliver Stone' ], 1987 : [ 'Bernardo Bertolucci' ], 1988 : [ 'Barry Levinson' ], 1989 : [ 'Oliver Stone' ], 1990 : [ 'Kevin Costner' ], 1991 : [ 'Jonathan Demme' ], 1992 : [ 'Clint Eastwood' ], 1993 : [ 'Steven Spielberg' ], 1994 : [ 'Robert Zemeckis' ], 1995 : [ 'Mel Gibson' ], 1996 : [ 'Anthony Minghella' ], 1997 : [ 'James Cameron' ], 1998 : [ 'Steven Spielberg' ], 1999 : [ 'Sam Mendes' ], 2000 : [ 'Steven Soderbergh' ], 2001 : [ 'Ron Howard' ], 2002 : [ 'Roman Polanski' ], 2003 : [ 'Peter Jackson' ], 2004 : [ 'Clint Eastwood' ], 2005 : [ 'Ang Lee' ], 2006 : [ 'Martin Scorsese' ], 2007 : [ 'Ethan Coen' , 'Joel Coen' ], 2008 : [ 'Danny Boyle' ], 2009 : [ 'Kathryn Bigelow' ], 2010 : [ 'Tom Hooper' ]} first step, is \u201copen the dictionary\u201d, i will need to list the directors as a key and the number of wins as a value, for that i can use the for loops and the get method of dictionaries 1 2 3 4 5 6 7 most_win_director = [] # i will use this list later to print the results win_dict = {} # this Dictionary will hold the directors and the values ( number of wins) for year , winner_name in winner . item (): for winner in winner_name : win_dict [ winner ] = win_dict . get ( winner , 0 ) + 1 so in this step i get a dictionary that holds the name and the number of wins 1 {'Peter Jackson': 1, 'Anthony Minghella': 1, 'Robert Redford': 1, 'Clint Eastwood': 2, 'Ron Howard': 1, 'Billy Wilder': 2, 'Steven Spielberg': 2, 'Richard Attenborough': 1, 'Mel Gibson': 1, 'Leo McCarey': 2, 'William Friedkin': 1, 'Barry Levinson': 1, 'Oliver Stone': 2, 'Warren Beatty': 1, 'Ang Lee': 1, 'Joseph L. Mankiewicz': 2, 'Sydney Pollack': 1, 'Robert Wise': 2, 'Woody Allen': 1, 'John Ford': 4, 'Bob Fosse': 1, 'Jerome Robbins': 1, 'Robert Benton': 1, 'Elia Kazan': 2, 'Frank Lloyd': 1, 'John G. Avildsen': 1, 'Tom Hooper': 1, 'Frank Borzage': 1, 'Sam Mendes': 1, 'John Huston': 1, 'Carol Reed': 1, 'Francis Ford Coppola': 1, 'Joel Coen': 1, 'Fred Zinnemann': 2, 'William Wyler': 3, 'Jonathan Demme': 1, 'Kathryn Bigelow': 1, 'Delbert Mann': 1, 'Danny Boyle': 1, 'George Cukor': 1, 'Norman Taurog': 1, 'Tony Richardson': 1, 'George Roy Hill': 1, 'James L. Brooks': 1, 'Martin Scorsese': 1, 'David Lean': 2, 'Franklin J. Schaffner': 1, 'Bernardo Bertolucci': 1, 'John Schlesinger': 1, 'Ethan Coen': 1, 'Michael Cimino': 1, 'Milos Forman': 2, 'Mike Nichols': 1, 'Michael Curtiz': 1, 'Steven Soderbergh': 1, 'Robert Zemeckis': 1, 'Kevin Costner': 1, 'Frank Capra': 3, 'Vincente Minnelli': 1, 'James Cameron': 1, 'George Stevens': 2, 'Roman Polanski': 1, 'Victor Fleming': 1} now I will need to create a variable highest_wins I will change the value of this variable every time I encounter a director with a higher number of wins, and I will clean the most_win_director and append the a new name. 1 2 3 4 5 6 7 8 9 10 11 highest_wins = 0 for director , wins in win_dict . items (): if wins > highest_wins : highest_wins = wins most_win_director . clear () most_win_director . append ( director ) elif wins == highest_wins : most_win_director . append ( director ) else : continue finally print the solution: 1 print ( \"most_win_director = {}\" . format ( most_win_director )) and here a more compact solution using max and list comprehension : 1 2 3 highest_count = max ( win_count_dict . values ()) most_win_director = [ key for key , value in win_count_dict . items () if value == highest_count ]","title":"Control Flow"},{"location":"Coding/Python/control flow.html#iterating_through_dictionaries_with_for_loops","text":"The iteration through Dictionaries in pyhton is a different than Swift, in this case to iterate and get back both key and value you need to use a built-in method item . For example: 1 2 3 4 5 6 7 8 9 cast = { \"Jerry Seinfeld\" : \"Jerry Seinfeld\" , \"Julia Louis-Dreyfus\" : \"Elaine Benes\" , \"Jason Alexander\" : \"George Costanza\" , \"Michael Richards\" : \"Cosmo Kramer\" } for key , value in cast . items (): print ( \"Actor: {} Role: {}\" . format ( key , value )) which will result in something like: 1 2 3 4 Actor: Jerry Seinfeld Role: Jerry Seinfeld Actor: Julia Louis-Dreyfus Role: Elaine Benes Actor: Jason Alexander Role: George Costanza Actor: Michael Richards Role: Cosmo Kramer","title":"Iterating through Dictionaries with For loops"},{"location":"Coding/Python/control flow.html#for_loops_vs_while_loops","text":"for loops are idea when the number of iteration are known or finite . Examples: When you have an iterable collection (list, string, set, tuple, dictionary): 1 for name in names : When you want to iterate through a loop for a definite number of times, using range() 1 for i in range ( 5 ): while loop are ideal when the iterations are to continue until a condition is met Examples: When you want to use comparison operations: 1 while count <= 100 : When you want to loop base on receiving specific user input 1 while user_input == 'y' the following are required to build a correct while loop: The condition for existing the while loop should be included. Check if the iteration conditions is met. Body of the loop should change the value of condition variables.","title":"for Loops vs. while Loops"},{"location":"Coding/Python/control flow.html#break_and_continue","text":"break terminates a loop. continue skips one iteration of a loop.","title":"Break and continue."},{"location":"Coding/Python/control flow.html#zip_and_enumerate","text":"zip and enumerate are build-in functions that can come handy when dealing with loops. Zip zip will return an iterator that combine multiples iterables into one sequence of tuples, this will be more clear with an example: 1 list ( zip ([ 'a' , 'b' , 'c' ], [ 1 , 2 , 3 ])) which out put will be: 1 [( 'a' , 1 ),( 'b' , 2 ),( 'c' , 3 )] In this case we can see that zip create a iterator that combine the two provided iterables and each iterator is a tuple with items in the position of the original iterable. the reverse or Unzip process can be done using the * 1 2 3 4 5 6 7 8 letters = [ 'a' , 'b' , 'c' ] nums = [ 1 , 2 , 3 ] for letter , num in zip ( letters , nums ): print ( \"{}:{}\" . format ( letter , num )) some_list = [( 'a' , 1 ),( 'b' , 2 ),( 'c' , 3 )] letters , nums = zip ( * some_list ) In Python3, zip methods returns a zip object instead of a list. This zip object is an iterator. Iterators are lazily evaluated. Lazy evaluation, or call-by-need is an evaluation strategy which delays the evaluation of an expression until its value is needed and which also avoids repeated evaluations Iterators returns only element at a time. len function cannot be used with iterators. We can loop over the zip object or the iterator to get the actual list Consider the below example: 1 2 3 4 5 6 7 8 9 10 11 12 list_a = [ 1 , 2 , 3 ] list_b = [ 4 , 5 , 6 ] zipped = zip ( a , b ) # Output: Zip Object. <zip at 0x4c10a30> len ( zipped ) # TypeError: object of type 'zip' has no len() zipped [ 0 ] # TypeError: 'zip' object is not subscriptable list_c = list ( zipped ) #Output: [(1, 4), (2, 5), (3, 6)] list_d = list ( zipped ) # Output []... Output is empty list becuase by the above statement zip got exhausted. Enumerate enumerate is a built in function that return an iterator of tuples containing indexes and values of a list, example: 1 2 3 4 letters = [ 'a' , 'b' , 'c' , 'd' , 'e' ] for i , letter in enumerate ( letters ): print ( i , letter ) this will be the output: 1 2 3 4 5 0 a 1 b 2 c 3 d 4 e an example of enumerate , here: 1 2 3 4 5 6 7 cast = [ \"Barney Stinson\" , \"Robin Scherbatsky\" , \"Ted Mosby\" , \"Lily Aldrin\" , \"Marshall Eriksen\" ] heights = [ 72 , 68 , 72 , 66 , 76 ] for i , character in enumerate ( cast ): cast [ i ] = character + \" \" + str ( heights [ i ]) print ( cast ) It will take 2 list and it will out put a single list of items that include a both original list as a single string 1 ['Barney Stinson 72', 'Robin Scherbatsky 68', 'Ted Mosby 72', 'Lily Aldrin 66', 'Marshall Eriksen 76']","title":"Zip and Enumerate"},{"location":"Coding/Python/control flow.html#list_comprehensions","text":"List comprehensions are just present in python and not in other languages, this are normally use to create a list in a quickly and concisely way, for example: 1 2 3 capitalized_cities = [] for city in cities : capitalized_cities . append ( city . title ()) can be reduce to: 1 capitalized_cities = [ city . title () for city in cities ] Conditional can be added to this list comprehensions (listcomps). be aware that if the conditional has a else statement the syntax will be a bit different. Lets start with a simply conditional. 1 squares = [ x ** 2 for x in range ( 9 ) if x % 2 == 0 ] this will create a list with the power of the even numbers if you want to add a else , you will get a syntax error 1 squares = [ x ** 2 for x in range ( 9 ) if x % 2 == 0 else x + 3 ] # this will produce a syntax error in this case, it is necessary move all the block to the beginning. 1 squares = [ x ** 2 if x % 2 == 0 else x + 3 for x in range ( 9 )]","title":"List Comprehensions"},{"location":"Coding/Python/control flow.html#exercise_using_the_dictionaries_and_for_loops","text":"Provide a list with the name(s) of the director(s) with the most Oscar wins. We are asking for a list because there could be more than 1 director tied for the most Oscar wins. 1 winners = { 1931 : [ 'Norman Taurog' ], 1932 : [ 'Frank Borzage' ], 1933 : [ 'Frank Lloyd' ], 1934 : [ 'Frank Capra' ], 1935 : [ 'John Ford' ], 1936 : [ 'Frank Capra' ], 1937 : [ 'Leo McCarey' ], 1938 : [ 'Frank Capra' ], 1939 : [ 'Victor Fleming' ], 1940 : [ 'John Ford' ], 1941 : [ 'John Ford' ], 1942 : [ 'William Wyler' ], 1943 : [ 'Michael Curtiz' ], 1944 : [ 'Leo McCarey' ], 1945 : [ 'Billy Wilder' ], 1946 : [ 'William Wyler' ], 1947 : [ 'Elia Kazan' ], 1948 : [ 'John Huston' ], 1949 : [ 'Joseph L. Mankiewicz' ], 1950 : [ 'Joseph L. Mankiewicz' ], 1951 : [ 'George Stevens' ], 1952 : [ 'John Ford' ], 1953 : [ 'Fred Zinnemann' ], 1954 : [ 'Elia Kazan' ], 1955 : [ 'Delbert Mann' ], 1956 : [ 'George Stevens' ], 1957 : [ 'David Lean' ], 1958 : [ 'Vincente Minnelli' ], 1959 : [ 'William Wyler' ], 1960 : [ 'Billy Wilder' ], 1961 : [ 'Jerome Robbins' , 'Robert Wise' ], 1962 : [ 'David Lean' ], 1963 : [ 'Tony Richardson' ], 1964 : [ 'George Cukor' ], 1965 : [ 'Robert Wise' ], 1966 : [ 'Fred Zinnemann' ], 1967 : [ 'Mike Nichols' ], 1968 : [ 'Carol Reed' ], 1969 : [ 'John Schlesinger' ], 1970 : [ 'Franklin J. Schaffner' ], 1971 : [ 'William Friedkin' ], 1972 : [ 'Bob Fosse' ], 1973 : [ 'George Roy Hill' ], 1974 : [ 'Francis Ford Coppola' ], 1975 : [ 'Milos Forman' ], 1976 : [ 'John G. Avildsen' ], 1977 : [ 'Woody Allen' ], 1978 : [ 'Michael Cimino' ], 1979 : [ 'Robert Benton' ], 1980 : [ 'Robert Redford' ], 1981 : [ 'Warren Beatty' ], 1982 : [ 'Richard Attenborough' ], 1983 : [ 'James L. Brooks' ], 1984 : [ 'Milos Forman' ], 1985 : [ 'Sydney Pollack' ], 1986 : [ 'Oliver Stone' ], 1987 : [ 'Bernardo Bertolucci' ], 1988 : [ 'Barry Levinson' ], 1989 : [ 'Oliver Stone' ], 1990 : [ 'Kevin Costner' ], 1991 : [ 'Jonathan Demme' ], 1992 : [ 'Clint Eastwood' ], 1993 : [ 'Steven Spielberg' ], 1994 : [ 'Robert Zemeckis' ], 1995 : [ 'Mel Gibson' ], 1996 : [ 'Anthony Minghella' ], 1997 : [ 'James Cameron' ], 1998 : [ 'Steven Spielberg' ], 1999 : [ 'Sam Mendes' ], 2000 : [ 'Steven Soderbergh' ], 2001 : [ 'Ron Howard' ], 2002 : [ 'Roman Polanski' ], 2003 : [ 'Peter Jackson' ], 2004 : [ 'Clint Eastwood' ], 2005 : [ 'Ang Lee' ], 2006 : [ 'Martin Scorsese' ], 2007 : [ 'Ethan Coen' , 'Joel Coen' ], 2008 : [ 'Danny Boyle' ], 2009 : [ 'Kathryn Bigelow' ], 2010 : [ 'Tom Hooper' ]} first step, is \u201copen the dictionary\u201d, i will need to list the directors as a key and the number of wins as a value, for that i can use the for loops and the get method of dictionaries 1 2 3 4 5 6 7 most_win_director = [] # i will use this list later to print the results win_dict = {} # this Dictionary will hold the directors and the values ( number of wins) for year , winner_name in winner . item (): for winner in winner_name : win_dict [ winner ] = win_dict . get ( winner , 0 ) + 1 so in this step i get a dictionary that holds the name and the number of wins 1 {'Peter Jackson': 1, 'Anthony Minghella': 1, 'Robert Redford': 1, 'Clint Eastwood': 2, 'Ron Howard': 1, 'Billy Wilder': 2, 'Steven Spielberg': 2, 'Richard Attenborough': 1, 'Mel Gibson': 1, 'Leo McCarey': 2, 'William Friedkin': 1, 'Barry Levinson': 1, 'Oliver Stone': 2, 'Warren Beatty': 1, 'Ang Lee': 1, 'Joseph L. Mankiewicz': 2, 'Sydney Pollack': 1, 'Robert Wise': 2, 'Woody Allen': 1, 'John Ford': 4, 'Bob Fosse': 1, 'Jerome Robbins': 1, 'Robert Benton': 1, 'Elia Kazan': 2, 'Frank Lloyd': 1, 'John G. Avildsen': 1, 'Tom Hooper': 1, 'Frank Borzage': 1, 'Sam Mendes': 1, 'John Huston': 1, 'Carol Reed': 1, 'Francis Ford Coppola': 1, 'Joel Coen': 1, 'Fred Zinnemann': 2, 'William Wyler': 3, 'Jonathan Demme': 1, 'Kathryn Bigelow': 1, 'Delbert Mann': 1, 'Danny Boyle': 1, 'George Cukor': 1, 'Norman Taurog': 1, 'Tony Richardson': 1, 'George Roy Hill': 1, 'James L. Brooks': 1, 'Martin Scorsese': 1, 'David Lean': 2, 'Franklin J. Schaffner': 1, 'Bernardo Bertolucci': 1, 'John Schlesinger': 1, 'Ethan Coen': 1, 'Michael Cimino': 1, 'Milos Forman': 2, 'Mike Nichols': 1, 'Michael Curtiz': 1, 'Steven Soderbergh': 1, 'Robert Zemeckis': 1, 'Kevin Costner': 1, 'Frank Capra': 3, 'Vincente Minnelli': 1, 'James Cameron': 1, 'George Stevens': 2, 'Roman Polanski': 1, 'Victor Fleming': 1} now I will need to create a variable highest_wins I will change the value of this variable every time I encounter a director with a higher number of wins, and I will clean the most_win_director and append the a new name. 1 2 3 4 5 6 7 8 9 10 11 highest_wins = 0 for director , wins in win_dict . items (): if wins > highest_wins : highest_wins = wins most_win_director . clear () most_win_director . append ( director ) elif wins == highest_wins : most_win_director . append ( director ) else : continue finally print the solution: 1 print ( \"most_win_director = {}\" . format ( most_win_director )) and here a more compact solution using max and list comprehension : 1 2 3 highest_count = max ( win_count_dict . values ()) most_win_director = [ key for key , value in win_count_dict . items () if value == highest_count ]","title":"Exercise using the dictionaries and for loops"},{"location":"Coding/Python/functions.html","text":"Default arguments \u00b6 We can add default arguments in a function to have default values for parameters that are unspecified in a function call: 1 2 3 def cylinder_volume ( height , radius = 5 ): pi = 3.14159 return height * pi * radius ** 2 Variable \u00b6 check the code 1 2 3 4 5 6 egg_bag = 0 def buy_eggs () egg_bag += 12 buy_eggs () This causes an UnboundLocalError, since Python doesn\u2019t allow functions to modify variables that are outside the function\u2019s scope. A better way would be to pass the variable as an argument and reassign it outside the function. a better solution will be: 1 2 3 4 5 6 egg_count = 0 def buy_eggs ( count ): return count + 12 # purchase a dozen eggs egg_count = buy_eggs ( egg_count ) Lambda Expressions \u00b6 You can use lambda expressions to create anonymous functions. These are functions that don\u2019t have a name. They are helpful for creating quick functions that aren\u2019t needed later in your code. This can be especially useful for higher order functions, or functions that take in other functions as arguments. With a lambda expression, this function: 1 2 def multiply ( x , y ): return x * y can be reduced to: 1 multiply = lambda x , y : x * y Components of a Lambda Function The lambda keyword is used to indicate that this is a lambda expression. Following lambda are one or more arguments for the anonymous function separated by commas, followed by a colon : . Similar to functions, the way the arguments are named in a lambda expression is arbitrary. Last is an expression that is evaluated and returned in this function. This is a lot like an expression you might see as a return statement in a function. map() is a higher-order built-in function that takes a function and iterable as inputs, and returns an iterator that applies the function to each element of the iterable. 1 2 3 4 5 6 7 8 9 numbers = [ [ 34 , 63 , 88 , 71 , 29 ], [ 90 , 78 , 51 , 27 , 45 ], [ 63 , 37 , 85 , 46 , 22 ], [ 51 , 22 , 34 , 11 , 18 ] ] averages = list ( map ( lambda num_list : sum ( num_list ) / len ( num_list ), numbers )) print ( averages ) in this case maps() is getting the value of numbers (an iterable variable) and use it in the lambda expression, it will get the values in numbers and will get the average. filter() is a higher-order built-in function that takes a function and iterable as inputs and returns an iterator with the elements from the iterable for which the function returns True, here and example: 1 2 3 4 cities = [ \"New York City\" , \"Los Angeles\" , \"Chicago\" , \"Mountain View\" , \"Denver\" , \"Boston\" ] short_cities = list ( filter ( lambda name : len ( name ) < 10 , cities )) print ( short_cities )","title":"Functions"},{"location":"Coding/Python/functions.html#default_arguments","text":"We can add default arguments in a function to have default values for parameters that are unspecified in a function call: 1 2 3 def cylinder_volume ( height , radius = 5 ): pi = 3.14159 return height * pi * radius ** 2","title":"Default arguments"},{"location":"Coding/Python/functions.html#variable","text":"check the code 1 2 3 4 5 6 egg_bag = 0 def buy_eggs () egg_bag += 12 buy_eggs () This causes an UnboundLocalError, since Python doesn\u2019t allow functions to modify variables that are outside the function\u2019s scope. A better way would be to pass the variable as an argument and reassign it outside the function. a better solution will be: 1 2 3 4 5 6 egg_count = 0 def buy_eggs ( count ): return count + 12 # purchase a dozen eggs egg_count = buy_eggs ( egg_count )","title":"Variable"},{"location":"Coding/Python/functions.html#lambda_expressions","text":"You can use lambda expressions to create anonymous functions. These are functions that don\u2019t have a name. They are helpful for creating quick functions that aren\u2019t needed later in your code. This can be especially useful for higher order functions, or functions that take in other functions as arguments. With a lambda expression, this function: 1 2 def multiply ( x , y ): return x * y can be reduced to: 1 multiply = lambda x , y : x * y Components of a Lambda Function The lambda keyword is used to indicate that this is a lambda expression. Following lambda are one or more arguments for the anonymous function separated by commas, followed by a colon : . Similar to functions, the way the arguments are named in a lambda expression is arbitrary. Last is an expression that is evaluated and returned in this function. This is a lot like an expression you might see as a return statement in a function. map() is a higher-order built-in function that takes a function and iterable as inputs, and returns an iterator that applies the function to each element of the iterable. 1 2 3 4 5 6 7 8 9 numbers = [ [ 34 , 63 , 88 , 71 , 29 ], [ 90 , 78 , 51 , 27 , 45 ], [ 63 , 37 , 85 , 46 , 22 ], [ 51 , 22 , 34 , 11 , 18 ] ] averages = list ( map ( lambda num_list : sum ( num_list ) / len ( num_list ), numbers )) print ( averages ) in this case maps() is getting the value of numbers (an iterable variable) and use it in the lambda expression, it will get the values in numbers and will get the average. filter() is a higher-order built-in function that takes a function and iterable as inputs and returns an iterator with the elements from the iterable for which the function returns True, here and example: 1 2 3 4 cities = [ \"New York City\" , \"Los Angeles\" , \"Chicago\" , \"Mountain View\" , \"Denver\" , \"Boston\" ] short_cities = list ( filter ( lambda name : len ( name ) < 10 , cities )) print ( short_cities )","title":"Lambda Expressions"},{"location":"Coding/Python/virtual_environments.html","text":"Installing Virtualenv \u00b6 Assuming python 3 it is already installed as well as pip we can start in this way: 1 pip install virtualenv Creating the Virtual Environment \u00b6 Now we can select the directory where we are going to save the virtual environment, in this example we will use the directory \u201c/Users/victoraguirre/Documents/026.workspace_Python\u201d, and we can provide a name for it ( virtual environment), in this case i will choose virtualEnv000 1 virtualenv -p python3 /Users/victoraguirre/Documents/026.workspace_Python/virtualEnv000 Activate environments \u00b6 now a Folder with the name of the environment it is going to be created, the first step to activate the environment will be to navigate to that folder and later call the activate file that is inside the bin directory 1 2 3 4 5 6 7 // on Mac cd /Users/victoraguirre/Documents/026.workspace_Python/virtualenv source bin/activate // On windows call /Users/victoraguirre/Documents/026.workspace_Python/virtualenv source bin/activate.bat As part of this exercise I will install Flask inside this virtual environment Install packages and create a file \u00b6 now lets install flask 1 pip install flask here the basic code for a page in flash now running 1 python app.py we have the server running ans the website up ( default IP http://127.0.0.1:5000) Deactivate environment \u00b6 Now, to stop the environment we just need to type deactivate","title":"Virtual Environments"},{"location":"Coding/Python/virtual_environments.html#installing_virtualenv","text":"Assuming python 3 it is already installed as well as pip we can start in this way: 1 pip install virtualenv","title":"Installing Virtualenv"},{"location":"Coding/Python/virtual_environments.html#creating_the_virtual_environment","text":"Now we can select the directory where we are going to save the virtual environment, in this example we will use the directory \u201c/Users/victoraguirre/Documents/026.workspace_Python\u201d, and we can provide a name for it ( virtual environment), in this case i will choose virtualEnv000 1 virtualenv -p python3 /Users/victoraguirre/Documents/026.workspace_Python/virtualEnv000","title":"Creating the Virtual Environment"},{"location":"Coding/Python/virtual_environments.html#activate_environments","text":"now a Folder with the name of the environment it is going to be created, the first step to activate the environment will be to navigate to that folder and later call the activate file that is inside the bin directory 1 2 3 4 5 6 7 // on Mac cd /Users/victoraguirre/Documents/026.workspace_Python/virtualenv source bin/activate // On windows call /Users/victoraguirre/Documents/026.workspace_Python/virtualenv source bin/activate.bat As part of this exercise I will install Flask inside this virtual environment","title":"Activate environments"},{"location":"Coding/Python/virtual_environments.html#install_packages_and_create_a_file","text":"now lets install flask 1 pip install flask here the basic code for a page in flash now running 1 python app.py we have the server running ans the website up ( default IP http://127.0.0.1:5000)","title":"Install packages and create a file"},{"location":"Coding/Python/virtual_environments.html#deactivate_environment","text":"Now, to stop the environment we just need to type deactivate","title":"Deactivate environment"},{"location":"Coding/Python/Algorithm/Magic_square.html","text":"The following was a challenge in HackerRank the name of the challenge Forming a Magic Square First lets define what is a Magic square What is a Magic square \u00b6 A magic square of order n n is an arrangement of n^2 n^2 numbers, usually distinct integers, in a square, such that the n numbers in all rows, all columns, and both diagonals sum to the same constant. A magic square contains the integers from 1 1 to n^2 n^2 . 2 7 6 9 5 1 4 3 8 The constant sum in every row, column and diagonal is called the magic constant or magic sum, M M . The magic constant of a normal magic square depends only on n and has the following value: M = n(n^2+1)/2 M = n(n^2+1)/2 more information fo how to generate ir here Magic Square The problem \u00b6 We define a magic square to be an nXn nXn matrix of distinct positive integers from 1 1 to n^2 n^2 where the sum of any row, column, or diagonal of length n n is always equal to the same number: the magic constant. You will be given a 3x3 3x3 matrix s s of integers in the inclusive range [1,9] [1,9] . We can convert any digit a a to any other digit b b in the range [1,9] [1,9] at cost of |a -b| |a -b| . Given s s , convert it into a magic square at minimal cost. Print this cost on a new line. The answer explained \u00b6 We will assume that we need to develop just the function to calculate the difference, but no the code necessary to display the answer. Considerations we are going to create an array with all the possible combination of a Magic Square 3x3 3x3 . Steps Create an array: We are going to define an array with all the possible combination. 1 2 3 4 5 6 7 8 9 10 possibles = [ [[ 8 , 1 , 6 ],[ 3 , 5 , 7 ],[ 4 , 9 , 2 ]], [[ 6 , 1 , 8 ],[ 7 , 5 , 3 ],[ 2 , 9 , 4 ]], [[ 4 , 9 , 2 ],[ 3 , 5 , 7 ],[ 8 , 1 , 6 ]], [[ 2 , 9 , 4 ],[ 7 , 5 , 3 ],[ 6 , 1 , 8 ]], [[ 8 , 3 , 4 ],[ 1 , 5 , 9 ],[ 6 , 7 , 2 ]], [[ 4 , 3 , 8 ],[ 9 , 5 , 1 ],[ 2 , 7 , 6 ]], [[ 6 , 7 , 2 ],[ 1 , 5 , 9 ],[ 8 , 3 , 4 ]], [[ 2 , 7 , 6 ],[ 9 , 5 , 1 ],[ 4 , 3 , 8 ]] ] Create a list where I will store all the |a-b| |a-b| difference. 1 diffs = [] First loop , where we are going to extract every possible magic square variant, and define a variable call cost that is going to hold the difference from the element in the input and the element in the Magic square variant 1 2 for possible in possibles : cost = 0 Second loop , in this one we want to make a tuple that holds each row from the magic square variant and the input, for that we are going to use zip() with the possible and the s which is the input zip(possible,s) and we are going to cast it to a list() 1 for p_row , s_row in list ( zip ( possible , s )): the first iteration will be something like this: 1 [([8, 1, 6], [4, 9, 2]), ([3, 5, 7], [3, 5, 7]), ([4, 9, 2], [8, 1, 5])] ... First element is going to be the row from the magic square variant and the second will be the row from the input. Third loop , Here we are going to create a tuple with each value from the Magic square variant and the input, this will be use to compare number by number 1 for p_num , s_num in list ( zip ( p_row , s_row )): will give an output like: 1 2 3 [(8, 4), (1, 9), (6, 2)] [(3, 3), (5, 5), (7, 7)] ... Comparison , inside the third loop we are going to compare each element for the magic square variant with the each element of the input, and store the absolute total difference of all the changes done. 1 2 if p_num != s_num : cost += abs ( p_num - s_num ) Populate the diffs list now all the different cost will be use to populate the diffs list 1 2 3 4 5 6 7 for possible in possibles : cost = 0 for p_row , s_row in list ( zip ( possible , s )): for p_num , s_num in list ( zip ( p_row , s_row )): if p_num != s_num : cost += abs ( p_num - s_num ) diffs . append ( cost ) Return the minimum value in the list returning the minimum value we will return the minimum difference ask in the challenge 1 return min ( diffs ) The answer (the script) \u00b6 so the the function will be like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def formingMagicSquare ( s ): possibles = [ [[ 8 , 1 , 6 ],[ 3 , 5 , 7 ],[ 4 , 9 , 2 ]], [[ 6 , 1 , 8 ],[ 7 , 5 , 3 ],[ 2 , 9 , 4 ]], [[ 4 , 9 , 2 ],[ 3 , 5 , 7 ],[ 8 , 1 , 6 ]], [[ 2 , 9 , 4 ],[ 7 , 5 , 3 ],[ 6 , 1 , 8 ]], [[ 8 , 3 , 4 ],[ 1 , 5 , 9 ],[ 6 , 7 , 2 ]], [[ 4 , 3 , 8 ],[ 9 , 5 , 1 ],[ 2 , 7 , 6 ]], [[ 6 , 7 , 2 ],[ 1 , 5 , 9 ],[ 8 , 3 , 4 ]], [[ 2 , 7 , 6 ],[ 9 , 5 , 1 ],[ 4 , 3 , 8 ]] ] diffs = [] for possible in possibles : cost = 0 for p_row , s_row in list ( zip ( possible , s )): for p_num , s_num in list ( zip ( p_row , s_row )): if p_num != s_num : cost += abs ( p_num - s_num ) diffs . append ( cost ) print ( diffs ) return min ( diffs )","title":"Magic Square"},{"location":"Coding/Python/Algorithm/Magic_square.html#what_is_a_magic_square","text":"A magic square of order n n is an arrangement of n^2 n^2 numbers, usually distinct integers, in a square, such that the n numbers in all rows, all columns, and both diagonals sum to the same constant. A magic square contains the integers from 1 1 to n^2 n^2 . 2 7 6 9 5 1 4 3 8 The constant sum in every row, column and diagonal is called the magic constant or magic sum, M M . The magic constant of a normal magic square depends only on n and has the following value: M = n(n^2+1)/2 M = n(n^2+1)/2 more information fo how to generate ir here Magic Square","title":"What is a Magic square"},{"location":"Coding/Python/Algorithm/Magic_square.html#the_problem","text":"We define a magic square to be an nXn nXn matrix of distinct positive integers from 1 1 to n^2 n^2 where the sum of any row, column, or diagonal of length n n is always equal to the same number: the magic constant. You will be given a 3x3 3x3 matrix s s of integers in the inclusive range [1,9] [1,9] . We can convert any digit a a to any other digit b b in the range [1,9] [1,9] at cost of |a -b| |a -b| . Given s s , convert it into a magic square at minimal cost. Print this cost on a new line.","title":"The problem"},{"location":"Coding/Python/Algorithm/Magic_square.html#the_answer_explained","text":"We will assume that we need to develop just the function to calculate the difference, but no the code necessary to display the answer. Considerations we are going to create an array with all the possible combination of a Magic Square 3x3 3x3 . Steps Create an array: We are going to define an array with all the possible combination. 1 2 3 4 5 6 7 8 9 10 possibles = [ [[ 8 , 1 , 6 ],[ 3 , 5 , 7 ],[ 4 , 9 , 2 ]], [[ 6 , 1 , 8 ],[ 7 , 5 , 3 ],[ 2 , 9 , 4 ]], [[ 4 , 9 , 2 ],[ 3 , 5 , 7 ],[ 8 , 1 , 6 ]], [[ 2 , 9 , 4 ],[ 7 , 5 , 3 ],[ 6 , 1 , 8 ]], [[ 8 , 3 , 4 ],[ 1 , 5 , 9 ],[ 6 , 7 , 2 ]], [[ 4 , 3 , 8 ],[ 9 , 5 , 1 ],[ 2 , 7 , 6 ]], [[ 6 , 7 , 2 ],[ 1 , 5 , 9 ],[ 8 , 3 , 4 ]], [[ 2 , 7 , 6 ],[ 9 , 5 , 1 ],[ 4 , 3 , 8 ]] ] Create a list where I will store all the |a-b| |a-b| difference. 1 diffs = [] First loop , where we are going to extract every possible magic square variant, and define a variable call cost that is going to hold the difference from the element in the input and the element in the Magic square variant 1 2 for possible in possibles : cost = 0 Second loop , in this one we want to make a tuple that holds each row from the magic square variant and the input, for that we are going to use zip() with the possible and the s which is the input zip(possible,s) and we are going to cast it to a list() 1 for p_row , s_row in list ( zip ( possible , s )): the first iteration will be something like this: 1 [([8, 1, 6], [4, 9, 2]), ([3, 5, 7], [3, 5, 7]), ([4, 9, 2], [8, 1, 5])] ... First element is going to be the row from the magic square variant and the second will be the row from the input. Third loop , Here we are going to create a tuple with each value from the Magic square variant and the input, this will be use to compare number by number 1 for p_num , s_num in list ( zip ( p_row , s_row )): will give an output like: 1 2 3 [(8, 4), (1, 9), (6, 2)] [(3, 3), (5, 5), (7, 7)] ... Comparison , inside the third loop we are going to compare each element for the magic square variant with the each element of the input, and store the absolute total difference of all the changes done. 1 2 if p_num != s_num : cost += abs ( p_num - s_num ) Populate the diffs list now all the different cost will be use to populate the diffs list 1 2 3 4 5 6 7 for possible in possibles : cost = 0 for p_row , s_row in list ( zip ( possible , s )): for p_num , s_num in list ( zip ( p_row , s_row )): if p_num != s_num : cost += abs ( p_num - s_num ) diffs . append ( cost ) Return the minimum value in the list returning the minimum value we will return the minimum difference ask in the challenge 1 return min ( diffs )","title":"The answer explained"},{"location":"Coding/Python/Algorithm/Magic_square.html#the_answer_the_script","text":"so the the function will be like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def formingMagicSquare ( s ): possibles = [ [[ 8 , 1 , 6 ],[ 3 , 5 , 7 ],[ 4 , 9 , 2 ]], [[ 6 , 1 , 8 ],[ 7 , 5 , 3 ],[ 2 , 9 , 4 ]], [[ 4 , 9 , 2 ],[ 3 , 5 , 7 ],[ 8 , 1 , 6 ]], [[ 2 , 9 , 4 ],[ 7 , 5 , 3 ],[ 6 , 1 , 8 ]], [[ 8 , 3 , 4 ],[ 1 , 5 , 9 ],[ 6 , 7 , 2 ]], [[ 4 , 3 , 8 ],[ 9 , 5 , 1 ],[ 2 , 7 , 6 ]], [[ 6 , 7 , 2 ],[ 1 , 5 , 9 ],[ 8 , 3 , 4 ]], [[ 2 , 7 , 6 ],[ 9 , 5 , 1 ],[ 4 , 3 , 8 ]] ] diffs = [] for possible in possibles : cost = 0 for p_row , s_row in list ( zip ( possible , s )): for p_num , s_num in list ( zip ( p_row , s_row )): if p_num != s_num : cost += abs ( p_num - s_num ) diffs . append ( cost ) print ( diffs ) return min ( diffs )","title":"The answer (the script)"},{"location":"Coding/Python/Django/Django tutorial - part 01.html","text":"Verify Django is installed and what Version are we using. \u00b6 to verify which version are we using we can use: 1 python -m django --version Creating a project \u00b6 To create a project, we need: * Navigate to the location where we want to store the project, we can use cd to do so. * After navigate to the correct directory, we type: 1 django-admin startproject mysite where mysite is the name of the project. The following will be the structure of the project: 1 2 3 4 5 6 7 mysite/ manage.py mysite/ __init__.py settings.py urls.py wsgi.py this files are: the outer mysite/ root directory is just a container for your project, the name doesn\u2019t matter to Django; you can rename it to anything you like. manage.py : A command line utility that let you interact with this Django project in various ways. the inner mysite/ directory is the actual Python package for the project. mysite/__init__.py :and empty file that tells Python package that this directory should be considered a Python package. mysite/settings.py : Settings/configuration for this Django project. mysite/urls.py : The URL declarations for this Django project; a \u201ctable of contents\u201d of your Django-powered site. mysite/wsgi.py : An entry-point for WSGI-compatible web servers to serve your project. See How to deploy with WSGI for more details. Creating the Polls app \u00b6 Each application we write in Django consists of a Python package that follows a certain convention. Django comes with a utility that automatically generates the basic directory structure of an app. What\u2019s the difference between a project and an app? An app is a Web application that does something \u2013 e.g., a Weblog system, a database of public records or a simple poll app. A project is a collection of configuration and apps for a particular website. A project can contain multiple apps. An app can be in multiple projects. In this case to create an app we will need to be at the same level that the manage.py file, and we can create the app with the command: 1 python manage.py startapp polls this will create a directory like this: 1 2 3 4 5 6 7 8 9 polls/ __init__.py admin.py apps.py migrations/ __init__.py models.py tests.py views.py this directory house the poll application Write your first view \u00b6 to write the first view open the file polls/views.py and add the following code: 1 2 3 4 5 from django.http import HttpResponse def index ( request ): return HttpResponse ( \"Hello, world. You're at the polls index.\" ) now to be able to see it we will need to create the URLconf. URLconf for the polls app \u00b6 To create the URLconf in the polls directory, create a file called urls.py , so now the app directory will looks like: 1 2 3 4 5 6 7 8 9 10 polls/ __init__.py admin.py apps.py migrations/ __init__.py models.py tests.py urls.py views.py in the Polls/urls.py add: 1 2 3 4 5 6 7 from django.urls import path from . import views urlpatterns = [ path ( '' , views . index , name = 'index' ), ] The next step is to point the root URLconf at the polls.urls , to achieve that, we will add something in mysite/urls.py . we need to import django.urls.include and insert an include() in the urlpatterns list so we will have: 1 2 3 4 5 6 7 from django.contrib import admin from django.urls import include , path urlpatterns = [ path ( 'polls/' , include ( 'polls.urls' )), path ( 'admin/' , admin . site . urls ), ] The include() function allows referencing other URLconfs. Whenever Django encounters include() , it chops off whatever part of the URL matched up to that point and sends the remaining string to the included URLconf for further processing. The idea behind include() is to make it easy to plug-and-play URLs. Since polls are in their own URLconf (polls/urls.py) , they can be placed under \u201c/polls/\u201d, or under \u201c/fun_polls/\u201d, or under \u201c/content/polls/\u201d, or any other path root, and the app will still work. When to use include() ? You should always use include() when you include other URL patterns. admin.site.urls is the only exception to this. Now the index is wired to the URLconf, so we should be able to access it under http://localhost:8000/polls/ first, lets run: 1 $ python manage.py runserver The path() function is important to find the correct URL. This function pass, two argument << path('polls/',include('polls.urls')) >>, the argument are route and view . path() argument: route \u00b6 This route argument is a string that contain the URLpattern, Django will start with the first pattern in urlpatterns and make its way down the list until find one pattern that matches. GET and POST parameters are ignore, for example: http://www.example.com/myapp/ , the URLconfig will look at myapp/ . path() argument: view \u00b6 When Django finds a matching pattern, it calls the specific view function with an HttpRequest object as it first argument and any \u201ccaptured\u201d value from the route as keyword argument (check More details about project structure for more details ) path() argument: kwargs \u00b6 Arbitrary keyword argument can be passed in a dictionary to the target view path() argument: name \u00b6 Naming your URL lets you refer to it unambiguously from elsewhere in Django, especially from within templates. for more explanation of this function we can visit the documentation or the notes I made in More details about project structure","title":"Django Tutorial (part 01)"},{"location":"Coding/Python/Django/Django tutorial - part 01.html#verify_django_is_installed_and_what_version_are_we_using","text":"to verify which version are we using we can use: 1 python -m django --version","title":"Verify Django is installed and what Version are we using."},{"location":"Coding/Python/Django/Django tutorial - part 01.html#creating_a_project","text":"To create a project, we need: * Navigate to the location where we want to store the project, we can use cd to do so. * After navigate to the correct directory, we type: 1 django-admin startproject mysite where mysite is the name of the project. The following will be the structure of the project: 1 2 3 4 5 6 7 mysite/ manage.py mysite/ __init__.py settings.py urls.py wsgi.py this files are: the outer mysite/ root directory is just a container for your project, the name doesn\u2019t matter to Django; you can rename it to anything you like. manage.py : A command line utility that let you interact with this Django project in various ways. the inner mysite/ directory is the actual Python package for the project. mysite/__init__.py :and empty file that tells Python package that this directory should be considered a Python package. mysite/settings.py : Settings/configuration for this Django project. mysite/urls.py : The URL declarations for this Django project; a \u201ctable of contents\u201d of your Django-powered site. mysite/wsgi.py : An entry-point for WSGI-compatible web servers to serve your project. See How to deploy with WSGI for more details.","title":"Creating a project"},{"location":"Coding/Python/Django/Django tutorial - part 01.html#creating_the_polls_app","text":"Each application we write in Django consists of a Python package that follows a certain convention. Django comes with a utility that automatically generates the basic directory structure of an app. What\u2019s the difference between a project and an app? An app is a Web application that does something \u2013 e.g., a Weblog system, a database of public records or a simple poll app. A project is a collection of configuration and apps for a particular website. A project can contain multiple apps. An app can be in multiple projects. In this case to create an app we will need to be at the same level that the manage.py file, and we can create the app with the command: 1 python manage.py startapp polls this will create a directory like this: 1 2 3 4 5 6 7 8 9 polls/ __init__.py admin.py apps.py migrations/ __init__.py models.py tests.py views.py this directory house the poll application","title":"Creating the Polls app"},{"location":"Coding/Python/Django/Django tutorial - part 01.html#write_your_first_view","text":"to write the first view open the file polls/views.py and add the following code: 1 2 3 4 5 from django.http import HttpResponse def index ( request ): return HttpResponse ( \"Hello, world. You're at the polls index.\" ) now to be able to see it we will need to create the URLconf.","title":"Write your first view"},{"location":"Coding/Python/Django/Django tutorial - part 01.html#urlconf_for_the_polls_app","text":"To create the URLconf in the polls directory, create a file called urls.py , so now the app directory will looks like: 1 2 3 4 5 6 7 8 9 10 polls/ __init__.py admin.py apps.py migrations/ __init__.py models.py tests.py urls.py views.py in the Polls/urls.py add: 1 2 3 4 5 6 7 from django.urls import path from . import views urlpatterns = [ path ( '' , views . index , name = 'index' ), ] The next step is to point the root URLconf at the polls.urls , to achieve that, we will add something in mysite/urls.py . we need to import django.urls.include and insert an include() in the urlpatterns list so we will have: 1 2 3 4 5 6 7 from django.contrib import admin from django.urls import include , path urlpatterns = [ path ( 'polls/' , include ( 'polls.urls' )), path ( 'admin/' , admin . site . urls ), ] The include() function allows referencing other URLconfs. Whenever Django encounters include() , it chops off whatever part of the URL matched up to that point and sends the remaining string to the included URLconf for further processing. The idea behind include() is to make it easy to plug-and-play URLs. Since polls are in their own URLconf (polls/urls.py) , they can be placed under \u201c/polls/\u201d, or under \u201c/fun_polls/\u201d, or under \u201c/content/polls/\u201d, or any other path root, and the app will still work. When to use include() ? You should always use include() when you include other URL patterns. admin.site.urls is the only exception to this. Now the index is wired to the URLconf, so we should be able to access it under http://localhost:8000/polls/ first, lets run: 1 $ python manage.py runserver The path() function is important to find the correct URL. This function pass, two argument << path('polls/',include('polls.urls')) >>, the argument are route and view .","title":"URLconf for the polls app"},{"location":"Coding/Python/Django/Django tutorial - part 01.html#path_argument_route","text":"This route argument is a string that contain the URLpattern, Django will start with the first pattern in urlpatterns and make its way down the list until find one pattern that matches. GET and POST parameters are ignore, for example: http://www.example.com/myapp/ , the URLconfig will look at myapp/ .","title":"path() argument: route"},{"location":"Coding/Python/Django/Django tutorial - part 01.html#path_argument_view","text":"When Django finds a matching pattern, it calls the specific view function with an HttpRequest object as it first argument and any \u201ccaptured\u201d value from the route as keyword argument (check More details about project structure for more details )","title":"path() argument: view"},{"location":"Coding/Python/Django/Django tutorial - part 01.html#path_argument_kwargs","text":"Arbitrary keyword argument can be passed in a dictionary to the target view","title":"path() argument: kwargs"},{"location":"Coding/Python/Django/Django tutorial - part 01.html#path_argument_name","text":"Naming your URL lets you refer to it unambiguously from elsewhere in Django, especially from within templates. for more explanation of this function we can visit the documentation or the notes I made in More details about project structure","title":"path() argument: name"},{"location":"Coding/Python/Django/Django tutorial - part 02.html","text":"Database Setup \u00b6 By default Django use SQLite as a database, this will be created when is need it, therefore, if we are not going to use any other database, we won\u2019t need to do anything. Now to see the configuration related with databases we can check mysite/settings.py , here we will find several configuration, one for those configuration is about the databases. To change to a different database, we will need to change the key in DATABASES \u2018default\u2019 to match the setting of the other database: ENGINE: the default will be django.db.backends.sqlite3 but we have djando.db.backends.postgresql , django.db.backends.mysql or django.db.backends.oracle NAME: this is the name of the database, in the case of the SQLite this will be a file, and the NAME should be the absolute path to the file os.path.join(BASE_DIR,'db.sqlite3') Installed Apps \u00b6 In the same document, mysite/settings.py , we have the INSTALLED_APPS , here Django holds the names of all Django applications that are activated in this Django instance. This apps can be reuse in more projects. django.contrib.admin admin site django.contrib.auth an authentication system django.contrib.contenttypes a framework for content types django.contrib.sessions A sessions framework django.contrib.messages A messaging framework django.contrib.staticfiles A framework for managing static files some of this apps make use of database tables, therefore we need to create the table for them, this can be done with: 1 python manage.py migrate the migrate command looks at the INSTALLED_APPS settings and create any necessary database table according to the database settings in the file mysite/settings.py Creating models \u00b6 the models are the database layout, with additional metadata, using the example of the polls, we are going to create two models: Questions and Choice , Question will contain two fields questions and publication date . A Choice has two fields: the text of the choice and the votes , each choice is linked to a question. all of this concept will be implement in a Python class, in the polls/models.py we have: 1 2 3 4 5 6 7 8 9 10 11 12 from django.db import models class Question ( models . Model ): question_text = models . CharField ( max_length = 200 ) pub_date = models . DateTimeField ( 'date published' ) class Choice ( models . Model ): question = models . ForeignKey ( Question , on_delete = models . CASCADE ) choice_text = models . CharField ( max_length = 200 ) votes = models . IntegerField ( default = 0 ) Each model is represented by a class that subclass django.db.models.Model each model has a number of class variables, each of which represent a database field in the model. Each field is represented by and instance for the Field class, for example CharField for characters fields and DateTimeField for datetimes, this tells Django what type of data each fields holds. There is something to remark, in this case each field have two names, one machine-friendly format, like question_text or pub_date and a human-friendly or human-readable, in this case Question.pub_date is the only one with this human-readable name (date published). Some Field classes have required arguments, for example CharField that required max_length and others can use default to set the default value, like Choice.votes Finally the relationship between both models is done using ForeignKey , that tells Django each Choice is related to a single Question , Django support the common database relationships; many-to-one, many-to-many and one-to-one. Activating the models \u00b6 The next step is to tell Django that polls ( or the app we are creating) is going to used this database, for that we are going to add something else to the file mysite/setting.py in the section INSTALLED_APPS , we are going to add the path to the PollConfig Class ( this calse is in polls/app.py ) 1 2 3 4 5 6 7 8 9 INSTALLED_APPS = [ 'polls.apps.PollsConfig' , 'django.contrib.admin' , 'django.contrib.auth' , 'django.contrib.contenttypes' , 'django.contrib.sessions' , 'django.contrib.messages' , 'django.contrib.staticfiles' , ] now Django will include the polls app, the next step is the following command: 1 $ python manage.py makemigrations polls and we will have a result like this: the command makemigrations tell Django that you\u2019ve made some changes to your models(in this case we make a new one) Migrations is how Django store changes to the models, you can reed the changes in the file polls/migrations/001_initial.py to see the SQL the migration will run we can use 1 $ python manage.py sqlmigrate polls 0001 now we need to run the command migrate to create the models tables in the database: 1 $ python manage.py migrate The migrate command takes all the migrations that haven\u2019t been applied (Django tracks which ones are applied using a special table in your database called django_migrations ) and runs them against your database - essentially, synchronizing the changes you made to your models with the schema in the database. so there will be three steps: Change your models (in models.py ) Run python manage.py makemigrations to create the migrations for those changes. Run python manage.py migrate to apply those changes to the database. How to check what Django is going to built \u00b6 We can check what Django is going to build once we execute migration , we can do it without affecting or executing that migration, this is done with the command sqlmigrate 1 python manage.py sqlmigrate polls 0001 and we will get back a message similar to this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 BEGIN ; -- -- Create model Question -- CREATE TABLE \"polls_question\" ( \"id\" integer NOT NULL PRIMARY KEY AUTOINCREMENT , \"question_text\" varchar ( 200 ) NOT NULL , \"pub_date\" datetime NOT NULL ); -- -- Create model Choice -- CREATE TABLE \"polls_choice\" ( \"id\" integer NOT NULL PRIMARY KEY AUTOINCREMENT , \"choice_text\" varchar ( 200 ) NOT NULL , \"votes\" integer NOT NULL , \"question_id\" integer NOT NULL REFERENCES \"polls_question\" ( \"id\" ) DEFERRABLE INITIALLY DEFERRED ); CREATE INDEX \"polls_choice_question_id_c5b4b260\" ON \"polls_choice\" ( \"question_id\" ); COMMIT ; The sqlmigrate command doesn\u2019t actually run the migration on your database - it just prints it to the screen so that you can see what SQL Django thinks is required Create a User \u00b6 There are three steps to create a use, the username, the email, and the password. To start the process we need run the command 1 python manage.py createsuperuser Now create the user 1 Username: admin The desired email address 1 Email Address: admin@example.com Finally the password 1 2 3 Password: ********** Password (again): ********* Superuser created successfully. Start the Development server \u00b6 The Django Admin site is activated by default, to explore it we need to run the server: 1 python manage.py runserver additionally we can add a specific port after runserver so we can choose in which port run the server now, once the server is running we can access to it 1 http://127.0.0.1:8000/admin/ we will be receive by a screen like once we log in we will see some editable fields, this are provided it by django.contrib.auth which is the authentication framework shipped by Django Make Polls app available on Admin console \u00b6 Now we need to teel the admin that the object Questions have a admin interface, for this we will need to make some modification in polls/admin.py 1 2 3 4 from django.contrib import admin from .models import Question admin . site . register ( Question ) Now that we register the Model, in oder words we register Question we can access it on the admin console Now we can create and delete questions in this case i would like to quote directly the Django documentation: Things to note here: The form is automatically generated from the Question model. The different model field types ( DateTimeField , CharField ) correspond to the appropriate HTML input widget. Each type of field knows how to display itself in the Django admin. Each DateTimeField gets free JavaScript shortcuts. Dates get a \u201cToday\u201d shortcut and calendar popup, and times get a \u201cNow\u201d shortcut and a convenient popup that lists commonly entered times.","title":"Django Tutorial (part 02)"},{"location":"Coding/Python/Django/Django tutorial - part 02.html#database_setup","text":"By default Django use SQLite as a database, this will be created when is need it, therefore, if we are not going to use any other database, we won\u2019t need to do anything. Now to see the configuration related with databases we can check mysite/settings.py , here we will find several configuration, one for those configuration is about the databases. To change to a different database, we will need to change the key in DATABASES \u2018default\u2019 to match the setting of the other database: ENGINE: the default will be django.db.backends.sqlite3 but we have djando.db.backends.postgresql , django.db.backends.mysql or django.db.backends.oracle NAME: this is the name of the database, in the case of the SQLite this will be a file, and the NAME should be the absolute path to the file os.path.join(BASE_DIR,'db.sqlite3')","title":"Database Setup"},{"location":"Coding/Python/Django/Django tutorial - part 02.html#installed_apps","text":"In the same document, mysite/settings.py , we have the INSTALLED_APPS , here Django holds the names of all Django applications that are activated in this Django instance. This apps can be reuse in more projects. django.contrib.admin admin site django.contrib.auth an authentication system django.contrib.contenttypes a framework for content types django.contrib.sessions A sessions framework django.contrib.messages A messaging framework django.contrib.staticfiles A framework for managing static files some of this apps make use of database tables, therefore we need to create the table for them, this can be done with: 1 python manage.py migrate the migrate command looks at the INSTALLED_APPS settings and create any necessary database table according to the database settings in the file mysite/settings.py","title":"Installed Apps"},{"location":"Coding/Python/Django/Django tutorial - part 02.html#creating_models","text":"the models are the database layout, with additional metadata, using the example of the polls, we are going to create two models: Questions and Choice , Question will contain two fields questions and publication date . A Choice has two fields: the text of the choice and the votes , each choice is linked to a question. all of this concept will be implement in a Python class, in the polls/models.py we have: 1 2 3 4 5 6 7 8 9 10 11 12 from django.db import models class Question ( models . Model ): question_text = models . CharField ( max_length = 200 ) pub_date = models . DateTimeField ( 'date published' ) class Choice ( models . Model ): question = models . ForeignKey ( Question , on_delete = models . CASCADE ) choice_text = models . CharField ( max_length = 200 ) votes = models . IntegerField ( default = 0 ) Each model is represented by a class that subclass django.db.models.Model each model has a number of class variables, each of which represent a database field in the model. Each field is represented by and instance for the Field class, for example CharField for characters fields and DateTimeField for datetimes, this tells Django what type of data each fields holds. There is something to remark, in this case each field have two names, one machine-friendly format, like question_text or pub_date and a human-friendly or human-readable, in this case Question.pub_date is the only one with this human-readable name (date published). Some Field classes have required arguments, for example CharField that required max_length and others can use default to set the default value, like Choice.votes Finally the relationship between both models is done using ForeignKey , that tells Django each Choice is related to a single Question , Django support the common database relationships; many-to-one, many-to-many and one-to-one.","title":"Creating models"},{"location":"Coding/Python/Django/Django tutorial - part 02.html#activating_the_models","text":"The next step is to tell Django that polls ( or the app we are creating) is going to used this database, for that we are going to add something else to the file mysite/setting.py in the section INSTALLED_APPS , we are going to add the path to the PollConfig Class ( this calse is in polls/app.py ) 1 2 3 4 5 6 7 8 9 INSTALLED_APPS = [ 'polls.apps.PollsConfig' , 'django.contrib.admin' , 'django.contrib.auth' , 'django.contrib.contenttypes' , 'django.contrib.sessions' , 'django.contrib.messages' , 'django.contrib.staticfiles' , ] now Django will include the polls app, the next step is the following command: 1 $ python manage.py makemigrations polls and we will have a result like this: the command makemigrations tell Django that you\u2019ve made some changes to your models(in this case we make a new one) Migrations is how Django store changes to the models, you can reed the changes in the file polls/migrations/001_initial.py to see the SQL the migration will run we can use 1 $ python manage.py sqlmigrate polls 0001 now we need to run the command migrate to create the models tables in the database: 1 $ python manage.py migrate The migrate command takes all the migrations that haven\u2019t been applied (Django tracks which ones are applied using a special table in your database called django_migrations ) and runs them against your database - essentially, synchronizing the changes you made to your models with the schema in the database. so there will be three steps: Change your models (in models.py ) Run python manage.py makemigrations to create the migrations for those changes. Run python manage.py migrate to apply those changes to the database.","title":"Activating the models"},{"location":"Coding/Python/Django/Django tutorial - part 02.html#how_to_check_what_django_is_going_to_built","text":"We can check what Django is going to build once we execute migration , we can do it without affecting or executing that migration, this is done with the command sqlmigrate 1 python manage.py sqlmigrate polls 0001 and we will get back a message similar to this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 BEGIN ; -- -- Create model Question -- CREATE TABLE \"polls_question\" ( \"id\" integer NOT NULL PRIMARY KEY AUTOINCREMENT , \"question_text\" varchar ( 200 ) NOT NULL , \"pub_date\" datetime NOT NULL ); -- -- Create model Choice -- CREATE TABLE \"polls_choice\" ( \"id\" integer NOT NULL PRIMARY KEY AUTOINCREMENT , \"choice_text\" varchar ( 200 ) NOT NULL , \"votes\" integer NOT NULL , \"question_id\" integer NOT NULL REFERENCES \"polls_question\" ( \"id\" ) DEFERRABLE INITIALLY DEFERRED ); CREATE INDEX \"polls_choice_question_id_c5b4b260\" ON \"polls_choice\" ( \"question_id\" ); COMMIT ; The sqlmigrate command doesn\u2019t actually run the migration on your database - it just prints it to the screen so that you can see what SQL Django thinks is required","title":"How to check what Django is going to built"},{"location":"Coding/Python/Django/Django tutorial - part 02.html#create_a_user","text":"There are three steps to create a use, the username, the email, and the password. To start the process we need run the command 1 python manage.py createsuperuser Now create the user 1 Username: admin The desired email address 1 Email Address: admin@example.com Finally the password 1 2 3 Password: ********** Password (again): ********* Superuser created successfully.","title":"Create a User"},{"location":"Coding/Python/Django/Django tutorial - part 02.html#start_the_development_server","text":"The Django Admin site is activated by default, to explore it we need to run the server: 1 python manage.py runserver additionally we can add a specific port after runserver so we can choose in which port run the server now, once the server is running we can access to it 1 http://127.0.0.1:8000/admin/ we will be receive by a screen like once we log in we will see some editable fields, this are provided it by django.contrib.auth which is the authentication framework shipped by Django","title":"Start the Development server"},{"location":"Coding/Python/Django/Django tutorial - part 02.html#make_polls_app_available_on_admin_console","text":"Now we need to teel the admin that the object Questions have a admin interface, for this we will need to make some modification in polls/admin.py 1 2 3 4 from django.contrib import admin from .models import Question admin . site . register ( Question ) Now that we register the Model, in oder words we register Question we can access it on the admin console Now we can create and delete questions in this case i would like to quote directly the Django documentation: Things to note here: The form is automatically generated from the Question model. The different model field types ( DateTimeField , CharField ) correspond to the appropriate HTML input widget. Each type of field knows how to display itself in the Django admin. Each DateTimeField gets free JavaScript shortcuts. Dates get a \u201cToday\u201d shortcut and calendar popup, and times get a \u201cNow\u201d shortcut and a convenient popup that lists commonly entered times.","title":"Make Polls app available on Admin console"},{"location":"Coding/Python/Django/Django tutorial - part 03.html","text":"In this part we will focus in the \u201cviews\u201d, the views are a type of webpage that serve a specific function and has a specific template, for this polls example we will have four views: Question \u2018index\u2019 page: Display the latest few questions. Question \u201cdetails\u201d page: Displays a question text, with no results but with a form to vote. Question \u201cresults\u201d page: display results for a particular question vote action: handle voting for a particular choice in a particular question. Now from Django tutorial page we have: In Django, web pages and other content are delivered by views. each view is represented by simple python function (or methods, in the case of class-based views). Django will choose a view by examining the URL that \u2018s requested ( to be precise the part of the URL after the domain name) A URL pattern is simply the general form of a URL - for example: /newsarchive/<year>/<month>/ The get from URL to a View, Django use \u2018URLconfs\u2019 a URLconfs map the URL pattern to views more info in URL dispatcher Writing more views \u00b6 Now, to add more views we can go to polls/views.py and add 1 2 3 4 5 6 7 8 9 def detail ( request , question_id ): return HttpResponse ( \"You're looking at question %s \" % question_id ) def results ( request , question_id ): response = \"you're looking at the results of question %s \" return HttpResponse ( response % question_id ) def vote ( request , question_id ): return HttpResponse ( \"you're voting on question %s \" % question_id ) wire these new views into the polls.urls module by adding the path() calls: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from django.urls import path from . import views urlpatterns = [ # ex: /polls/ path ( '' , views . index , name = 'index' ), # ex: /polls/5/ path ( '<int:question_id>/' , views . detail , name = 'detail' ), # ex: /polls/5/results/ path ( '<int:question_id>/results/' , views . results , name = 'results' ), # ex: /polls/5/vote/ path ( '<int:question_id>/vote/' , views . vote , name = 'vote' ), ] When somebody requests a page from your website \u2013 say, \u201c/polls/34/\u201d , Django will load the mysite.urls Python module because it\u2019s pointed to by the ROOT_URLCONF setting. It finds the variable named urlpatterns and traverses the patterns in order. After finding the match at 'polls/' , it strips off the matching text (\"polls/\") and sends the remaining text \u2013 \"34/\" \u2013 to the \u2018polls.urls\u2019 URLconf for further processing. There it matches '<int:question_id>/' , resulting in a call to the detail() view like so: 1 detail ( request =< HttpRequest object > , question_id = 34 ) The question_id=34 part comes from <int:question_id> . Using angle brackets \u201ccaptures\u201d part of the URL and sends it as a keyword argument to the view function. The :question_id> part of the string defines the name that will be used to identify the matched pattern, and the <int: part is a converter that determines what patterns should match this part of the URL path. views that actually do something \u00b6 The view a clear responsibility, returns a HttpResponse object containing the content of the page requested, or raise a Http404 exception the rest its depends of the user. the view can record or retrieve content of a database, create zip or pdf files, can use the template system of Django or a third party, it can do pretty much everything you want and using whatever you want from the python libraries. in the words of the tutorial \u201call Django wants is that HttpResponse or and exception\u201d in this example we will continue with the build-in database, and we will display the latest 5 polls separated by comas. 1 2 3 4 5 6 7 8 from django.http import HttpResponse from .models import Question def index ( request ): latest_question_list = Question . objects . order_by ( '-pub_date' )[: 5 ] output = ', ' . join ([ q . question_text for q in latest_question_list ]) return HttpResponse ( output ) the only problem here, is that all the code is hardcoded, which will make it difficult to make modification to it, is there where the feature of Templates came handy, with Django template system we can separate the design from the python code making it easy to change the appearance without the need to change the python code. Templates and how to do it \u00b6 First, we need a new directory, it will be call templates it will be in the directory polls , Django will look for the templates in this directory. here some explanation from the Django documentation: Your project\u2019s TEMPLATES setting describes how Django will load and render templates. The default settings file configures a DjangoTemplates back-end whose APP_DIRS option is set to True . By convention DjangoTemplates looks for a \u201ctemplates\u201d subdirectory in each of the INSTALLED_APPS . Inside this templates directory we need to create a new directory called polls and inside a document call index.html , so it will be something like polls/templates/polls/index.html inside this new file, the template file, we are going to add the following code 1 2 3 4 5 6 7 8 9 10 {%if latest_question_list %} < ul > {%for question in latest_question_list %} < li >< a href = \"/polls/{{question.id}}\" > {{question.question_text}} </ a > </ li > {% endfor %} </ ul > {% else %} < p > No polls are available </ p > {% endif %} but if we run it in this way we will have a lot of errors, we need to make sure we make modification to the polls/views.py file Modify the views file \u00b6 make sure to add from .models import Question to import the models import the templates with from django.template import loader modify the function index in the following way 1 2 3 4 5 6 7 def index ( request ): latest_question_list = Question . objects . order_by ( '-pub_date' )[: 5 ] template = loader . get_template ( 'polls/index.html' ) context = { 'latest_question_list' : latest_question_list , } return HttpResponse ( template . render ( context , request )) so the script polls/views.py will be 1 2 3 4 5 6 7 8 9 10 11 12 13 from django.http import HttpResponse from django.template import loader from .models import Question def index ( request ): latest_question_list = Question . objects . order_by ( '-pub_date' )[: 5 ] template = loader . get_template ( 'polls/index.html' ) context = { 'latest_question_list' : latest_question_list , } return HttpResponse ( template . render ( context , request )) now if we go to our browser http://127.0.0.1:8888/polls/ we will see a list of the polls Shortcut render() \u00b6 There is a way to make the previous code a bit shorter, that will be using the method render() this method will take as a first argument the request object , second a template, third (optional) a dictionary, and it will give back a HttpResponse so the new index() view will be 1 2 3 4 5 6 7 8 9 from django.shortcuts import render from .models import Question def index ( request ): latest_question_list = Question . objects . order_by ( '-pub_date' )[: 5 ] context = { 'latest_question_list' : latest_question_list } return render ( request , 'polls/index.html' , context ) Raising 404 errors \u00b6 Now, for those cases where the page is not found, of the question is not found we can use the Http404 error, for that we will need to raise an exception, we are going to use the details view on polls/views.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from django.shortcuts import render #from django.template import loader #cuz we are using render() # Create your views here. from django.http import HttpResponse from .models import Question #Because we are using render() # def index(request): # latest_question_list = Question.objects.order_by('-pub_date')[:5] # template = loader.get_template('polls/index.html') # context = { # 'latest_question_list': latest_question_list, # } # return HttpResponse(template.render(context, request)) def index ( request ): latest_question_list = Question . objects . order_by ( '-pub_date' )[: 5 ] context = { 'latest_question_list' : latest_question_list } return render ( request , 'polls/index.html' , context ) def detail ( request , question_id ): try : question = Question . objects . get ( pk = question_id ) #pk stand for primary key except Question . DoesNotExist : raise Http404 ( \"Question does not exist\" ) return render ( request , 'polls/detail.html' ,{ 'question' : question }) def results ( request , question_id ): response = \"you're looking at the results of question %s \" return HttpResponse ( response % question_id ) def vote ( request , question_id ): return HttpResponse ( \"you're voting on question %s \" % question_id ) and now we need to create the template, for that we will create a new file on polls/templates/polls this file will be details.html so the full address will be polls/templates/polls/details.html and inside 1 {{question}} Shortcut get_object-or-404() \u00b6 Like with the previous scenario there is a way to make the code shorter, for that we can use get_object_or_404() get_object_or_404() take as first argument the model, this method can take several other keyword arguments that can be pass to the get() function of the model\u2019s manager. It raise the Http404 if the object doesn\u2019t exist so the modification to the details() view will be 1 2 3 def detail ( request , question_id ): question = get_object_or_404 ( Question , pk = question_id ) return render ( request , 'polls/detail.html' , { 'question' : question }) making the hold file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 from django.shortcuts import render #from django.template import loader #cuz we are using render() # Create your views here. from django.http import HttpResponse from .models import Question #Because we are using render() # def index(request): # latest_question_list = Question.objects.order_by('-pub_date')[:5] # template = loader.get_template('polls/index.html') # context = { # 'latest_question_list': latest_question_list, # } # return HttpResponse(template.render(context, request)) def index ( request ): latest_question_list = Question . objects . order_by ( '-pub_date' )[: 5 ] context = { 'latest_question_list' : latest_question_list } return render ( request , 'polls/index.html' , context ) # because we use get_object_or_404() # def detail(request, question_id): # try: # question = Question.objects.get(pk=question_id) #pk stand for primary key # except Question.DoesNotExist: # raise Http404(\"Question does not exist\") # return render(request, 'polls/detail.html',{'question': question}) def detail ( request , question_id ): question = get_object_or_404 ( Question , pk = question_id ) return render ( request , 'polls/detail.html' , { 'question' : question }) def results ( request , question_id ): response = \"you're looking at the results of question %s \" return HttpResponse ( response % question_id ) def vote ( request , question_id ): return HttpResponse ( \"you're voting on question %s \" % question_id ) There\u2019s also a get_list_or_404() function, which works just as get_object_or_404() \u2013 except using filter() instead of get() . It raises Http404 if the list is empty. Using the template system - detail() \u00b6 now using the context variable question the template polls/detail.html will be 1 2 3 4 5 6 < h1 > {{question.question_text}} </ h1 > < ul > {% for choice in question.choice_set.all %} < li > {{ choice.choice_text}} </ li > {% endfor %} </ ul > In the templates the dot-lookup notation is use to access variable attributes, example {{ question.question_text}} , Django first make a dictionary lookup if this fail it will do a list lookup. Method-calling happens in the {% for %} loop: question.choice_set.all will create a iterable of the object Choice the statement is interpreted by python as questionn.choice_set.all() this iterable object is suitable for the {% for %} . Documentation about templates here template guide Brief view to The Django template language \u00b6 Template are rendered in a context, Rendering replace the content of the variables of this context, tags are execute and then everything else is output as is. There are 4 constructs in Django: Variables Tags Filters Comments Variables \u00b6 The variables are placeholders that going to be located between \u201c{{\u201d and \u201c}}\u201d, the output of this variables it is going to depend of the context, which is a dict-like object that map key values, everything will be clear with the example Let say the context is: 1 {'first_name': 'John', 'last_name': 'Doe'} and we have something like 1 My first name is {{ first_name }}. My last name is {{ last_name }}. we will have an answer like 1 My first name is John. My last name is Doe. The notation implemented for Dictionary lookup, object lookup and list lookup will be 1 2 3 {{ my_dict.key }} {{ my_object.attribute }} {{ my_list.0 }} Tag \u00b6 Provide arbitrary logic in the rendering process. The definition is vague on purpose, this can serve control structures, get database data or access other templates It is surrounded by \u201c{%\u201d and \u201c%}\u201d 1 {% if user.is_authenticated %}Hello, {{ user.username }}.{% endif %} More information about Tag Filter \u00b6 Filters transform the values of the variable or the tags for a filter like: 1 {{ django|title }} with a context like: 1 {'django': 'the web framework for perfectionists with deadlines'} result will be 1 The Web Framework For Perfectionists With Deadlines more information about Filter Commets \u00b6 The comments look like 1 {# comments #} for a multi-line comments 1 {% comments %} Removing hardcore URLs in templates \u00b6 To maintain a loosely couple approach we will, need to modify the hard-code that we have in the polls/index.html template, this can be achieve thank to the name argument in the path() function in the pools.urls module, we will use the tag url to do this in the template. so from a template like 1 < li >< a href = \"/polls/{{ question.id }}/\" > {{ question.question_text }} </ a ></ li > we have a template like 1 < li >< a href = \"{% url 'detail' question.id %}\" > {{ question.question_text }} </ a ></ li > this works because Django will look the URL definition as specified in polls.urls module. and the we can see \u2018detail\u2019 defined 1 2 3 4 ... # the 'name' value as called by the {% url %} template tag path ( '<int:question_id>/' , views . detail , name = 'detail' ), ... so if in the future we decided to change the url of the polls we can do it in polls/urls.py and just in that file Namespacing URL names \u00b6 In this example we have just one app, but in a real prohject we will have more than one,in that case how we make the system know which details template to grap, in that case we use app_name , for that we need to make changes in polls/urls.py 1 2 3 4 5 6 7 8 9 10 11 from django.urls import path from . import views app_name = 'polls' urlpatterns = [ path ( '' , views . index , name = 'index' ), path ( '<int:question_id>/' , views . detail , name = 'detail' ), path ( '<int:question_id>/results/' , views . results , name = 'results' ), path ( '<int:question_id>/vote/' , views . vote , name = 'vote' ), ] in this case we added app_name = 'polls' now we need to change polls/index.html from 1 < li >< a href = \"{% url 'detail' question.id %}\" > {{ question.question_text }} </ a ></ li > to 1 < li >< a href = \"{% url 'polls:detail' question.id %}\" > {{ question.question_text }} </ a ></ li >","title":"Django Tutorial (part 03)"},{"location":"Coding/Python/Django/Django tutorial - part 03.html#writing_more_views","text":"Now, to add more views we can go to polls/views.py and add 1 2 3 4 5 6 7 8 9 def detail ( request , question_id ): return HttpResponse ( \"You're looking at question %s \" % question_id ) def results ( request , question_id ): response = \"you're looking at the results of question %s \" return HttpResponse ( response % question_id ) def vote ( request , question_id ): return HttpResponse ( \"you're voting on question %s \" % question_id ) wire these new views into the polls.urls module by adding the path() calls: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from django.urls import path from . import views urlpatterns = [ # ex: /polls/ path ( '' , views . index , name = 'index' ), # ex: /polls/5/ path ( '<int:question_id>/' , views . detail , name = 'detail' ), # ex: /polls/5/results/ path ( '<int:question_id>/results/' , views . results , name = 'results' ), # ex: /polls/5/vote/ path ( '<int:question_id>/vote/' , views . vote , name = 'vote' ), ] When somebody requests a page from your website \u2013 say, \u201c/polls/34/\u201d , Django will load the mysite.urls Python module because it\u2019s pointed to by the ROOT_URLCONF setting. It finds the variable named urlpatterns and traverses the patterns in order. After finding the match at 'polls/' , it strips off the matching text (\"polls/\") and sends the remaining text \u2013 \"34/\" \u2013 to the \u2018polls.urls\u2019 URLconf for further processing. There it matches '<int:question_id>/' , resulting in a call to the detail() view like so: 1 detail ( request =< HttpRequest object > , question_id = 34 ) The question_id=34 part comes from <int:question_id> . Using angle brackets \u201ccaptures\u201d part of the URL and sends it as a keyword argument to the view function. The :question_id> part of the string defines the name that will be used to identify the matched pattern, and the <int: part is a converter that determines what patterns should match this part of the URL path.","title":"Writing more views"},{"location":"Coding/Python/Django/Django tutorial - part 03.html#views_that_actually_do_something","text":"The view a clear responsibility, returns a HttpResponse object containing the content of the page requested, or raise a Http404 exception the rest its depends of the user. the view can record or retrieve content of a database, create zip or pdf files, can use the template system of Django or a third party, it can do pretty much everything you want and using whatever you want from the python libraries. in the words of the tutorial \u201call Django wants is that HttpResponse or and exception\u201d in this example we will continue with the build-in database, and we will display the latest 5 polls separated by comas. 1 2 3 4 5 6 7 8 from django.http import HttpResponse from .models import Question def index ( request ): latest_question_list = Question . objects . order_by ( '-pub_date' )[: 5 ] output = ', ' . join ([ q . question_text for q in latest_question_list ]) return HttpResponse ( output ) the only problem here, is that all the code is hardcoded, which will make it difficult to make modification to it, is there where the feature of Templates came handy, with Django template system we can separate the design from the python code making it easy to change the appearance without the need to change the python code.","title":"views that actually do something"},{"location":"Coding/Python/Django/Django tutorial - part 03.html#templates_and_how_to_do_it","text":"First, we need a new directory, it will be call templates it will be in the directory polls , Django will look for the templates in this directory. here some explanation from the Django documentation: Your project\u2019s TEMPLATES setting describes how Django will load and render templates. The default settings file configures a DjangoTemplates back-end whose APP_DIRS option is set to True . By convention DjangoTemplates looks for a \u201ctemplates\u201d subdirectory in each of the INSTALLED_APPS . Inside this templates directory we need to create a new directory called polls and inside a document call index.html , so it will be something like polls/templates/polls/index.html inside this new file, the template file, we are going to add the following code 1 2 3 4 5 6 7 8 9 10 {%if latest_question_list %} < ul > {%for question in latest_question_list %} < li >< a href = \"/polls/{{question.id}}\" > {{question.question_text}} </ a > </ li > {% endfor %} </ ul > {% else %} < p > No polls are available </ p > {% endif %} but if we run it in this way we will have a lot of errors, we need to make sure we make modification to the polls/views.py file","title":"Templates and how to do it"},{"location":"Coding/Python/Django/Django tutorial - part 03.html#modify_the_views_file","text":"make sure to add from .models import Question to import the models import the templates with from django.template import loader modify the function index in the following way 1 2 3 4 5 6 7 def index ( request ): latest_question_list = Question . objects . order_by ( '-pub_date' )[: 5 ] template = loader . get_template ( 'polls/index.html' ) context = { 'latest_question_list' : latest_question_list , } return HttpResponse ( template . render ( context , request )) so the script polls/views.py will be 1 2 3 4 5 6 7 8 9 10 11 12 13 from django.http import HttpResponse from django.template import loader from .models import Question def index ( request ): latest_question_list = Question . objects . order_by ( '-pub_date' )[: 5 ] template = loader . get_template ( 'polls/index.html' ) context = { 'latest_question_list' : latest_question_list , } return HttpResponse ( template . render ( context , request )) now if we go to our browser http://127.0.0.1:8888/polls/ we will see a list of the polls","title":"Modify the views file"},{"location":"Coding/Python/Django/Django tutorial - part 03.html#shortcut_render","text":"There is a way to make the previous code a bit shorter, that will be using the method render() this method will take as a first argument the request object , second a template, third (optional) a dictionary, and it will give back a HttpResponse so the new index() view will be 1 2 3 4 5 6 7 8 9 from django.shortcuts import render from .models import Question def index ( request ): latest_question_list = Question . objects . order_by ( '-pub_date' )[: 5 ] context = { 'latest_question_list' : latest_question_list } return render ( request , 'polls/index.html' , context )","title":"Shortcut render()"},{"location":"Coding/Python/Django/Django tutorial - part 03.html#raising_404_errors","text":"Now, for those cases where the page is not found, of the question is not found we can use the Http404 error, for that we will need to raise an exception, we are going to use the details view on polls/views.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from django.shortcuts import render #from django.template import loader #cuz we are using render() # Create your views here. from django.http import HttpResponse from .models import Question #Because we are using render() # def index(request): # latest_question_list = Question.objects.order_by('-pub_date')[:5] # template = loader.get_template('polls/index.html') # context = { # 'latest_question_list': latest_question_list, # } # return HttpResponse(template.render(context, request)) def index ( request ): latest_question_list = Question . objects . order_by ( '-pub_date' )[: 5 ] context = { 'latest_question_list' : latest_question_list } return render ( request , 'polls/index.html' , context ) def detail ( request , question_id ): try : question = Question . objects . get ( pk = question_id ) #pk stand for primary key except Question . DoesNotExist : raise Http404 ( \"Question does not exist\" ) return render ( request , 'polls/detail.html' ,{ 'question' : question }) def results ( request , question_id ): response = \"you're looking at the results of question %s \" return HttpResponse ( response % question_id ) def vote ( request , question_id ): return HttpResponse ( \"you're voting on question %s \" % question_id ) and now we need to create the template, for that we will create a new file on polls/templates/polls this file will be details.html so the full address will be polls/templates/polls/details.html and inside 1 {{question}}","title":"Raising 404 errors"},{"location":"Coding/Python/Django/Django tutorial - part 03.html#shortcut_get_object-or-404","text":"Like with the previous scenario there is a way to make the code shorter, for that we can use get_object_or_404() get_object_or_404() take as first argument the model, this method can take several other keyword arguments that can be pass to the get() function of the model\u2019s manager. It raise the Http404 if the object doesn\u2019t exist so the modification to the details() view will be 1 2 3 def detail ( request , question_id ): question = get_object_or_404 ( Question , pk = question_id ) return render ( request , 'polls/detail.html' , { 'question' : question }) making the hold file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 from django.shortcuts import render #from django.template import loader #cuz we are using render() # Create your views here. from django.http import HttpResponse from .models import Question #Because we are using render() # def index(request): # latest_question_list = Question.objects.order_by('-pub_date')[:5] # template = loader.get_template('polls/index.html') # context = { # 'latest_question_list': latest_question_list, # } # return HttpResponse(template.render(context, request)) def index ( request ): latest_question_list = Question . objects . order_by ( '-pub_date' )[: 5 ] context = { 'latest_question_list' : latest_question_list } return render ( request , 'polls/index.html' , context ) # because we use get_object_or_404() # def detail(request, question_id): # try: # question = Question.objects.get(pk=question_id) #pk stand for primary key # except Question.DoesNotExist: # raise Http404(\"Question does not exist\") # return render(request, 'polls/detail.html',{'question': question}) def detail ( request , question_id ): question = get_object_or_404 ( Question , pk = question_id ) return render ( request , 'polls/detail.html' , { 'question' : question }) def results ( request , question_id ): response = \"you're looking at the results of question %s \" return HttpResponse ( response % question_id ) def vote ( request , question_id ): return HttpResponse ( \"you're voting on question %s \" % question_id ) There\u2019s also a get_list_or_404() function, which works just as get_object_or_404() \u2013 except using filter() instead of get() . It raises Http404 if the list is empty.","title":"Shortcut get_object-or-404()"},{"location":"Coding/Python/Django/Django tutorial - part 03.html#using_the_template_system_-_detail","text":"now using the context variable question the template polls/detail.html will be 1 2 3 4 5 6 < h1 > {{question.question_text}} </ h1 > < ul > {% for choice in question.choice_set.all %} < li > {{ choice.choice_text}} </ li > {% endfor %} </ ul > In the templates the dot-lookup notation is use to access variable attributes, example {{ question.question_text}} , Django first make a dictionary lookup if this fail it will do a list lookup. Method-calling happens in the {% for %} loop: question.choice_set.all will create a iterable of the object Choice the statement is interpreted by python as questionn.choice_set.all() this iterable object is suitable for the {% for %} . Documentation about templates here template guide","title":"Using the template system  - detail()"},{"location":"Coding/Python/Django/Django tutorial - part 03.html#brief_view_to_the_django_template_language","text":"Template are rendered in a context, Rendering replace the content of the variables of this context, tags are execute and then everything else is output as is. There are 4 constructs in Django: Variables Tags Filters Comments","title":"Brief view to The Django template language"},{"location":"Coding/Python/Django/Django tutorial - part 03.html#variables","text":"The variables are placeholders that going to be located between \u201c{{\u201d and \u201c}}\u201d, the output of this variables it is going to depend of the context, which is a dict-like object that map key values, everything will be clear with the example Let say the context is: 1 {'first_name': 'John', 'last_name': 'Doe'} and we have something like 1 My first name is {{ first_name }}. My last name is {{ last_name }}. we will have an answer like 1 My first name is John. My last name is Doe. The notation implemented for Dictionary lookup, object lookup and list lookup will be 1 2 3 {{ my_dict.key }} {{ my_object.attribute }} {{ my_list.0 }}","title":"Variables"},{"location":"Coding/Python/Django/Django tutorial - part 03.html#tag","text":"Provide arbitrary logic in the rendering process. The definition is vague on purpose, this can serve control structures, get database data or access other templates It is surrounded by \u201c{%\u201d and \u201c%}\u201d 1 {% if user.is_authenticated %}Hello, {{ user.username }}.{% endif %} More information about Tag","title":"Tag"},{"location":"Coding/Python/Django/Django tutorial - part 03.html#filter","text":"Filters transform the values of the variable or the tags for a filter like: 1 {{ django|title }} with a context like: 1 {'django': 'the web framework for perfectionists with deadlines'} result will be 1 The Web Framework For Perfectionists With Deadlines more information about Filter","title":"Filter"},{"location":"Coding/Python/Django/Django tutorial - part 03.html#commets","text":"The comments look like 1 {# comments #} for a multi-line comments 1 {% comments %}","title":"Commets"},{"location":"Coding/Python/Django/Django tutorial - part 03.html#removing_hardcore_urls_in_templates","text":"To maintain a loosely couple approach we will, need to modify the hard-code that we have in the polls/index.html template, this can be achieve thank to the name argument in the path() function in the pools.urls module, we will use the tag url to do this in the template. so from a template like 1 < li >< a href = \"/polls/{{ question.id }}/\" > {{ question.question_text }} </ a ></ li > we have a template like 1 < li >< a href = \"{% url 'detail' question.id %}\" > {{ question.question_text }} </ a ></ li > this works because Django will look the URL definition as specified in polls.urls module. and the we can see \u2018detail\u2019 defined 1 2 3 4 ... # the 'name' value as called by the {% url %} template tag path ( '<int:question_id>/' , views . detail , name = 'detail' ), ... so if in the future we decided to change the url of the polls we can do it in polls/urls.py and just in that file","title":"Removing hardcore URLs in templates"},{"location":"Coding/Python/Django/Django tutorial - part 03.html#namespacing_url_names","text":"In this example we have just one app, but in a real prohject we will have more than one,in that case how we make the system know which details template to grap, in that case we use app_name , for that we need to make changes in polls/urls.py 1 2 3 4 5 6 7 8 9 10 11 from django.urls import path from . import views app_name = 'polls' urlpatterns = [ path ( '' , views . index , name = 'index' ), path ( '<int:question_id>/' , views . detail , name = 'detail' ), path ( '<int:question_id>/results/' , views . results , name = 'results' ), path ( '<int:question_id>/vote/' , views . vote , name = 'vote' ), ] in this case we added app_name = 'polls' now we need to change polls/index.html from 1 < li >< a href = \"{% url 'detail' question.id %}\" > {{ question.question_text }} </ a ></ li > to 1 < li >< a href = \"{% url 'polls:detail' question.id %}\" > {{ question.question_text }} </ a ></ li >","title":"Namespacing URL names"},{"location":"Coding/Python/Django/More details about project structure.html","text":"More Details about the project structure and essencial methods/files \u00b6 django-admin and manage.py \u00b6 django-admin is the Django\u2019s command-line utility, the manage.py is created automatically when the project is created and it does the same thing as django-admin. Usage 1 2 $ django-admin <command> [options] $ manage.py <command> [option] Getting runtime help 1 django-admin help Run django-admin help to display usage information and a list of commands Run django-admin help --commands to display a list of all available commands determining the version 1 django-admin version to get the version use in this project. Brief view to URL dispatcher \u00b6 To design URLs for an app, you create a Python module informally called a URLconf (URL configuration). This module is pure Python code and is a mapping between URL path expressions to Python functions (your views). How Django processes a request \u00b6 When a user requests a page from your Django-powered site, this is the algorithm the system follows to determine which Python code to execute: Django determines the root URLconf module to use. Ordinarily, this is the value of the ROOT_URLCONF setting, but if the incoming HttpRequest object has a urlconf attribute (set by middleware), its value will be used in place of the ROOT_URLCONF setting. Django loads that Python module and looks for the variable urlpatterns . This should be a sequence of django.urls.path() and/or django.urls.re_path() instances. Django runs through each URL pattern, in order, and stops at the first one that matches the requested URL. Once one of the URL patterns matches, Django imports and calls the given view, which is a simple Python function (or a class-based view). The view gets passed the following arguments: An instance of HttpRequest . If the matched URL pattern returned no named groups, then the matches from the regular expression are provided as positional arguments. The keyword arguments are made up of any named parts matched by the path expression, overridden by any arguments specified in the optional kwargs argument to django.urls.path() or django.urls.re_path() . If no URL pattern matches, or if an exception is raised during any point in this process, Django invokes an appropriate error-handling view. Example 1 2 3 4 5 6 7 8 9 10 from django.urls import path from . import views urlpatterns = [ path ( 'articles/2003/' , views . special_case_2003 ), path ( 'articles/<int:year>/' , views . year_archive ), path ( 'articles/<int:year>/<int:month>/' , views . month_archive ), path ( 'articles/<int:year>/<int:month>/<slug:slug>/' , views . article_detail ), ] Notes: To capture a value from the URL, use angle brackets. Captured values can optionally include a converter type. For example, use to capture an integer parameter. If a converter isn\u2019t included, any string, excluding a / character, is matched. There\u2019s no need to add a leading slash, because every URL has that. For example, it\u2019s articles , not /articles . Example requests: A request to /articles/2005/03/ would match the third entry in the list. Django would call the function views.month_archive(request, year=2005, month=3) . /articles/2003/ would match the first pattern in the list, not the second one, because the patterns are tested in order, and the first one is the first test to pass. Feel free to exploit the ordering to insert special cases like this. Here, Django would call the function views.special_case_2003(request) /articles/2003 would not match any of these patterns, because each pattern requires that the URL end with a slash. /articles/2003/03/building-a-django-site/ would match the final pattern. Django would call the function views.article_detail(request, year=2003, month=3, slug=\"building-a-django-site\") . Path converters\u00b6 \u00b6 The following path converters are available by default: str - Matches any non-empty string, excluding the path separator, \u2018/\u2019. This is the default if a converter isn\u2019t included in the expression. int - Matches zero or any positive integer. Returns an int. slug - Matches any slug string consisting of ASCII letters or numbers, plus the hyphen and underscore characters. For example, building-your-1st-django-site. uuid - Matches a formatted UUID. To prevent multiple URLs from mapping to the same page, dashes must be included and letters must be lowercase. For example, 075194d3-6885-417e-a8a8-6c931e272f00 . Returns a UUID instance. path - Matches any non-empty string, including the path separator, \u2019/\u2019 . This allows you to match against a complete URL path rather than just a segment of a URL path as with str. path() \u00b6 path(route,view,kwarg=None,name=None) Returns an element for inclussion in urlpatters , for example: 1 2 3 4 5 6 7 8 9 10 from django.urls import include , path urlpatterns = [ path ( 'index/' , views . index , name = 'main-view' ), path ( 'bio/<username>/' , views . bio , name = 'bio' ), path ( 'articles/<slug:title>/' , views . article , name = 'article-detail' ), path ( 'articles/<slug:title>/<int:section>/' , views . section , name = 'article-section' ), path ( 'weblog/' , include ( 'blog.urls' )), ... ] The route is in more cases a string, this string can contain anlge brakets, like <username> in the example above, this can be use to capture a part of the URL and send it as a keyword argument to the view, this angle brakes may include converter specification, like <int:section> , in this case the it will catch or match a string of decimal digits and converts the value to a int . The view argument is a view that will be the result of the match, this argument can be a include() The kwargs argument allows you to pass additional arguments to the view function or method, more info here . The name is an useful argument to name the URL and its advantage are mentioned here more versions of path() such as re_path() for regular expression adn more information about include() can be find in django.urls.path","title":"(Optional) More details about file structure"},{"location":"Coding/Python/Django/More details about project structure.html#more_details_about_the_project_structure_and_essencial_methodsfiles","text":"","title":"More Details about the project structure and  essencial methods/files"},{"location":"Coding/Python/Django/More details about project structure.html#django-admin_and_managepy","text":"django-admin is the Django\u2019s command-line utility, the manage.py is created automatically when the project is created and it does the same thing as django-admin. Usage 1 2 $ django-admin <command> [options] $ manage.py <command> [option] Getting runtime help 1 django-admin help Run django-admin help to display usage information and a list of commands Run django-admin help --commands to display a list of all available commands determining the version 1 django-admin version to get the version use in this project.","title":"django-admin and manage.py"},{"location":"Coding/Python/Django/More details about project structure.html#brief_view_to_url_dispatcher","text":"To design URLs for an app, you create a Python module informally called a URLconf (URL configuration). This module is pure Python code and is a mapping between URL path expressions to Python functions (your views).","title":"Brief view to URL dispatcher"},{"location":"Coding/Python/Django/More details about project structure.html#how_django_processes_a_request","text":"When a user requests a page from your Django-powered site, this is the algorithm the system follows to determine which Python code to execute: Django determines the root URLconf module to use. Ordinarily, this is the value of the ROOT_URLCONF setting, but if the incoming HttpRequest object has a urlconf attribute (set by middleware), its value will be used in place of the ROOT_URLCONF setting. Django loads that Python module and looks for the variable urlpatterns . This should be a sequence of django.urls.path() and/or django.urls.re_path() instances. Django runs through each URL pattern, in order, and stops at the first one that matches the requested URL. Once one of the URL patterns matches, Django imports and calls the given view, which is a simple Python function (or a class-based view). The view gets passed the following arguments: An instance of HttpRequest . If the matched URL pattern returned no named groups, then the matches from the regular expression are provided as positional arguments. The keyword arguments are made up of any named parts matched by the path expression, overridden by any arguments specified in the optional kwargs argument to django.urls.path() or django.urls.re_path() . If no URL pattern matches, or if an exception is raised during any point in this process, Django invokes an appropriate error-handling view. Example 1 2 3 4 5 6 7 8 9 10 from django.urls import path from . import views urlpatterns = [ path ( 'articles/2003/' , views . special_case_2003 ), path ( 'articles/<int:year>/' , views . year_archive ), path ( 'articles/<int:year>/<int:month>/' , views . month_archive ), path ( 'articles/<int:year>/<int:month>/<slug:slug>/' , views . article_detail ), ] Notes: To capture a value from the URL, use angle brackets. Captured values can optionally include a converter type. For example, use to capture an integer parameter. If a converter isn\u2019t included, any string, excluding a / character, is matched. There\u2019s no need to add a leading slash, because every URL has that. For example, it\u2019s articles , not /articles . Example requests: A request to /articles/2005/03/ would match the third entry in the list. Django would call the function views.month_archive(request, year=2005, month=3) . /articles/2003/ would match the first pattern in the list, not the second one, because the patterns are tested in order, and the first one is the first test to pass. Feel free to exploit the ordering to insert special cases like this. Here, Django would call the function views.special_case_2003(request) /articles/2003 would not match any of these patterns, because each pattern requires that the URL end with a slash. /articles/2003/03/building-a-django-site/ would match the final pattern. Django would call the function views.article_detail(request, year=2003, month=3, slug=\"building-a-django-site\") .","title":"How Django processes a request"},{"location":"Coding/Python/Django/More details about project structure.html#path_converters","text":"The following path converters are available by default: str - Matches any non-empty string, excluding the path separator, \u2018/\u2019. This is the default if a converter isn\u2019t included in the expression. int - Matches zero or any positive integer. Returns an int. slug - Matches any slug string consisting of ASCII letters or numbers, plus the hyphen and underscore characters. For example, building-your-1st-django-site. uuid - Matches a formatted UUID. To prevent multiple URLs from mapping to the same page, dashes must be included and letters must be lowercase. For example, 075194d3-6885-417e-a8a8-6c931e272f00 . Returns a UUID instance. path - Matches any non-empty string, including the path separator, \u2019/\u2019 . This allows you to match against a complete URL path rather than just a segment of a URL path as with str.","title":"Path converters\u00b6"},{"location":"Coding/Python/Django/More details about project structure.html#path","text":"path(route,view,kwarg=None,name=None) Returns an element for inclussion in urlpatters , for example: 1 2 3 4 5 6 7 8 9 10 from django.urls import include , path urlpatterns = [ path ( 'index/' , views . index , name = 'main-view' ), path ( 'bio/<username>/' , views . bio , name = 'bio' ), path ( 'articles/<slug:title>/' , views . article , name = 'article-detail' ), path ( 'articles/<slug:title>/<int:section>/' , views . section , name = 'article-section' ), path ( 'weblog/' , include ( 'blog.urls' )), ... ] The route is in more cases a string, this string can contain anlge brakets, like <username> in the example above, this can be use to capture a part of the URL and send it as a keyword argument to the view, this angle brakes may include converter specification, like <int:section> , in this case the it will catch or match a string of decimal digits and converts the value to a int . The view argument is a view that will be the result of the match, this argument can be a include() The kwargs argument allows you to pass additional arguments to the view function or method, more info here . The name is an useful argument to name the URL and its advantage are mentioned here more versions of path() such as re_path() for regular expression adn more information about include() can be find in django.urls.path","title":"path()"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 1.html","text":"Lets start with some basic concept that are present in bold in the course Machine language : LAnguage use for the computers, very rudimentary. Natural Language : Language use by people, it is constantly evolving, new words are created every day and old one disappear. Instruction list (IL): a complete set of know commands What is a language \u00b6 I paraphrased the original information of the course since it is condense and simple An Alphabet: A set of symbols use to build a word. A Lexis: also known as dictionary, it is a set of words available in to be use. A Syntax: A set of rules used to determine if a certain string of words make a valid sentence Semantics: a set of rules to determining if a certain phrase make sense. The IL o Instruction list is the alphabet for the machine, this is the simplest set of symbols that can be use to give commands to the computer. In order to give instruction to the computer we need a intermediary language, complex enough to be human readable, simple enough so computer can follow, this languages are called High-Level programming Language , a program written with this language is call source code and a file containing this code will be a source file . Compilation vs. Interpretation \u00b6 A code or source code must be correct in many senses: Alphabetically: A program must be written in recognizable script, using recognizable character, as example Roman alphabet. Lexically: Each programming language ahas a dictionary and one must master it, although is simpler than a natural language dictionary. Syntactically: Each language has it rules and must be obeyed Semantically: The program has to make sense Assuming the program is written correctly, now we need the computer to translate it to machine language, this transformation from high-Level Programming Language into Machine Language . Compilation \u00b6 The source code is translate once (but this act needs to be repeated each time we modify the code) by getting a file, that contain the machine code, now the code can be distribute. Interpretation \u00b6 We can translate the source program each time it has to be run; the program performing this kind of transformation is called an interpreter, as it interprets the code every time it is intended to be executed; it also means that you cannot just distribute the source code as-is , because the end-user also needs the interpreter to execute it. :\u2013Compilation\u2013: Interpretation * The execution of the translated code is usually faster; * You can run the code as soon as you complete it - there are no additional phases of translation; ADVANTAGES * Only the user has to have the compiler - the end-user may use the code without it; * The code is stored using programming language, not the machine one - this means that it can be run on computers using different machine languages; you don\u2019t compile your code separately for each different architecture. * The translated code is stored using machine language - as it is very hard to understand it, your own inventions and programming tricks are likely to remain your secret. DISADVANTAGES * The compilation itself may be a very time-consuming process - you may not be able to run your code immediately after any amendment; *don\u2019t expect that interpretation will ramp your code to high speed - your code will share the computer\u2019s power with the interpreter, so it can\u2019t be really fast; * You have to have as many compilers as hardware platforms you want your code to be run on. * Both you and the end user have to have the interpreter to run your code. Python is an Interpreted Language you just need the Python Interpreter Python Goals \u00b6 An Easy and Intuitive yet powerful language. Open Source Understandable code, just like plain English. Suitable for Everyday programming . Python, CPhyton and Cython \u00b6 Python is maintain for people around the PSF or Python Software Foundation, the president is the creator of the language itself, Guido van Rossum , these Pythons are called Canonical Python was implemented in C, it was, it is, and, probably continue been, implemented in C, this is why it is often refer as CPython Cython \u00b6 Cython is other family member of Python, it is a solution for many trade off, such the lack of efficiency, once the code is written in Python, and it is prove that there is not issue, one can translate this code to C. This is what Cython is intended to do so, translate python code to C. The last two \u201cflavors\u201d of python mentioned in the course are Jython and PyPy . Jython and pypy \u00b6 Jython is similar to Cython, but instead of translate the python script to C, this will translate to java, it is specially use in big systems, where java is the main language, this Flavor, Jython just support Python 2 and there is not implementation for Python3 so far. pypy it is an environment written in RPython and is mostly use for develop python, it doesn\u2019t run the interpreter it translate everything to C.","title":"Module 1"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 1.html#what_is_a_language","text":"I paraphrased the original information of the course since it is condense and simple An Alphabet: A set of symbols use to build a word. A Lexis: also known as dictionary, it is a set of words available in to be use. A Syntax: A set of rules used to determine if a certain string of words make a valid sentence Semantics: a set of rules to determining if a certain phrase make sense. The IL o Instruction list is the alphabet for the machine, this is the simplest set of symbols that can be use to give commands to the computer. In order to give instruction to the computer we need a intermediary language, complex enough to be human readable, simple enough so computer can follow, this languages are called High-Level programming Language , a program written with this language is call source code and a file containing this code will be a source file .","title":"What is a language"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 1.html#compilation_vs_interpretation","text":"A code or source code must be correct in many senses: Alphabetically: A program must be written in recognizable script, using recognizable character, as example Roman alphabet. Lexically: Each programming language ahas a dictionary and one must master it, although is simpler than a natural language dictionary. Syntactically: Each language has it rules and must be obeyed Semantically: The program has to make sense Assuming the program is written correctly, now we need the computer to translate it to machine language, this transformation from high-Level Programming Language into Machine Language .","title":"Compilation vs. Interpretation"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 1.html#compilation","text":"The source code is translate once (but this act needs to be repeated each time we modify the code) by getting a file, that contain the machine code, now the code can be distribute.","title":"Compilation"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 1.html#interpretation","text":"We can translate the source program each time it has to be run; the program performing this kind of transformation is called an interpreter, as it interprets the code every time it is intended to be executed; it also means that you cannot just distribute the source code as-is , because the end-user also needs the interpreter to execute it. :\u2013Compilation\u2013: Interpretation * The execution of the translated code is usually faster; * You can run the code as soon as you complete it - there are no additional phases of translation; ADVANTAGES * Only the user has to have the compiler - the end-user may use the code without it; * The code is stored using programming language, not the machine one - this means that it can be run on computers using different machine languages; you don\u2019t compile your code separately for each different architecture. * The translated code is stored using machine language - as it is very hard to understand it, your own inventions and programming tricks are likely to remain your secret. DISADVANTAGES * The compilation itself may be a very time-consuming process - you may not be able to run your code immediately after any amendment; *don\u2019t expect that interpretation will ramp your code to high speed - your code will share the computer\u2019s power with the interpreter, so it can\u2019t be really fast; * You have to have as many compilers as hardware platforms you want your code to be run on. * Both you and the end user have to have the interpreter to run your code. Python is an Interpreted Language you just need the Python Interpreter","title":"Interpretation"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 1.html#python_goals","text":"An Easy and Intuitive yet powerful language. Open Source Understandable code, just like plain English. Suitable for Everyday programming .","title":"Python Goals"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 1.html#python_cphyton_and_cython","text":"Python is maintain for people around the PSF or Python Software Foundation, the president is the creator of the language itself, Guido van Rossum , these Pythons are called Canonical Python was implemented in C, it was, it is, and, probably continue been, implemented in C, this is why it is often refer as CPython","title":"Python, CPhyton and Cython"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 1.html#cython","text":"Cython is other family member of Python, it is a solution for many trade off, such the lack of efficiency, once the code is written in Python, and it is prove that there is not issue, one can translate this code to C. This is what Cython is intended to do so, translate python code to C. The last two \u201cflavors\u201d of python mentioned in the course are Jython and PyPy .","title":"Cython"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 1.html#jython_and_pypy","text":"Jython is similar to Cython, but instead of translate the python script to C, this will translate to java, it is specially use in big systems, where java is the main language, this Flavor, Jython just support Python 2 and there is not implementation for Python3 so far. pypy it is an environment written in RPython and is mostly use for develop python, it doesn\u2019t run the interpreter it translate everything to C.","title":"Jython and pypy"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 2.html","text":"Function \u00b6 A function in the context of Python is separate part of code that is able to do: Cause some effect as an example the function print() allow use to display something in the console Evaluate a value or some values this is closer to the definition of function in mathematics A function in python can do one of the previous mentioned things or it can do both at the same time. A function might be include it in python, the so call Build-in functions, can be an add-on, and might required installation, or can be done by ourselves. For the name of the function, if those are created by use, we need to pay attention to the name, the name must be self-Evident or significant. Anatomy of a function \u00b6 The function will have 3 components: An Effect. A Result. An Argument or Arguments. Python functions can take none, one or more arguments. A standard convention that we need to follow are the pair of parenthesis ad the end of the function\u2019s name. 1 print(\"Hello, World!\") in this example the parameter is a string this is one of the data types present in python, as it is shown in the example the string is delimited by quotes. The Invocation of a python function will be 1 function_name(argument) the work flow will be as follow: Python check if the name of the function is legal or if it exist within its domain. Python check if the function\u2019s requirements for the number and type of argument is correct Python leave the current part of the code momentarily and jump to the function. Python execute the code, causes the desired effect (if any), evaluates the results (if any) and finished the task Python return to the main code and resume the execution. Keyword arguments \u00b6 This are special arguments that can be pass to the function, these arguments can modify the behavior of the function, as and example: 1 2 print ( \"My name is\" , \"Python.\" , end = \" \" ) print ( \"Monty Python.\" ) the result is 1 My name is Python. Monty Python. Which is different for the normal behavior in order to use this arguments we need some rules: the keyword arguments consist of three elements the keyword, in this case end the equal = and the value, in this case \" \" the keyword argument must be add at the end of the last positional argument. In this case the keyword argument modify the behavior of the print() function, by default at the end of this function it is a change of line, but in this case we change it to a space. For the print() function we have other keyword argument, in this case is sep this argument will modify the way python separate each argument pass to the function, example: 1 print ( \"My\" , \"name\" , \"is\" , \"Monty\" , \"Python.\" , sep = \"-\" ) will give as a result: 1 My-name-is-Monty-Python. Literals \u00b6 The closest definition for literals is the word itself, it is a literal value, for example, lets check this two values: 1 123 and 1 c the first one it is 123 123 the second will be more difficult, it can be the speed of light or the hypotenuse in a Pythagorean theorem. In this case what is the literal? 123 123 is the literal and c it is not. Mode about literals \u00b6 Lets see the following example 1 2 print ( \"2\" ) print ( 2 ) if we execute the code we will see the same number, but these two are not the same, the first one is a \u201cString\u201d a series of characters the second is a numerical value, python will take this numerical value and will converted to machine representation ( bits ). Notes about integer notation \u00b6 In case we need to make a integer in a base different from the decimal, let say octal or hexadecimal we can use the notation that python provide. Octal \u00b6 For octal we ca us de prefix 0o , for example 0o123 ( that is 83) we can print it like this: 1 print ( 0 o123 ) Hexadecimal \u00b6 For the hexadecimal we will use 0x example, ox123 ( that is 291), we can print it like this: 1 print ( 0x123 )","title":"Module 2"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 2.html#function","text":"A function in the context of Python is separate part of code that is able to do: Cause some effect as an example the function print() allow use to display something in the console Evaluate a value or some values this is closer to the definition of function in mathematics A function in python can do one of the previous mentioned things or it can do both at the same time. A function might be include it in python, the so call Build-in functions, can be an add-on, and might required installation, or can be done by ourselves. For the name of the function, if those are created by use, we need to pay attention to the name, the name must be self-Evident or significant.","title":"Function"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 2.html#anatomy_of_a_function","text":"The function will have 3 components: An Effect. A Result. An Argument or Arguments. Python functions can take none, one or more arguments. A standard convention that we need to follow are the pair of parenthesis ad the end of the function\u2019s name. 1 print(\"Hello, World!\") in this example the parameter is a string this is one of the data types present in python, as it is shown in the example the string is delimited by quotes. The Invocation of a python function will be 1 function_name(argument) the work flow will be as follow: Python check if the name of the function is legal or if it exist within its domain. Python check if the function\u2019s requirements for the number and type of argument is correct Python leave the current part of the code momentarily and jump to the function. Python execute the code, causes the desired effect (if any), evaluates the results (if any) and finished the task Python return to the main code and resume the execution.","title":"Anatomy of a function"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 2.html#keyword_arguments","text":"This are special arguments that can be pass to the function, these arguments can modify the behavior of the function, as and example: 1 2 print ( \"My name is\" , \"Python.\" , end = \" \" ) print ( \"Monty Python.\" ) the result is 1 My name is Python. Monty Python. Which is different for the normal behavior in order to use this arguments we need some rules: the keyword arguments consist of three elements the keyword, in this case end the equal = and the value, in this case \" \" the keyword argument must be add at the end of the last positional argument. In this case the keyword argument modify the behavior of the print() function, by default at the end of this function it is a change of line, but in this case we change it to a space. For the print() function we have other keyword argument, in this case is sep this argument will modify the way python separate each argument pass to the function, example: 1 print ( \"My\" , \"name\" , \"is\" , \"Monty\" , \"Python.\" , sep = \"-\" ) will give as a result: 1 My-name-is-Monty-Python.","title":"Keyword arguments"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 2.html#literals","text":"The closest definition for literals is the word itself, it is a literal value, for example, lets check this two values: 1 123 and 1 c the first one it is 123 123 the second will be more difficult, it can be the speed of light or the hypotenuse in a Pythagorean theorem. In this case what is the literal? 123 123 is the literal and c it is not.","title":"Literals"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 2.html#mode_about_literals","text":"Lets see the following example 1 2 print ( \"2\" ) print ( 2 ) if we execute the code we will see the same number, but these two are not the same, the first one is a \u201cString\u201d a series of characters the second is a numerical value, python will take this numerical value and will converted to machine representation ( bits ).","title":"Mode about literals"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 2.html#notes_about_integer_notation","text":"In case we need to make a integer in a base different from the decimal, let say octal or hexadecimal we can use the notation that python provide.","title":"Notes about integer notation"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 2.html#octal","text":"For octal we ca us de prefix 0o , for example 0o123 ( that is 83) we can print it like this: 1 print ( 0 o123 )","title":"Octal"},{"location":"Coding/Python/Preparation for PCPP-32-1/Module 2.html#hexadecimal","text":"For the hexadecimal we will use 0x example, ox123 ( that is 291), we can print it like this: 1 print ( 0x123 )","title":"Hexadecimal"},{"location":"Coding/Python/Scripts/JSON Objects in Python.html","text":"JSON Object in python let start for the basic and the well know definition of what is JSON JSON stand for Java Script Object Notation and it is a light weight data format with many similarities to python dictionaries. JSON objects can be parse by many, if not all, modern browsers, which is ideal for transporting data between a client and a server. Defining a JSON \u00b6 First let make and example file, that can be copy and save as a .txt and it will represent our JSON file 1 2 3 4 5 6 7 8 9 { \"album_title\" : \"Yellow Submarine\" , \"release_year\" : 1966 , \"won_grammy\" : false , \"band\" : \"The Beatles\" , \"album_sale\" : null , \"musicians\" : [ \"John Lennon\" , \"Paul McCartney\" , \"George Harrison\" , \"Ringo Starr\" ], \"studio\" : { \"studio_name\" : \"Abbey Road Studios\" , \"location\" : \"London, England\" } } First, what we know, the keys are strings and the values can be strings, numbers (floats or ints), boolean values, lists, null, or another JSON object. second, what are the difference with Python dictionaries; A JSON objects, for example, represent the boolean values as lower case and empty values are null . A python dictionary equivalent is: 1 2 3 4 5 6 7 8 9 { \"album_title\" : \"Yellow Submarine\" , \"release_year\" : 1966 , \"won_grammy\" : False , \"band\" : \"The Beatles\" , \"album_sale\" : None , \"musicians\" : [ \"John Lennon\" , \"Paul McCartney\" , \"George Harrison\" , \"Ringo Starr\" ], \"studio\" : { \"studio_name\" : \"Abbey Road Studios\" , \"location\" : \"London, England\" } } Notice that in python dictionaries boolean values are capitalize and empty value are None I will save this JSON in a .txt and called \u2018album.txt\u2019, later import json library and use dir() to check the available methods: 1 2 import json print ( dir ( json )) load() and dump() \u00b6 We will focus in load() and dump() methods, for that we will open the file, and album.txt that contain a JSON object, we will open this file using open() and we can choose if open in writing mode or reading mode, this case will be reading so the second argument will be 'r' 1 2 3 4 5 import json as j album_json_file = open ( \"album.TXT\" , 'r' ) album = j . load ( album_json_file ) print ( album ) if we use print(type(album)) the result will be a python dictionary type <class 'dict'> <class 'dict'> We can verify this by checking two values, the value of \u2018won_grammy\u2019 that now is False and the value of \u2018album_sale\u2019 that now is None . Not that is parse as a dictionary we can call the different values as a dictionary: 1 print ( \"Album Title: \" , album [ 'album_title' ]) or 1 print ( \"Release Year: \" , album [ 'release_year' ]) JSON in a string form and the method loads() \u00b6 In client server applications it is common for JSON objects to arrive in the form of strings. For example, our JSON object for album information can look something like this: 1 2 3 4 5 6 7 8 album_string = \"\"\"{\"album_title\" : \"Yellow Submarine\", \"release_year\" : 1966, \"won_grammy\" : false, \"band\" : \"The Beatles\", \"album_sale\": null, \"musicians\" : [\"John Lennon\", \"Paul McCartney\", \"George Harrison\", \"Ringo Starr\"], \"studio\" : {\"studio_name\": \"Abbey Road Studios\", \"location\": \"London, England\"} }\"\"\" To load this data using the JSON module, we use the loads() method: 1 2 3 album_s = json . loads ( album_string ) album_string . close () print ( album_s ) Now, suppose we start with dictionary formatted data and wish to send this data to a database in JSON format. We can use the dumps() methods to convert dictionaries to string JSON objects. 1 2 3 4 5 album2 = { 'album_title' : 'Yellow Submarine' , 'release_year' : 1966 , 'won_grammy' : False , 'band' : 'The Beatles' , 'album_sale' : None , 'musicians' : [ 'John Lennon' , 'Paul McCartney' , 'George Harrison' , 'Ringo Starr' ], 'studio' : { 'studio_name' : 'Abbey Road Studios' , 'location' : 'London, England' }} print ( json . dumps ( album2 )) print ( type ( json . dumps ( album2 ))) We see that the \u2018album_sale\u2019 value, \u2018None\u2019, is now \u2018null\u2019 and the \u2018won_grammy\u2019 value, \u2018False\u2019, is now \u2018false\u2019. Finally, we can write this JSON object to a \u2018.txt\u2019 file using the dump method: 1 2 file2 = open ( \"album2.txt\" , \"w\" ) json . dump ( album2 , file2 )","title":"JSON Object in python"},{"location":"Coding/Python/Scripts/JSON Objects in Python.html#defining_a_json","text":"First let make and example file, that can be copy and save as a .txt and it will represent our JSON file 1 2 3 4 5 6 7 8 9 { \"album_title\" : \"Yellow Submarine\" , \"release_year\" : 1966 , \"won_grammy\" : false , \"band\" : \"The Beatles\" , \"album_sale\" : null , \"musicians\" : [ \"John Lennon\" , \"Paul McCartney\" , \"George Harrison\" , \"Ringo Starr\" ], \"studio\" : { \"studio_name\" : \"Abbey Road Studios\" , \"location\" : \"London, England\" } } First, what we know, the keys are strings and the values can be strings, numbers (floats or ints), boolean values, lists, null, or another JSON object. second, what are the difference with Python dictionaries; A JSON objects, for example, represent the boolean values as lower case and empty values are null . A python dictionary equivalent is: 1 2 3 4 5 6 7 8 9 { \"album_title\" : \"Yellow Submarine\" , \"release_year\" : 1966 , \"won_grammy\" : False , \"band\" : \"The Beatles\" , \"album_sale\" : None , \"musicians\" : [ \"John Lennon\" , \"Paul McCartney\" , \"George Harrison\" , \"Ringo Starr\" ], \"studio\" : { \"studio_name\" : \"Abbey Road Studios\" , \"location\" : \"London, England\" } } Notice that in python dictionaries boolean values are capitalize and empty value are None I will save this JSON in a .txt and called \u2018album.txt\u2019, later import json library and use dir() to check the available methods: 1 2 import json print ( dir ( json ))","title":"Defining a JSON"},{"location":"Coding/Python/Scripts/JSON Objects in Python.html#load_and_dump","text":"We will focus in load() and dump() methods, for that we will open the file, and album.txt that contain a JSON object, we will open this file using open() and we can choose if open in writing mode or reading mode, this case will be reading so the second argument will be 'r' 1 2 3 4 5 import json as j album_json_file = open ( \"album.TXT\" , 'r' ) album = j . load ( album_json_file ) print ( album ) if we use print(type(album)) the result will be a python dictionary type <class 'dict'> <class 'dict'> We can verify this by checking two values, the value of \u2018won_grammy\u2019 that now is False and the value of \u2018album_sale\u2019 that now is None . Not that is parse as a dictionary we can call the different values as a dictionary: 1 print ( \"Album Title: \" , album [ 'album_title' ]) or 1 print ( \"Release Year: \" , album [ 'release_year' ])","title":"load() and dump()"},{"location":"Coding/Python/Scripts/JSON Objects in Python.html#json_in_a_string_form_and_the_method_loads","text":"In client server applications it is common for JSON objects to arrive in the form of strings. For example, our JSON object for album information can look something like this: 1 2 3 4 5 6 7 8 album_string = \"\"\"{\"album_title\" : \"Yellow Submarine\", \"release_year\" : 1966, \"won_grammy\" : false, \"band\" : \"The Beatles\", \"album_sale\": null, \"musicians\" : [\"John Lennon\", \"Paul McCartney\", \"George Harrison\", \"Ringo Starr\"], \"studio\" : {\"studio_name\": \"Abbey Road Studios\", \"location\": \"London, England\"} }\"\"\" To load this data using the JSON module, we use the loads() method: 1 2 3 album_s = json . loads ( album_string ) album_string . close () print ( album_s ) Now, suppose we start with dictionary formatted data and wish to send this data to a database in JSON format. We can use the dumps() methods to convert dictionaries to string JSON objects. 1 2 3 4 5 album2 = { 'album_title' : 'Yellow Submarine' , 'release_year' : 1966 , 'won_grammy' : False , 'band' : 'The Beatles' , 'album_sale' : None , 'musicians' : [ 'John Lennon' , 'Paul McCartney' , 'George Harrison' , 'Ringo Starr' ], 'studio' : { 'studio_name' : 'Abbey Road Studios' , 'location' : 'London, England' }} print ( json . dumps ( album2 )) print ( type ( json . dumps ( album2 ))) We see that the \u2018album_sale\u2019 value, \u2018None\u2019, is now \u2018null\u2019 and the \u2018won_grammy\u2019 value, \u2018False\u2019, is now \u2018false\u2019. Finally, we can write this JSON object to a \u2018.txt\u2019 file using the dump method: 1 2 file2 = open ( \"album2.txt\" , \"w\" ) json . dump ( album2 , file2 )","title":"JSON in a string form and the method loads()"},{"location":"Coding/Python/Scripts/Scraping_Usign_Python.html","text":"We are going to use Beautifulsoup and other libraries to scrap Wikipedia: List of Countries: On accessing the first page, we\u2019ll extract the list of countries, their population and percentage of world population. Country: We\u2019ll then access each country\u2019s page, and get information including total area, percentage water, and GDP (nominal). Import Libraries \u00b6 Numpy: To be use with arrays. Pandas: To convert the data in a tabular structure so we can manipulate it. Urllib: To open the url from which we would like to extract the data. BeautifulSoup: This library helps us to get the HTML structure of the page that we want to work with. We can then, use its functions to access specific elements and extract relevant information. 1 2 3 4 5 import numpy as np import pandas as pd from urllib.request import urlopen from bs4 import BeautifulSoup Understanding the data \u00b6 Getting the \u201craw\u201d data \u00b6 First we will need to get the \u201craw\u201d data, the HTML content of the specific URL, for this we will create a function that will return this \u201craw\u201d data. 1 2 3 4 def getHTMLContent ( link ): html = urlopen ( link ) soup = Beautifulsoup ( html , 'html.parser' ) return soup The Function getHTMLContent() will receive a link or URL, later we are going to use urlopen() to open this URL, tis will enable us to apply Beautifulsoup library . There are different parsers, for XML and HTML, but in this case, we will use just html.parser and we return the output of this parser, Beautifulsoup(markup,'html.parser') , so we can extract our data. Finding the data and display it with prettify() \u00b6 we get the content of the URL but the information we need is in a table so we are going to store the information we got in a variable, later we will apply the method find_all() from Beautifulsoup and the tag table so we can get all the tables in this HTML, later we are going to print it in a readable way using the method prettify() 1 2 3 4 5 content = getHTMLContent ( URL ) #print(content) tables = content . find_all ( 'table' ) for table in tables : print ( table . prettify ()) The code will print all the table in this HTML, therefore we will need to check which is the table that we need, but first let see the code so far: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np import pandas as pd from urllib.request import urlopen from bs4 import BeautifulSoup URL = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population' #get the HTML def getHTMLContent ( link ): html = urlopen ( link ) soup = BeautifulSoup ( html , 'html.parser' ) return soup content = getHTMLContent ( URL ) #print(content) tables = content . find_all ( 'table' ) for table in tables : print ( table . prettify ()) Now we need one table in specific so we will need to find this table with the class wikitable sortable , We will use the method find() , since this method allow us no just find a tag but a tag with a specific tag, once we have the table we will get all the rows 1 2 table = content . find ( 'table' , { 'class' : 'wikitable sortable' }) rows = table . find_all ( 'tr' ) now we have all the rows, we need to iterate over them to find the cell that contain the link to the country page> we know that HTML table are detonated with <tr></tr> an each row is either be heading <th></th> or data <td></td> and we know that the country page is in the second column so cells[1] and we use find() to find the a elements, and we extract the link or href with get() : 1 2 3 4 5 for row in rows : cells = row . find_all ( 'td' ) if len ( cells ) > 1 : country_link = cells [ 1 ] . find ( 'a' ) print ( country_link . get ( 'href' )) so the code will be 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import numpy as np import pandas as pd from urllib.request import urlopen from bs4 import BeautifulSoup URL = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population' #get the HTML def getHTMLContent ( link ): html = urlopen ( link ) soup = BeautifulSoup ( html , 'html.parser' ) return soup content = getHTMLContent ( URL ) #print(content) tables = content . find_all ( 'table' ) #for table in tables: #print(table.prettify()) table = content . find ( 'table' , { 'class' : 'wikitable sortable' }) rows = table . find_all ( 'tr' ) for row in rows : cells = row . find_all ( 'td' ) if len ( cells ) > 1 : country_link = cells [ 1 ] . find ( 'a' ) print ( country_link . get ( 'href' )) and the result We can see that the information or links that we got back don\u2019t include the first part of the URL so we will need to prefix \u201chttps://en.wikipedia.org\u201d. Later we will create a variable called root_URL that will contain \u201chttps://en.wikipedia.org\u201d Data for Each country \u00b6 Now, we Will use the list of link to go to each country page and locate the card to the right of the screen where there is the remaining information. inspecting the country page we found that the class for card mentioned above is infobox geography vcard Here is when we run into some issues, we are looking for the following fields: Area > Total area Water (%) GDP (nominal) > Total Per capita but their order vary in each country page, so we will need to make some adjustment. Modifying current code \u00b6 We will start making a small modification in the code, we will create a new function that will return a list with the link of all the counties 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import numpy as np import pandas as pd from urllib.request import urlopen from bs4 import BeautifulSoup root_URL = 'https://en.wikipedia.org' URL = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population' country_links = [] #get the HTML def getHTMLContent ( link ): html = urlopen ( link ) soup = BeautifulSoup ( html , 'html.parser' ) return soup # get the link of all countries def getCountryLinks ( URL ): content = getHTMLContent ( URL ) #tables = content.find_all('table') table = content . find ( 'table' , { 'class' : 'wikitable sortable' }) rows = table . find_all ( 'tr' ) for row in rows : cells = row . find_all ( 'td' ) if len ( cells ) > 1 : country_link = cells [ 1 ] . find ( 'a' ) . get ( 'href' ) #print(country_link.get('href')) country_links . append ( country_link ) getCountryLinks ( URL ) print ( country_links ) Values in different positions - Something to thing about it \u00b6 Now, with few pages inspected we can see where will be a problem, the location of the different values we want are in different position depending of the country. We now the information is in tags with classes mergedrow or mergedrowbottom but the position of the row that contain the information it is not constant, in the first example, we see that we don\u2019t have water(%) but in the second page we do have this value, making the position of population one position down in comparison with the first example To solve this we will add some extra code. We will start by adding a new variable called additional_details , later use it to append the information we collect form each country page to the table with the list of countries. Next, we will create a function getadditionalDetails() and a variable flag read_content , we will explain the usage later. we are going to use three type of function here: get() With this function we will find and get the reference of a particular element. get_text() the function will return the text that is within the opening and close tags of an element. strip() This will remove the white spaces at both sides of the string or text. getadditionalDetails() function: \u00b6 First, we will create a new variable 1 2 # to be use in getAdditionalDetails additional_details = [] next, the function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def getAdditionalDetails ( country_link ): try : country_page = getHTMLContent ( 'https://en.wikipedia.org' + country_link ) table = country_page . find ( 'table' , { 'class' : 'infobox geography vcard' }) read_content = False for tr in table . find_all ( 'tr' ): if ( tr . get ( 'class' ) == [ 'mergedtoprow' ] and not read_content ): link = tr . find ( 'a' ) if ( link and ( link . get_text () . strip () == 'Area' or ( link . get_text () . strip () == 'GDP' and tr . find ( 'span' ) . get_text () . strip () == '(nominal)' ))): read_content = True if ( link and ( link . get_text () . strip () == 'Population' )): read_content = False elif (( tr . get ( 'class' ) == [ 'mergedrow' ] or tr . get ( 'class' ) == [ 'mergedbottomrow' ]) and read_content ): additional_details . append ( tr . find ( 'td' ) . get_text () . strip ( ' \\n ' )) if ( tr . find ( 'div' ) . get_text () . strip () != '\u2022 \\xa0 Total area' and tr . find ( 'div' ) . get_text () . strip () != '\u2022 \\xa0 Total' ): read_content = False return additional_details except Exception as error : print ( 'Error occurred: {}' . format ( error )) return [] We have a try/except block that will wrap the logic and it will prevent the script to crash if one of the links doesn\u2019t provide the information in the same format or if the page is empty We get the links for the countries, remember those links don\u2019t have the root url so here we concatenate them with the root URL. We find the table with the classes infobox geography vcard , and set the flag read_content . We use a for loop to iterate over all the <tr> tags on the table. We use a decision block to check if the class in the <tr> is mergedtoprow and not read_content . if the condition are match we save the content of <a> on the variable link . the next two condition blocks are checking if the information on the page is following the order that we want, in which case we set the read_content to True , otherwise read_content will be set as False. the elif part of the initial conditional will be the one that will populate the variable additional_details Finally the except block, this will be used to report back if there is any issue. Create the dataset \u00b6","title":"Scraping Usign Python"},{"location":"Coding/Python/Scripts/Scraping_Usign_Python.html#import_libraries","text":"Numpy: To be use with arrays. Pandas: To convert the data in a tabular structure so we can manipulate it. Urllib: To open the url from which we would like to extract the data. BeautifulSoup: This library helps us to get the HTML structure of the page that we want to work with. We can then, use its functions to access specific elements and extract relevant information. 1 2 3 4 5 import numpy as np import pandas as pd from urllib.request import urlopen from bs4 import BeautifulSoup","title":"Import Libraries"},{"location":"Coding/Python/Scripts/Scraping_Usign_Python.html#understanding_the_data","text":"","title":"Understanding the data"},{"location":"Coding/Python/Scripts/Scraping_Usign_Python.html#getting_the_raw_data","text":"First we will need to get the \u201craw\u201d data, the HTML content of the specific URL, for this we will create a function that will return this \u201craw\u201d data. 1 2 3 4 def getHTMLContent ( link ): html = urlopen ( link ) soup = Beautifulsoup ( html , 'html.parser' ) return soup The Function getHTMLContent() will receive a link or URL, later we are going to use urlopen() to open this URL, tis will enable us to apply Beautifulsoup library . There are different parsers, for XML and HTML, but in this case, we will use just html.parser and we return the output of this parser, Beautifulsoup(markup,'html.parser') , so we can extract our data.","title":"Getting the \"raw\" data"},{"location":"Coding/Python/Scripts/Scraping_Usign_Python.html#finding_the_data_and_display_it_with_prettify","text":"we get the content of the URL but the information we need is in a table so we are going to store the information we got in a variable, later we will apply the method find_all() from Beautifulsoup and the tag table so we can get all the tables in this HTML, later we are going to print it in a readable way using the method prettify() 1 2 3 4 5 content = getHTMLContent ( URL ) #print(content) tables = content . find_all ( 'table' ) for table in tables : print ( table . prettify ()) The code will print all the table in this HTML, therefore we will need to check which is the table that we need, but first let see the code so far: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np import pandas as pd from urllib.request import urlopen from bs4 import BeautifulSoup URL = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population' #get the HTML def getHTMLContent ( link ): html = urlopen ( link ) soup = BeautifulSoup ( html , 'html.parser' ) return soup content = getHTMLContent ( URL ) #print(content) tables = content . find_all ( 'table' ) for table in tables : print ( table . prettify ()) Now we need one table in specific so we will need to find this table with the class wikitable sortable , We will use the method find() , since this method allow us no just find a tag but a tag with a specific tag, once we have the table we will get all the rows 1 2 table = content . find ( 'table' , { 'class' : 'wikitable sortable' }) rows = table . find_all ( 'tr' ) now we have all the rows, we need to iterate over them to find the cell that contain the link to the country page> we know that HTML table are detonated with <tr></tr> an each row is either be heading <th></th> or data <td></td> and we know that the country page is in the second column so cells[1] and we use find() to find the a elements, and we extract the link or href with get() : 1 2 3 4 5 for row in rows : cells = row . find_all ( 'td' ) if len ( cells ) > 1 : country_link = cells [ 1 ] . find ( 'a' ) print ( country_link . get ( 'href' )) so the code will be 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import numpy as np import pandas as pd from urllib.request import urlopen from bs4 import BeautifulSoup URL = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population' #get the HTML def getHTMLContent ( link ): html = urlopen ( link ) soup = BeautifulSoup ( html , 'html.parser' ) return soup content = getHTMLContent ( URL ) #print(content) tables = content . find_all ( 'table' ) #for table in tables: #print(table.prettify()) table = content . find ( 'table' , { 'class' : 'wikitable sortable' }) rows = table . find_all ( 'tr' ) for row in rows : cells = row . find_all ( 'td' ) if len ( cells ) > 1 : country_link = cells [ 1 ] . find ( 'a' ) print ( country_link . get ( 'href' )) and the result We can see that the information or links that we got back don\u2019t include the first part of the URL so we will need to prefix \u201chttps://en.wikipedia.org\u201d. Later we will create a variable called root_URL that will contain \u201chttps://en.wikipedia.org\u201d","title":"Finding the data and display it with prettify()"},{"location":"Coding/Python/Scripts/Scraping_Usign_Python.html#data_for_each_country","text":"Now, we Will use the list of link to go to each country page and locate the card to the right of the screen where there is the remaining information. inspecting the country page we found that the class for card mentioned above is infobox geography vcard Here is when we run into some issues, we are looking for the following fields: Area > Total area Water (%) GDP (nominal) > Total Per capita but their order vary in each country page, so we will need to make some adjustment.","title":"Data for Each country"},{"location":"Coding/Python/Scripts/Scraping_Usign_Python.html#modifying_current_code","text":"We will start making a small modification in the code, we will create a new function that will return a list with the link of all the counties 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import numpy as np import pandas as pd from urllib.request import urlopen from bs4 import BeautifulSoup root_URL = 'https://en.wikipedia.org' URL = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population' country_links = [] #get the HTML def getHTMLContent ( link ): html = urlopen ( link ) soup = BeautifulSoup ( html , 'html.parser' ) return soup # get the link of all countries def getCountryLinks ( URL ): content = getHTMLContent ( URL ) #tables = content.find_all('table') table = content . find ( 'table' , { 'class' : 'wikitable sortable' }) rows = table . find_all ( 'tr' ) for row in rows : cells = row . find_all ( 'td' ) if len ( cells ) > 1 : country_link = cells [ 1 ] . find ( 'a' ) . get ( 'href' ) #print(country_link.get('href')) country_links . append ( country_link ) getCountryLinks ( URL ) print ( country_links )","title":"Modifying current code"},{"location":"Coding/Python/Scripts/Scraping_Usign_Python.html#values_in_different_positions_-_something_to_thing_about_it","text":"Now, with few pages inspected we can see where will be a problem, the location of the different values we want are in different position depending of the country. We now the information is in tags with classes mergedrow or mergedrowbottom but the position of the row that contain the information it is not constant, in the first example, we see that we don\u2019t have water(%) but in the second page we do have this value, making the position of population one position down in comparison with the first example To solve this we will add some extra code. We will start by adding a new variable called additional_details , later use it to append the information we collect form each country page to the table with the list of countries. Next, we will create a function getadditionalDetails() and a variable flag read_content , we will explain the usage later. we are going to use three type of function here: get() With this function we will find and get the reference of a particular element. get_text() the function will return the text that is within the opening and close tags of an element. strip() This will remove the white spaces at both sides of the string or text.","title":"Values in different positions - Something to thing about it"},{"location":"Coding/Python/Scripts/Scraping_Usign_Python.html#getadditionaldetails_function","text":"First, we will create a new variable 1 2 # to be use in getAdditionalDetails additional_details = [] next, the function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def getAdditionalDetails ( country_link ): try : country_page = getHTMLContent ( 'https://en.wikipedia.org' + country_link ) table = country_page . find ( 'table' , { 'class' : 'infobox geography vcard' }) read_content = False for tr in table . find_all ( 'tr' ): if ( tr . get ( 'class' ) == [ 'mergedtoprow' ] and not read_content ): link = tr . find ( 'a' ) if ( link and ( link . get_text () . strip () == 'Area' or ( link . get_text () . strip () == 'GDP' and tr . find ( 'span' ) . get_text () . strip () == '(nominal)' ))): read_content = True if ( link and ( link . get_text () . strip () == 'Population' )): read_content = False elif (( tr . get ( 'class' ) == [ 'mergedrow' ] or tr . get ( 'class' ) == [ 'mergedbottomrow' ]) and read_content ): additional_details . append ( tr . find ( 'td' ) . get_text () . strip ( ' \\n ' )) if ( tr . find ( 'div' ) . get_text () . strip () != '\u2022 \\xa0 Total area' and tr . find ( 'div' ) . get_text () . strip () != '\u2022 \\xa0 Total' ): read_content = False return additional_details except Exception as error : print ( 'Error occurred: {}' . format ( error )) return [] We have a try/except block that will wrap the logic and it will prevent the script to crash if one of the links doesn\u2019t provide the information in the same format or if the page is empty We get the links for the countries, remember those links don\u2019t have the root url so here we concatenate them with the root URL. We find the table with the classes infobox geography vcard , and set the flag read_content . We use a for loop to iterate over all the <tr> tags on the table. We use a decision block to check if the class in the <tr> is mergedtoprow and not read_content . if the condition are match we save the content of <a> on the variable link . the next two condition blocks are checking if the information on the page is following the order that we want, in which case we set the read_content to True , otherwise read_content will be set as False. the elif part of the initial conditional will be the one that will populate the variable additional_details Finally the except block, this will be used to report back if there is any issue.","title":"getadditionalDetails() function:"},{"location":"Coding/Python/Scripts/Scraping_Usign_Python.html#create_the_dataset","text":"","title":"Create the dataset"},{"location":"Coding/Python/Scripts/Scrapy_and_Scrapyrt.html","text":"Scrapy is a free and open-source web crawling framework written in Python. With Scrapy we can send requests to websites and parse the HTML code received as response. Scrapyrt (Scrapy realtime), we can create an HTTP server that can control Scrapy through HTTP requests. The response send by the server will be data formatted as JSON and it will contain the data scraped by Scrapy. With the combination of these two tools, We can create an entire API without need a database. Set up Scrapy and create your spider \u00b6 To install Scrapy we can use pip 1 pip install scrapy Once the installation completed, you can start a Scrapy project by running: 1 scrapy startproject <project_name> We will use an article from Medium.com as guide, for that we will use coinmarketcap.com 1 scrapy startproject coinmarketcap We will scrape the URL: https://coinmarketcap.com/all/views/all/ . It contains information about cryptocurrencies such as their current prices, their price variations, etc. The goal is to collect those data with Scrapy and then to return them as JSON value with Scrapyrt. the project folder structure should currently look like this: The First Spider \u00b6 To create the new spider we need to create a new file in the spiders folder. The file\u2019s name doesn\u2019t really matter, it should just represent what your spider is scraping. in this example we simply call it coinSpider.py . First let\u2019s create a class that inherits from scrapy.Spider. 1 2 3 import scrapy class CoinSpider ( scrapy . Spider ): A Spider class must have a name attribute. This element will help you to inform Scrapy which crawler you want to start. 1 2 3 4 import scrapy class CoinSpider ( scrapy . Spider ): name = \"coin\" Now we need to tell Scrapy what is the URL we want to send the request to. We\u2019ll use start_requests method. This method will return the Scrapy request to the URL we want to crawl. 1 2 3 4 5 6 7 8 import scrapy class CoinSpider ( scrapy . Spider ): name = \"coin\" def start_requests ( self ): url = \"https://coinmarketcap.com/all/views/all/\" yield scrapy . Request ( url = url , callback = self . parse ) The scrapy.Request function takes the URL we want to crawl as the first parameter and a callback function that will parse the response we\u2019ll receive from the request. Now we need to create that callback function 1 2 3 4 5 6 7 8 9 10 def parse ( self , response ): for row in response . css ( \"tbody tr\" ): yield { \"name\" : row . css ( \"a.currency-name-container::text\" ) . extract_first (), \"symbol\" : row . css ( \"td.col-symbol::text\" ) . extract_first (), \"market_cap\" : row . css ( \"td.market-cap::text\" ) . extract_first (), \"price\" : row . css ( \"a.price::attr(data-usd)\" ) . extract_first (), \"circulating_supply\" : row . css ( \"td.circulating-supply span::attr(data-supply)\" ) . extract_first (), \"volume\" : row . css ( \"a.volume::attr(data-usd)\" ) . extract_first () } Our parse method will go through each row of the table containing the cryptocurrency data that we want for our API. It then selects the wanted information using CSS selector. The line for row in response.css(\u201ctbody tr\u201d) basically says \u201ctake the content of the response, select all the <tr> in the <tbody> , assign individually the content of each of them in the row variable\u201d. The value of this variable would look like something like this for the first line of the table: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 < tr id = \"id-bitcoin\" class = \"odd\" role = \"row\" > < td class = \"text-center\" > 1 </ td > < td class = \"no-wrap currency-name\" data-sort = \"Bitcoin\" > < div class = \"s-s-1 logo-sprite\" ></ div > < span class = \"currency-symbol visible-xs\" >< a class = \"link-secondary\" href = \"/currencies/bitcoin/\" > BTC </ a ></ span > < br class = \"visible-xs\" > < a class = \"currency-name-container link-secondary\" href = \"/currencies/bitcoin/\" > Bitcoin </ a > </ td > < td class = \"text-left col-symbol\" > BTC </ td > < td class = \"no-wrap market-cap text-right\" data-usd = \"1.10882148223e+11\" data-btc = \"17226975.0\" data-sort = \"1.10882148223e+11\" > $110 &nbsp; 882 &nbsp; 148 &nbsp; 223 </ td > < td class = \"no-wrap text-right\" data-sort = \"6436.54200598\" > < a href = \"/currencies/bitcoin/#markets\" class = \"price\" data-usd = \"6436.54200598\" data-btc = \"1.0\" > $6 &nbsp; 436,54 </ a > </ td > < td class = \"no-wrap text-right circulating-supply\" data-sort = \"17226975.0\" > < span data-supply = \"17226975.0\" data-supply-container = \"\" > 17 &nbsp; 226 &nbsp; 975 </ span > </ td > < td class = \"no-wrap text-right \" data-sort = \"3666361510.97\" > < a href = \"/currencies/bitcoin/#markets\" class = \"volume\" data-usd = \"3666361510.97\" data-btc = \"571144.12818\" > $3 &nbsp; 666 &nbsp; 361 &nbsp; 511 </ a > </ td > < td class = \"no-wrap percent-change text-right positive_change\" data-timespan = \"1h\" data-percentusd = \"0.01\" data-symbol = \"BTC\" data-sort = \"0.0133683\" > 0,01% </ td > < td class = \"no-wrap percent-change text-right positive_change\" data-timespan = \"24h\" data-percentusd = \"0.65\" data-symbol = \"BTC\" data-sort = \"0.648688\" > 0,65% </ td > < td class = \"no-wrap percent-change text-right positive_change\" data-timespan = \"7d\" data-percentusd = \"0.97\" data-symbol = \"BTC\" data-sort = \"0.974922\" > 0,97% </ td > < td class = \"more-options-cell dropdown\" data-more-options = \"\" data-cc-id = \"1\" data-cc-slug = \"bitcoin\" > < button class = \"btn btn-transparent dropdown-toggle\" type = \"button\" id = \"dropdown-menu-1\" data-toggle = \"dropdown\" > < span class = \"glyphicons glyphicons-more text-gray\" ></ span > </ button > < ul class = \"dropdown-menu dropdown-menu-right\" role = \"menu\" aria-labelledby = \"dropdown-menu-1\" > < li role = \"presentation\" >< a role = \"menuitem\" tabindex = \"-1\" href = \"#\" data-watchlist-add = \"\" > Add to Watchlist </ a ></ li > < li role = \"presentation\" >< a role = \"menuitem\" tabindex = \"-1\" href = \"#\" data-watchlist-remove = \"\" style = \"display: none;\" > Remove from Watchlist </ a ></ li > < li class = \"disabled\" role = \"presentation\" >< a role = \"menuitem\" tabindex = \"-1\" href = \"#\" data-watchlist-full = \"\" style = \"display: none;\" > Watchlist full! </ a ></ li > < li role = \"presentation\" >< a role = \"menuitem\" tabindex = \"-1\" href = \"/currencies/bitcoin/#charts\" > View Chart </ a ></ li > < li role = \"presentation\" >< a role = \"menuitem\" tabindex = \"-1\" href = \"/currencies/bitcoin/#markets\" > View Markets </ a ></ li > < li role = \"presentation\" >< a role = \"menuitem\" tabindex = \"-1\" href = \"/currencies/bitcoin/historical-data/\" > View Historical Data </ a ></ li > < li role = \"presentation\" >< a role = \"menuitem\" tabindex = \"-1\" href = \"https://payments.changelly.com/?crypto=BTC&amp;fiat=USD&amp;ref_id=coinmarketcap\" target = \"_blank\" rel = \"nofollow noopener\" > Buy / Sell Instantly </ a ></ li > </ ul > </ td > </ tr > We then loop through each row and apply one more CSS selector to extract the exact value that we want. For example; The name of the currency is contained in a link <a> which has the class currency-name-container assigned to it. By adding ::text to the selector, we specify that we want the text between <a> and </a> . The method .extract_first() is added after the selector to indicate that we want the first value found by the parser. We repeat the process with all the data we want to extract, and we then return them in a dictionary. Quick note: if the data that you want to extract is not between two HTML tags but in an attribute, you can use ::attr(<name_of_the_attribute>) in the CSS selector. In our case we have ::attr(data-usd) as an example. So putting everything together the spider will look like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import scrapy class CoinSpider ( scrapy . Spider ): name = \"coin\" def start_requests ( self ): url = \"https://coinmarketcap.com/all/views/all/\" yield scrapy . Request ( url = url , callback = self . parse ) def parse ( self , response ): for row in response . css ( \"tbody tr\" ): yield { \"name\" : row . css ( \"a.currency-name-container::text\" ) . extract_first (), \"symbol\" : row . css ( \"td.col-symbol::text\" ) . extract_first (), \"market_cap\" : row . css ( \"td.market-cap::text\" ) . extract_first (), \"price\" : row . css ( \"a.price::attr(data-usd)\" ) . extract_first (), \"circulating_supply\" : row . css ( \"td.circulating-supply span::attr(data-supply)\" ) . extract_first (), \"volume\" : row . css ( \"a.volume::attr(data-usd)\" ) . extract_first () } Run the spider \u00b6 Using the terminal and set our working directory in your Scrapy project folder. example: C:\\Users\\BlackDesktop\\Documents\\Hero of Alexandria\\006. [Scrapy] Create an API with Scrapy and Scrapyrt\\coinmarketcap To start the crawler and save the scraped data in a JSON file, run the following command: scrapy crawl <name_of_the_spider> -o <output_file_name>.json In our case: scrapy crawl coin -o coin.json The file coin.json should be created at the root of your coinmarketcap folder It should contain the result scraped by the spider similar to the following format: 1 2 3 4 5 6 [ { \"name\" : \"Bitcoin\" , \"symbol\" : \"BTC\" , \"market_cap\" : \"\\n$111,793,976,147\\n\" , \"price\" : \"6489.45341094\" , \"circulating_supply\" : \"17227025.0\" , \"volume\" : \"3643933075.18\" }, { \"name\" : \"Ethereum\" , \"symbol\" : \"ETH\" , \"market_cap\" : \"\\n$28,021,091,521\\n\" , \"price\" : \"276.039836485\" , \"circulating_supply\" : \"101511042.311\" , \"volume\" : \"1356884351.47\" }, { \"name\" : \"XRP\" , \"symbol\" : \"XRP\" , \"market_cap\" : \"\\n$12,774,073,210\\n\" , \"price\" : \"0.323193723266\" , \"circulating_supply\" : \"39524508956.0\" , \"volume\" : \"221046052.002\" }, { \"name\" : \"Bitcoin Cash\" , \"symbol\" : \"BCH\" , \"market_cap\" : \"\\n$9,107,466,682\\n\" , \"price\" : \"526.167151135\" , \"circulating_supply\" : \"17309075.0\" , \"volume\" : \"291574904.596\" }, ... Install Scrapyrt and combine it with our project \u00b6 Let\u2019s now use Scrapyrt to serve those data through an HTTP request instead of having them saved in a JSON file. To install scrapyrt we run pip install scrapyrt To use it, open your terminal again and set your working directory to the Scrapy project folder. Then run the following command: scrapyrt -p <PORT> <PORT> can be replaced with a port number. For example scrapyrt -p 3000 With this command Scrapyrt will setup locally a simple HTTP server that will allow you to control your crawler. We can access it with a GET request through the endpoint http://localhost:<PORT>/crawl.json. To work properly it also needs at least these two arguments: start_requests (Boolean) and spider_name (string) . to see the results we can open the browser on: http://localhost:3000/crawl.json?start_requests=true&spider_name=coin The result should look like this:","title":"Scrapy and Scrapyrt"},{"location":"Coding/Python/Scripts/Scrapy_and_Scrapyrt.html#set_up_scrapy_and_create_your_spider","text":"To install Scrapy we can use pip 1 pip install scrapy Once the installation completed, you can start a Scrapy project by running: 1 scrapy startproject <project_name> We will use an article from Medium.com as guide, for that we will use coinmarketcap.com 1 scrapy startproject coinmarketcap We will scrape the URL: https://coinmarketcap.com/all/views/all/ . It contains information about cryptocurrencies such as their current prices, their price variations, etc. The goal is to collect those data with Scrapy and then to return them as JSON value with Scrapyrt. the project folder structure should currently look like this:","title":"Set up Scrapy and create your spider"},{"location":"Coding/Python/Scripts/Scrapy_and_Scrapyrt.html#the_first_spider","text":"To create the new spider we need to create a new file in the spiders folder. The file\u2019s name doesn\u2019t really matter, it should just represent what your spider is scraping. in this example we simply call it coinSpider.py . First let\u2019s create a class that inherits from scrapy.Spider. 1 2 3 import scrapy class CoinSpider ( scrapy . Spider ): A Spider class must have a name attribute. This element will help you to inform Scrapy which crawler you want to start. 1 2 3 4 import scrapy class CoinSpider ( scrapy . Spider ): name = \"coin\" Now we need to tell Scrapy what is the URL we want to send the request to. We\u2019ll use start_requests method. This method will return the Scrapy request to the URL we want to crawl. 1 2 3 4 5 6 7 8 import scrapy class CoinSpider ( scrapy . Spider ): name = \"coin\" def start_requests ( self ): url = \"https://coinmarketcap.com/all/views/all/\" yield scrapy . Request ( url = url , callback = self . parse ) The scrapy.Request function takes the URL we want to crawl as the first parameter and a callback function that will parse the response we\u2019ll receive from the request. Now we need to create that callback function 1 2 3 4 5 6 7 8 9 10 def parse ( self , response ): for row in response . css ( \"tbody tr\" ): yield { \"name\" : row . css ( \"a.currency-name-container::text\" ) . extract_first (), \"symbol\" : row . css ( \"td.col-symbol::text\" ) . extract_first (), \"market_cap\" : row . css ( \"td.market-cap::text\" ) . extract_first (), \"price\" : row . css ( \"a.price::attr(data-usd)\" ) . extract_first (), \"circulating_supply\" : row . css ( \"td.circulating-supply span::attr(data-supply)\" ) . extract_first (), \"volume\" : row . css ( \"a.volume::attr(data-usd)\" ) . extract_first () } Our parse method will go through each row of the table containing the cryptocurrency data that we want for our API. It then selects the wanted information using CSS selector. The line for row in response.css(\u201ctbody tr\u201d) basically says \u201ctake the content of the response, select all the <tr> in the <tbody> , assign individually the content of each of them in the row variable\u201d. The value of this variable would look like something like this for the first line of the table: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 < tr id = \"id-bitcoin\" class = \"odd\" role = \"row\" > < td class = \"text-center\" > 1 </ td > < td class = \"no-wrap currency-name\" data-sort = \"Bitcoin\" > < div class = \"s-s-1 logo-sprite\" ></ div > < span class = \"currency-symbol visible-xs\" >< a class = \"link-secondary\" href = \"/currencies/bitcoin/\" > BTC </ a ></ span > < br class = \"visible-xs\" > < a class = \"currency-name-container link-secondary\" href = \"/currencies/bitcoin/\" > Bitcoin </ a > </ td > < td class = \"text-left col-symbol\" > BTC </ td > < td class = \"no-wrap market-cap text-right\" data-usd = \"1.10882148223e+11\" data-btc = \"17226975.0\" data-sort = \"1.10882148223e+11\" > $110 &nbsp; 882 &nbsp; 148 &nbsp; 223 </ td > < td class = \"no-wrap text-right\" data-sort = \"6436.54200598\" > < a href = \"/currencies/bitcoin/#markets\" class = \"price\" data-usd = \"6436.54200598\" data-btc = \"1.0\" > $6 &nbsp; 436,54 </ a > </ td > < td class = \"no-wrap text-right circulating-supply\" data-sort = \"17226975.0\" > < span data-supply = \"17226975.0\" data-supply-container = \"\" > 17 &nbsp; 226 &nbsp; 975 </ span > </ td > < td class = \"no-wrap text-right \" data-sort = \"3666361510.97\" > < a href = \"/currencies/bitcoin/#markets\" class = \"volume\" data-usd = \"3666361510.97\" data-btc = \"571144.12818\" > $3 &nbsp; 666 &nbsp; 361 &nbsp; 511 </ a > </ td > < td class = \"no-wrap percent-change text-right positive_change\" data-timespan = \"1h\" data-percentusd = \"0.01\" data-symbol = \"BTC\" data-sort = \"0.0133683\" > 0,01% </ td > < td class = \"no-wrap percent-change text-right positive_change\" data-timespan = \"24h\" data-percentusd = \"0.65\" data-symbol = \"BTC\" data-sort = \"0.648688\" > 0,65% </ td > < td class = \"no-wrap percent-change text-right positive_change\" data-timespan = \"7d\" data-percentusd = \"0.97\" data-symbol = \"BTC\" data-sort = \"0.974922\" > 0,97% </ td > < td class = \"more-options-cell dropdown\" data-more-options = \"\" data-cc-id = \"1\" data-cc-slug = \"bitcoin\" > < button class = \"btn btn-transparent dropdown-toggle\" type = \"button\" id = \"dropdown-menu-1\" data-toggle = \"dropdown\" > < span class = \"glyphicons glyphicons-more text-gray\" ></ span > </ button > < ul class = \"dropdown-menu dropdown-menu-right\" role = \"menu\" aria-labelledby = \"dropdown-menu-1\" > < li role = \"presentation\" >< a role = \"menuitem\" tabindex = \"-1\" href = \"#\" data-watchlist-add = \"\" > Add to Watchlist </ a ></ li > < li role = \"presentation\" >< a role = \"menuitem\" tabindex = \"-1\" href = \"#\" data-watchlist-remove = \"\" style = \"display: none;\" > Remove from Watchlist </ a ></ li > < li class = \"disabled\" role = \"presentation\" >< a role = \"menuitem\" tabindex = \"-1\" href = \"#\" data-watchlist-full = \"\" style = \"display: none;\" > Watchlist full! </ a ></ li > < li role = \"presentation\" >< a role = \"menuitem\" tabindex = \"-1\" href = \"/currencies/bitcoin/#charts\" > View Chart </ a ></ li > < li role = \"presentation\" >< a role = \"menuitem\" tabindex = \"-1\" href = \"/currencies/bitcoin/#markets\" > View Markets </ a ></ li > < li role = \"presentation\" >< a role = \"menuitem\" tabindex = \"-1\" href = \"/currencies/bitcoin/historical-data/\" > View Historical Data </ a ></ li > < li role = \"presentation\" >< a role = \"menuitem\" tabindex = \"-1\" href = \"https://payments.changelly.com/?crypto=BTC&amp;fiat=USD&amp;ref_id=coinmarketcap\" target = \"_blank\" rel = \"nofollow noopener\" > Buy / Sell Instantly </ a ></ li > </ ul > </ td > </ tr > We then loop through each row and apply one more CSS selector to extract the exact value that we want. For example; The name of the currency is contained in a link <a> which has the class currency-name-container assigned to it. By adding ::text to the selector, we specify that we want the text between <a> and </a> . The method .extract_first() is added after the selector to indicate that we want the first value found by the parser. We repeat the process with all the data we want to extract, and we then return them in a dictionary. Quick note: if the data that you want to extract is not between two HTML tags but in an attribute, you can use ::attr(<name_of_the_attribute>) in the CSS selector. In our case we have ::attr(data-usd) as an example. So putting everything together the spider will look like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import scrapy class CoinSpider ( scrapy . Spider ): name = \"coin\" def start_requests ( self ): url = \"https://coinmarketcap.com/all/views/all/\" yield scrapy . Request ( url = url , callback = self . parse ) def parse ( self , response ): for row in response . css ( \"tbody tr\" ): yield { \"name\" : row . css ( \"a.currency-name-container::text\" ) . extract_first (), \"symbol\" : row . css ( \"td.col-symbol::text\" ) . extract_first (), \"market_cap\" : row . css ( \"td.market-cap::text\" ) . extract_first (), \"price\" : row . css ( \"a.price::attr(data-usd)\" ) . extract_first (), \"circulating_supply\" : row . css ( \"td.circulating-supply span::attr(data-supply)\" ) . extract_first (), \"volume\" : row . css ( \"a.volume::attr(data-usd)\" ) . extract_first () }","title":"The First Spider"},{"location":"Coding/Python/Scripts/Scrapy_and_Scrapyrt.html#run_the_spider","text":"Using the terminal and set our working directory in your Scrapy project folder. example: C:\\Users\\BlackDesktop\\Documents\\Hero of Alexandria\\006. [Scrapy] Create an API with Scrapy and Scrapyrt\\coinmarketcap To start the crawler and save the scraped data in a JSON file, run the following command: scrapy crawl <name_of_the_spider> -o <output_file_name>.json In our case: scrapy crawl coin -o coin.json The file coin.json should be created at the root of your coinmarketcap folder It should contain the result scraped by the spider similar to the following format: 1 2 3 4 5 6 [ { \"name\" : \"Bitcoin\" , \"symbol\" : \"BTC\" , \"market_cap\" : \"\\n$111,793,976,147\\n\" , \"price\" : \"6489.45341094\" , \"circulating_supply\" : \"17227025.0\" , \"volume\" : \"3643933075.18\" }, { \"name\" : \"Ethereum\" , \"symbol\" : \"ETH\" , \"market_cap\" : \"\\n$28,021,091,521\\n\" , \"price\" : \"276.039836485\" , \"circulating_supply\" : \"101511042.311\" , \"volume\" : \"1356884351.47\" }, { \"name\" : \"XRP\" , \"symbol\" : \"XRP\" , \"market_cap\" : \"\\n$12,774,073,210\\n\" , \"price\" : \"0.323193723266\" , \"circulating_supply\" : \"39524508956.0\" , \"volume\" : \"221046052.002\" }, { \"name\" : \"Bitcoin Cash\" , \"symbol\" : \"BCH\" , \"market_cap\" : \"\\n$9,107,466,682\\n\" , \"price\" : \"526.167151135\" , \"circulating_supply\" : \"17309075.0\" , \"volume\" : \"291574904.596\" }, ...","title":"Run the spider"},{"location":"Coding/Python/Scripts/Scrapy_and_Scrapyrt.html#install_scrapyrt_and_combine_it_with_our_project","text":"Let\u2019s now use Scrapyrt to serve those data through an HTTP request instead of having them saved in a JSON file. To install scrapyrt we run pip install scrapyrt To use it, open your terminal again and set your working directory to the Scrapy project folder. Then run the following command: scrapyrt -p <PORT> <PORT> can be replaced with a port number. For example scrapyrt -p 3000 With this command Scrapyrt will setup locally a simple HTTP server that will allow you to control your crawler. We can access it with a GET request through the endpoint http://localhost:<PORT>/crawl.json. To work properly it also needs at least these two arguments: start_requests (Boolean) and spider_name (string) . to see the results we can open the browser on: http://localhost:3000/crawl.json?start_requests=true&spider_name=coin The result should look like this:","title":"Install Scrapyrt and combine it with our project"},{"location":"Coding/Python/Scripts/create_dataset_api_python.html","text":"The idea will be follow the article from medium Creating a dataset using an API with Python from this we will change code at will to fit more my personal style or just to try different things. Import Libraries \u00b6 There will be 3 main libraries to be use in this project: request this will help use to get content from the API usign the method get() and to decide the format of how are we getting this info using json() so we can handle the answer for he API using JSON. json with this library we can work with JSON pandas this will help us to create the dataframes that later can be export to a .cvs file. we will import numpy as well, \u2026 just in case. Understanding the API \u00b6 We want first to understand what is the information that the API is giving us, what type of data and what values, so for that we are going to start by calling the API and display the content. first, the API end point is https://wind-bow.glitch.me/twitch-api/channels/freecodecamp this API doesn\u2019t require any authentication which will make the process easier 1 2 3 4 5 6 7 8 9 import numpy as np import pandas as pd import requests import json url = \"https://wind-bow.glitch.me/twitch-api/channels/freecodecamp\" JSONContent = requests . get ( url ) . json () content = json . dumps ( JSONContent , indent = 4 , sort_keys = True ) print ( content ) and the result is: The screen-shot looks messy but we will see the response after explain a bit the code. We use the variable URL in order to store the end point for the API, Later we use requests.get(url).json() that will give use the response of the API (URL) in a JSON format, as a final step we dump the data with dump() so we can see the content in a readable way, this is done thanks to the parameters indent=4 and sort_key=True . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 { \"_id\" : 79776140 , \"_links\" : { \"chat\" : \"https://api.twitch.tv/kraken/chat/freecodecamp\" , \"commercial\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/commercial\" , \"editors\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/editors\" , \"features\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/features\" , \"follows\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/follows\" , \"self\" : \"https://api.twitch.tv/kraken/channels/freecodecamp\" , \"stream_key\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/stream_key\" , \"subscriptions\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/subscriptions\" , \"teams\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/teams\" , \"videos\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/videos\" }, \"background\" : null , \"banner\" : null , \"broadcaster_language\" : \"en\" , \"created_at\" : \"2015-01-14T03:36:47Z\" , \"delay\" : null , \"display_name\" : \"FreeCodeCamp\" , \"followers\" : 11770 , \"game\" : \"Creative\" , \"language\" : \"en\" , \"logo\" : \"https://static-cdn.jtvnw.net/jtv_user_pictures/freecodecamp-profile_image-d9514f2df0962329-300x300.png\" , \"mature\" : false , \"name\" : \"freecodecamp\" , \"partner\" : false , \"profile_banner\" : \"https://static-cdn.jtvnw.net/jtv_user_pictures/freecodecamp-profile_banner-6f5e3445ff474aec-480.png\" , \"profile_banner_background_color\" : null , \"status\" : \"Some GoLang Today #go #golang #youtube\" , \"updated_at\" : \"2018-09-19T23:01:33Z\" , \"url\" : \"https://www.twitch.tv/freecodecamp\" , \"video_banner\" : \"https://static-cdn.jtvnw.net/jtv_user_pictures/freecodecamp-channel_offline_image-b8e133c78cd51cb0-1920x1080.png\" , \"views\" : 216340 } From this response we get enough information to continue, we are going to use few properties: _id display_name status followers views Creating the dataset \u00b6 First we will need to make a list that will contain the name of the channels that we want to get the information, later we will: Use the append() method to populate a variable with the properties we want. use the DataFrame() from pandas library to create the dataframe which is a similar structure to a table List of channels \u00b6 1 2 # List of channels we want to access channels = [ \"ESL_SC2\" , \"OgamingSC2\" , \"cretetion\" , \"freecodecamp\" , \"storbeck\" , \"habathcx\" , \"RobotCaleb\" ] Variable to save the information after hit the API \u00b6 1 Channels_list = [] Loop to get response for each channels \u00b6 1 2 3 for channel in channels ; JSONContent = requests . get ( \"https://wind-bow.glitch.me/twitch-api/channels/\" + channel ) . json () Channels_list . append ([ JSONContent [ \"_id\" ], JSONContent [ \"display_name\" ], JSONContent [ \"status\" ], JSONContent [ \"followers\" ], JSONContent [ \"views\" ]]) we use the [] to access the specific parameter, to this point the script will loop like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import numpy as np import pandas as pd import requests import json url = \"https://wind-bow.glitch.me/twitch-api/channels/freecodecamp\" JSONContent = requests . get ( url ) . json () content = json . dumps ( JSONContent , indent = 4 , sort_keys = True ) #print(content) channels = [ \"ESL_SC2\" , \"OgamingSC2\" , \"cretetion\" , \"freecodecamp\" , \"storbeck\" , \"habathcx\" , \"RobotCaleb\" ] channels_list = [] for channel in channels : JSONContent = requests . get ( \"https://wind-bow.glitch.me/twitch-api/channels/\" + channel ) . json () channels_list . append ([ JSONContent [ '_id' ], JSONContent [ 'display_name' ], JSONContent [ 'status' ], JSONContent [ 'followers' ], JSONContent [ 'views' ]]) #print(channels_list) dataset = pd . DataFrame ( channels_list ) print ( dataset . sample ( 5 )) producing this result: sample(5) allow me to display 5 random records Enhancing the Dataset \u00b6 Now we can see that are few things we can improve in the dataset we are creating; The headings: there right now just numbers, but we can name them to represent the columns in more meaningful way. There are some empty cells we can remove them The headings \u00b6 we can use the method columns() from pandas to name the columns 1 2 3 4 5 dataset = pd . DataFrame ( channels_list ) dataset . columns = [ 'ID' , 'Name' , 'Status' , 'Followers' , \"Views\" ] print ( dataset . sample ( 5 )) Removing rows with Empty columns \u00b6 In this case we are going to use dropan(axis = 0, how = 'any', inplace = True) this will drop the rows that has some empty columns, after this we need to reindex the dataframe for that we use dataset.index = pd.RangeIndex(len(dataset.index)) 1 2 dataset . dropna ( axis = 0 , how = 'any' , inplace = True ) dataset . index = pd . RangeIndex ( len ( dataset . index )) so the code will be: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import numpy as np import pandas as pd import requests import json url = \"https://wind-bow.glitch.me/twitch-api/channels/freecodecamp\" JSONContent = requests . get ( url ) . json () content = json . dumps ( JSONContent , indent = 4 , sort_keys = True ) #print(content) channels = [ \"ESL_SC2\" , \"OgamingSC2\" , \"cretetion\" , \"freecodecamp\" , \"storbeck\" , \"habathcx\" , \"RobotCaleb\" ] channels_list = [] for channel in channels : JSONContent = requests . get ( \"https://wind-bow.glitch.me/twitch-api/channels/\" + channel ) . json () channels_list . append ([ JSONContent [ '_id' ], JSONContent [ 'display_name' ], JSONContent [ 'status' ], JSONContent [ 'followers' ], JSONContent [ 'views' ]]) #print(channels_list) dataset = pd . DataFrame ( channels_list ) #Name the columns dataset . columns = [ 'ID' , 'Name' , 'Status' , 'Followers' , \"Views\" ] #drop rows with empty columns dataset . dropna ( axis = 0 , how = 'any' , inplace = True ) #re-index the DataFrame dataset . index = pd . RangeIndex ( len ( dataset . index )) print ( dataset . sample ( 5 ))","title":"Creating a dataset with api and python"},{"location":"Coding/Python/Scripts/create_dataset_api_python.html#import_libraries","text":"There will be 3 main libraries to be use in this project: request this will help use to get content from the API usign the method get() and to decide the format of how are we getting this info using json() so we can handle the answer for he API using JSON. json with this library we can work with JSON pandas this will help us to create the dataframes that later can be export to a .cvs file. we will import numpy as well, \u2026 just in case.","title":"Import Libraries"},{"location":"Coding/Python/Scripts/create_dataset_api_python.html#understanding_the_api","text":"We want first to understand what is the information that the API is giving us, what type of data and what values, so for that we are going to start by calling the API and display the content. first, the API end point is https://wind-bow.glitch.me/twitch-api/channels/freecodecamp this API doesn\u2019t require any authentication which will make the process easier 1 2 3 4 5 6 7 8 9 import numpy as np import pandas as pd import requests import json url = \"https://wind-bow.glitch.me/twitch-api/channels/freecodecamp\" JSONContent = requests . get ( url ) . json () content = json . dumps ( JSONContent , indent = 4 , sort_keys = True ) print ( content ) and the result is: The screen-shot looks messy but we will see the response after explain a bit the code. We use the variable URL in order to store the end point for the API, Later we use requests.get(url).json() that will give use the response of the API (URL) in a JSON format, as a final step we dump the data with dump() so we can see the content in a readable way, this is done thanks to the parameters indent=4 and sort_key=True . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 { \"_id\" : 79776140 , \"_links\" : { \"chat\" : \"https://api.twitch.tv/kraken/chat/freecodecamp\" , \"commercial\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/commercial\" , \"editors\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/editors\" , \"features\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/features\" , \"follows\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/follows\" , \"self\" : \"https://api.twitch.tv/kraken/channels/freecodecamp\" , \"stream_key\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/stream_key\" , \"subscriptions\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/subscriptions\" , \"teams\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/teams\" , \"videos\" : \"https://api.twitch.tv/kraken/channels/freecodecamp/videos\" }, \"background\" : null , \"banner\" : null , \"broadcaster_language\" : \"en\" , \"created_at\" : \"2015-01-14T03:36:47Z\" , \"delay\" : null , \"display_name\" : \"FreeCodeCamp\" , \"followers\" : 11770 , \"game\" : \"Creative\" , \"language\" : \"en\" , \"logo\" : \"https://static-cdn.jtvnw.net/jtv_user_pictures/freecodecamp-profile_image-d9514f2df0962329-300x300.png\" , \"mature\" : false , \"name\" : \"freecodecamp\" , \"partner\" : false , \"profile_banner\" : \"https://static-cdn.jtvnw.net/jtv_user_pictures/freecodecamp-profile_banner-6f5e3445ff474aec-480.png\" , \"profile_banner_background_color\" : null , \"status\" : \"Some GoLang Today #go #golang #youtube\" , \"updated_at\" : \"2018-09-19T23:01:33Z\" , \"url\" : \"https://www.twitch.tv/freecodecamp\" , \"video_banner\" : \"https://static-cdn.jtvnw.net/jtv_user_pictures/freecodecamp-channel_offline_image-b8e133c78cd51cb0-1920x1080.png\" , \"views\" : 216340 } From this response we get enough information to continue, we are going to use few properties: _id display_name status followers views","title":"Understanding the API"},{"location":"Coding/Python/Scripts/create_dataset_api_python.html#creating_the_dataset","text":"First we will need to make a list that will contain the name of the channels that we want to get the information, later we will: Use the append() method to populate a variable with the properties we want. use the DataFrame() from pandas library to create the dataframe which is a similar structure to a table","title":"Creating the dataset"},{"location":"Coding/Python/Scripts/create_dataset_api_python.html#list_of_channels","text":"1 2 # List of channels we want to access channels = [ \"ESL_SC2\" , \"OgamingSC2\" , \"cretetion\" , \"freecodecamp\" , \"storbeck\" , \"habathcx\" , \"RobotCaleb\" ]","title":"List of channels"},{"location":"Coding/Python/Scripts/create_dataset_api_python.html#variable_to_save_the_information_after_hit_the_api","text":"1 Channels_list = []","title":"Variable to save the information after hit the API"},{"location":"Coding/Python/Scripts/create_dataset_api_python.html#loop_to_get_response_for_each_channels","text":"1 2 3 for channel in channels ; JSONContent = requests . get ( \"https://wind-bow.glitch.me/twitch-api/channels/\" + channel ) . json () Channels_list . append ([ JSONContent [ \"_id\" ], JSONContent [ \"display_name\" ], JSONContent [ \"status\" ], JSONContent [ \"followers\" ], JSONContent [ \"views\" ]]) we use the [] to access the specific parameter, to this point the script will loop like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import numpy as np import pandas as pd import requests import json url = \"https://wind-bow.glitch.me/twitch-api/channels/freecodecamp\" JSONContent = requests . get ( url ) . json () content = json . dumps ( JSONContent , indent = 4 , sort_keys = True ) #print(content) channels = [ \"ESL_SC2\" , \"OgamingSC2\" , \"cretetion\" , \"freecodecamp\" , \"storbeck\" , \"habathcx\" , \"RobotCaleb\" ] channels_list = [] for channel in channels : JSONContent = requests . get ( \"https://wind-bow.glitch.me/twitch-api/channels/\" + channel ) . json () channels_list . append ([ JSONContent [ '_id' ], JSONContent [ 'display_name' ], JSONContent [ 'status' ], JSONContent [ 'followers' ], JSONContent [ 'views' ]]) #print(channels_list) dataset = pd . DataFrame ( channels_list ) print ( dataset . sample ( 5 )) producing this result: sample(5) allow me to display 5 random records","title":"Loop to get response for each channels"},{"location":"Coding/Python/Scripts/create_dataset_api_python.html#enhancing_the_dataset","text":"Now we can see that are few things we can improve in the dataset we are creating; The headings: there right now just numbers, but we can name them to represent the columns in more meaningful way. There are some empty cells we can remove them","title":"Enhancing the Dataset"},{"location":"Coding/Python/Scripts/create_dataset_api_python.html#the_headings","text":"we can use the method columns() from pandas to name the columns 1 2 3 4 5 dataset = pd . DataFrame ( channels_list ) dataset . columns = [ 'ID' , 'Name' , 'Status' , 'Followers' , \"Views\" ] print ( dataset . sample ( 5 ))","title":"The headings"},{"location":"Coding/Python/Scripts/create_dataset_api_python.html#removing_rows_with_empty_columns","text":"In this case we are going to use dropan(axis = 0, how = 'any', inplace = True) this will drop the rows that has some empty columns, after this we need to reindex the dataframe for that we use dataset.index = pd.RangeIndex(len(dataset.index)) 1 2 dataset . dropna ( axis = 0 , how = 'any' , inplace = True ) dataset . index = pd . RangeIndex ( len ( dataset . index )) so the code will be: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import numpy as np import pandas as pd import requests import json url = \"https://wind-bow.glitch.me/twitch-api/channels/freecodecamp\" JSONContent = requests . get ( url ) . json () content = json . dumps ( JSONContent , indent = 4 , sort_keys = True ) #print(content) channels = [ \"ESL_SC2\" , \"OgamingSC2\" , \"cretetion\" , \"freecodecamp\" , \"storbeck\" , \"habathcx\" , \"RobotCaleb\" ] channels_list = [] for channel in channels : JSONContent = requests . get ( \"https://wind-bow.glitch.me/twitch-api/channels/\" + channel ) . json () channels_list . append ([ JSONContent [ '_id' ], JSONContent [ 'display_name' ], JSONContent [ 'status' ], JSONContent [ 'followers' ], JSONContent [ 'views' ]]) #print(channels_list) dataset = pd . DataFrame ( channels_list ) #Name the columns dataset . columns = [ 'ID' , 'Name' , 'Status' , 'Followers' , \"Views\" ] #drop rows with empty columns dataset . dropna ( axis = 0 , how = 'any' , inplace = True ) #re-index the DataFrame dataset . index = pd . RangeIndex ( len ( dataset . index )) print ( dataset . sample ( 5 ))","title":"Removing rows with Empty columns"},{"location":"Coding/Python/Scripts/create_excel_files.html","text":"First we are going to create a csv file using the build-in csv module, after that we will proceed to use a third party library to create a xlsx file. Creating a CSV file \u00b6 To be able to work with this type of file we are going to import CSV in the following way 1 import csv CVS Module \u00b6 the CVS module includes all necessary methods built-in some of which are: csv.reader csv.writer csv.DictReader csv.DictWriter with this method we can edit, modify and manipulate the stored data in a csv file. Initial preparation \u00b6 First we will need to prepare the file so it can be run with python namefile.py for that we add the if __name__ == \"__main__\" 1 2 3 4 5 6 7 import csv if __name__ = \"__main__\" : main () Name of the file, header and data \u00b6 Now, we proceed to define some variables, the name of the file with filename , later the headers ( or the name of the columns) with header and provide some data with data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import csv filename = \"imdb_top_4.csv\" header = ( \"Rank\" , \"Rating\" , \"Title\" ) data = [ ( 1 , 9.2 , \"The Shawshank Redemption(1994)\" ), ( 2 , 9.2 , \"The Godfather(1972)\" ), ( 3 , 9 , \"The Godfather: Part II(1974)\" ), ( 4 , 8.9 , \"Pulp Fiction(1994)\" ) ] if __name__ = \"__main__\" : main () Function to write on the file \u00b6 The next step will be create a function that write the data on the file, this function will have 3 parameters, header , data , and filename . Inside this function we will write the first line of the file with the content of header, this will give the name to the columns and later with a for loop we will write the data. 1 2 3 4 5 6 def writer ( header , data , filename ): with open ( filename , \"w\" , newline = \" \" ) as csvfile : movies = cvs . writer ( csvfile ) movies . writerow ( header ) for x in data : movies . writerow ( x ) so the full, first version of the script will be, notice that there is a call to the function writer after the variable data is defined. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import csv filename = \"imdb_top_4.csv\" header = ( \"Rank\" , \"Rating\" , \"Title\" ) data = [ ( 1 , 9.2 , \"The Shawshank Redemption(1994)\" ), ( 2 , 9.2 , \"The Godfather(1972)\" ), ( 3 , 9 , \"The Godfather: Part II(1974)\" ), ( 4 , 8.9 , \"Pulp Fiction(1994)\" ) ] writer ( header , data , filename , \"write\" ) def writer ( header , data , filename ): with open ( filename , \"w\" , newline = \" \" ) as csvfile : movies = cvs . writer ( csvfile ) movies . writerow ( header ) for x in data : movies . writerow ( x ) if __name__ = \"__main__\" : main () And the result will be like: Updating a CSV files \u00b6 To update this type of file and in this case we will need to create a new function named \u2018 updater \u2018 that will just take the filename as a parameter. 1 2 3 4 5 6 7 def updater ( filename ): with open ( filename , newline = \"\" ) as file : readData = [ row for row in csv . DictReader ( file )] readData [ 0 ][ 'Rating' ] = '9.4' readHeader = readData [ 0 ] . keys () writer ( readHeader , readData , filename , \"update\" ) In this function: We open the file define in filename and we are going to called it \u2018file\u2019. Save all the information from that file in a variable called readData , cvs.DictReader the next line readData[0]['Rating'] = '9.4' is hard-coding the value 9.4 in the \u2018Rating\u2019 column. the last line will tell the function writer that we are executing an update ( this option is not yet define in the function writer that will be the next step) finally we add a call to the function uodater after the call to the function writer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import csv filename = \"imdb_top_4.csv\" header = ( \"Rank\" , \"Rating\" , \"Title\" ) data = [ ( 1 , 9.2 , \"The Shawshank Redemption(1994)\" ), ( 2 , 9.2 , \"The Godfather(1972)\" ), ( 3 , 9 , \"The Godfather: Part II(1974)\" ), ( 4 , 8.9 , \"Pulp Fiction(1994)\" ) ] writer ( header , data , filename , \"write\" ) updater ( filename ) def writer ( header , data , filename ): with open ( filename , \"w\" , newline = \" \" ) as csvfile : movies = cvs . writer ( csvfile ) movies . writerow ( header ) for x in data : movies . writerow ( x ) # TODO option to update def updater ( filename ): with open ( filename , newline = \"\" ) as file : readData = [ row for row in csv . DictReader ( file )] readData [ 0 ][ 'Rating' ] = '9.4' readHeader = readData [ 0 ] . keys () writer ( readHeader , readData , filename , \"update\" ) if __name__ = \"__main__\" : main () Create the option for update \u00b6 Now, we need to modify the function writer to be able to receive an extra parameter, and inside, some modifications to handle these options for \u201c write \u201d and \u201c update \u201d. lets add the extra parameter, this will be called option 1 2 def writer ( header , data , filename , option ): ... Inside we are going to create a decision loop to execute some part of the code depending of the the parameter option 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def writer ( header , data , filename , option ): with open ( filename , \"w\" , newline = \"\" ) as csvfile : if option == \"write\" : movies = csv . writer ( csvfile ) movies . writerow ( headers ) for x in data : movies . writerow ( x ) elif option == \"update\" : writer = csv . DictWriter ( csvfile , fieldnames = headers ) writer . writeheader () write . writerow ( data ) else : print ( \"option is not know\" ) More information about DictWriter here but basically here is use to write a row with the field names. so with all this changes we will have a script that look like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import csv filename = \"imdb_top_4.csv\" header = ( \"Rank\" , \"Rating\" , \"Title\" ) data = [ ( 1 , 9.2 , \"The Shawshank Redemption(1994)\" ), ( 2 , 9.2 , \"The Godfather(1972)\" ), ( 3 , 9 , \"The Godfather: Part II(1974)\" ), ( 4 , 8.9 , \"Pulp Fiction(1994)\" ) ] writer ( header , data , filename , \"write\" ) updater ( filename ) def writer ( header , data , filename , option ): with open ( filename , \"w\" , newline = \"\" ) as csvfile : if option == \"write\" : movies = csv . writer ( csvfile ) movies . writerow ( headers ) for x in data : movies . writerow ( x ) elif option == \"update\" : writer = csv . DictWriter ( csvfile , fieldnames = headers ) writer . writeheader () write . writerow ( data ) else : print ( \"option is not know\" ) def updater ( filename ): with open ( filename , newline = \"\" ) as file : readData = [ row for row in csv . DictReader ( file )] readData [ 0 ][ 'Rating' ] = '9.4' readHeader = readData [ 0 ] . keys () writer ( readHeader , readData , filename , \"update\" ) if __name__ = \"__main__\" : main () The xlsx File \u00b6 This will be an enhancement of the previous part, that doesn\u2019t mean previous part is not a solution, just that this solution will include openpyxl which is a all in one solution to work with worksheets, loading, updating m renaming and deleting them. Basic terminology \u00b6 WorkBook is the name for a an Excel file in openpyxl . A workbook consist of sheets( default is 1 sheet). sheets are referenced by their names. A Sheet consist of rows ( horizontal lines) starting from the number 1 and columns ( vertical lines) starting the letter A. Rows and columns result in a grid and form cells which may contain some data ( numerical or string value) or formulas. First Steps with openpyxl \u00b6 First we need to install openpyxl this can be done using pip 1 pip install openpyxl For the most basic usage, this been creating a new workbook with just one sheet Create a workbook \u00b6 To create a workbook we can use the function Workbook() 1 2 from openpyxl import Workbook wb = Workbook () now, we need to create the first sheet, the following statement is just for the first sheet 1 2 3 4 from openpyxl import Workbook wb = Workbook () ws = wb . active by default the name of the sheet will be \u201csheet\u201d and a number, so the first sheet will be \u201csheet\u201d, the second \u201cSheet1\u201d, etc. Now, we have the workbook and the first sheet, what about the second sheet?, To create the second sheet and all the following sheets, we use create_sheet(\"name of the sheet\") 1 2 3 4 5 from openpyxl import Workbook wb = Workbook () ws = wb . active ws1 = wb . create_sheet ( \"second sheet\" ) We can use the create_sheet(\"name of the sheet\") and by defautl the sheet will be created after the previous sheet, but if we want to insert the sheet in a specific spot we can use create_sheet(\"name of the sheet\",0) in this case this sheet will be insert in the first spot. At any moment we can change the title of the sheet using .title as follow 1 2 3 4 5 6 7 8 from openpyxl import Workbook wb = Workbook () ws = wb . active ws1 = wb . create_sheet ( \"second sheet\" ) #change the name of the first sheet ws . title = \"first sheet\" Save the workbook \u00b6 To save the workbook, first we need to give it a name dest_filename = \"name of the file\" , later, we use the function save(filename=\"name of the workbook\") 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from openpyxl import Workbook # Creating the workbook wb = Workbook () # the name of the file dest_filename = 'test_openpyxl.xlsx' ws = wb . active ws1 = wb . create_sheet ( \"second sheet\" ) #change the name of the first sheet ws . title = \"first sheet\" wb . save ( filename = dest_filename ) For more details or tutorial we can visit openpyxl documentation we just saw how to create a workbook, now, for this example, we are going to focus in manipulate a workbook that already exist, for that reason we will create one called \u201cCustomers1.xlsx\u201d and continue working on it Working with xlsx files \u00b6 Now, we are going to work with an existing file called \u201c customers1.xlsx \u201d, this look like this: The file contain 6 columns and 11 rows, We are going to import it and: Print the sheets name save the current sheet on the variable current_Sheet print the value of the second column and 4 row (B4). 1 2 3 4 import openpyxl as opxl theFile = opxl . load_workbook ( 'Customers1.xlsx' ) print ( theFile . sheetnames ) the result: We got the file, and display the sheets names, now step 2 and 3, Save the current sheet in a variable and print the value of the cell B4. 1 2 3 4 5 6 7 import openpyxl as opxl theFile = opxl . load_workbook ( 'Customers1.xlsx' ) print ( theFile . sheetnames ) current_sheet = theFile [ 'customers 1' ] print ( current_sheet [ 'B4' ] . value ) there is hardcoded values in this script which make it not that flexible, but we can make modifications in the future, for now we are going to do the following: Read the file Get all sheet names Loop through all sheets In the last step, the code will print values that are located in B4 fields of each found sheet inside the workbook. 1 2 3 4 5 6 7 8 9 10 import openpyxl as opxl the_File = opxl . load_workbook ( 'Customers1.xlsx' ) print ( the_File . sheetnames ) all_sheets = the_File . sheetnames for x in all_sheets : current_sheet = the_File [ x ] print ( current_sheet [ 'B4' ] . value ) Next, We will use the string 'ABCDEF' to create a loop through the content of each sheet. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import openpyxl as opxl the_File = opxl . load_workbook ( 'Customers1.xlsx' ) print ( the_File . sheetnames ) columns_marks = 'ABCDEF' all_sheets = the_File . sheetnames for x in all_sheets : current_sheet = the_File [ x ] for row in range ( 1 , current_sheet . max_row + 1 ): for column in columns_marks : cell_name = \"{}{}\" . format ( column , row ) # creat the structure B4 or C4 etc print ( \"Cell Position: {} has the value {}\" . format ( cell_name , current_sheet [ cell_name ] . value )) Rudimentary way to find content of a column \u00b6 The idea will be find the content of the column named \u201ctelephone\u201d and display its content. For that we will start by creating a function that will hold the loops, this loops will look row by row and column by column until we found the column named \u201ctelephone\u201d 1 2 3 4 5 6 def find_specific_cell (): for row in range ( 1 , current_sheet . max_row + 1 ): for column in columns_marks : cell_name = \"{}{}\" . format ( column , row ) if current_sheet [ cell_name ] . value == \"telephone\" : return cell_name now with the function, we can create a loop that iterate over the different sheets looking for the specific column that we are looking for 1 2 3 for sheet in all_sheets : current_sheet = the_File [ sheet ] print ( find_specific_cell ) so the script including this new function will be: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import openpyxl as opxl the_File = opxl . load_workbook ( 'Customers1.xlsx' ) print ( the_File . sheetnames ) columns_marks = 'ABCDEF' all_sheets = the_File . sheetnames def find_specific_cell (): for row in range ( 1 , current_sheet . max_row + 1 ): for column in columns_marks : cell_name = \"{}{}\" . format ( column , row ) # create the structure B4 or C4 etc if current_sheet [ cell_name ] . value == \"telephone\" : return cell_name for sheet in all_sheets : print ( \"Current sheet is: {}\" . format ( sheet )) current_sheet = the_File [ sheet ] print ( find_specific_cell ()) so, with few modification and the creation of the second functions we have 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import openpyxl as opxl the_File = opxl . load_workbook ( 'Customers1.xlsx' ) print ( the_File . sheetnames ) columns_marks = 'ABCDEF' #adding all the sheets to this variable all_sheets = the_File . sheetnames def find_specific_cell (): for row in range ( 1 , current_sheet . max_row + 1 ): for column in columns_marks : cell_name = \"{}{}\" . format ( column , row ) if current_sheet [ cell_name ] . value == \"telephone\" : return ( cell_name , column ) def print_all ( cell_name ): _ , column_letter = cell_name for row in range ( 1 , current_sheet . max_row + 1 ): cell = column_letter + str ( row ) print ( current_sheet [ cell ] . value ) for sheet in all_sheets : print ( \"Current sheet is: {}\" . format ( sheet )) current_sheet = the_File [ sheet ] cell = find_specific_cell () print ( print_all ( cell ))","title":"Create Excel Files With Python"},{"location":"Coding/Python/Scripts/create_excel_files.html#creating_a_csv_file","text":"To be able to work with this type of file we are going to import CSV in the following way 1 import csv","title":"Creating a CSV file"},{"location":"Coding/Python/Scripts/create_excel_files.html#cvs_module","text":"the CVS module includes all necessary methods built-in some of which are: csv.reader csv.writer csv.DictReader csv.DictWriter with this method we can edit, modify and manipulate the stored data in a csv file.","title":"CVS Module"},{"location":"Coding/Python/Scripts/create_excel_files.html#initial_preparation","text":"First we will need to prepare the file so it can be run with python namefile.py for that we add the if __name__ == \"__main__\" 1 2 3 4 5 6 7 import csv if __name__ = \"__main__\" : main ()","title":"Initial preparation"},{"location":"Coding/Python/Scripts/create_excel_files.html#name_of_the_file_header_and_data","text":"Now, we proceed to define some variables, the name of the file with filename , later the headers ( or the name of the columns) with header and provide some data with data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import csv filename = \"imdb_top_4.csv\" header = ( \"Rank\" , \"Rating\" , \"Title\" ) data = [ ( 1 , 9.2 , \"The Shawshank Redemption(1994)\" ), ( 2 , 9.2 , \"The Godfather(1972)\" ), ( 3 , 9 , \"The Godfather: Part II(1974)\" ), ( 4 , 8.9 , \"Pulp Fiction(1994)\" ) ] if __name__ = \"__main__\" : main ()","title":"Name of the file, header and data"},{"location":"Coding/Python/Scripts/create_excel_files.html#function_to_write_on_the_file","text":"The next step will be create a function that write the data on the file, this function will have 3 parameters, header , data , and filename . Inside this function we will write the first line of the file with the content of header, this will give the name to the columns and later with a for loop we will write the data. 1 2 3 4 5 6 def writer ( header , data , filename ): with open ( filename , \"w\" , newline = \" \" ) as csvfile : movies = cvs . writer ( csvfile ) movies . writerow ( header ) for x in data : movies . writerow ( x ) so the full, first version of the script will be, notice that there is a call to the function writer after the variable data is defined. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import csv filename = \"imdb_top_4.csv\" header = ( \"Rank\" , \"Rating\" , \"Title\" ) data = [ ( 1 , 9.2 , \"The Shawshank Redemption(1994)\" ), ( 2 , 9.2 , \"The Godfather(1972)\" ), ( 3 , 9 , \"The Godfather: Part II(1974)\" ), ( 4 , 8.9 , \"Pulp Fiction(1994)\" ) ] writer ( header , data , filename , \"write\" ) def writer ( header , data , filename ): with open ( filename , \"w\" , newline = \" \" ) as csvfile : movies = cvs . writer ( csvfile ) movies . writerow ( header ) for x in data : movies . writerow ( x ) if __name__ = \"__main__\" : main () And the result will be like:","title":"Function to write on the file"},{"location":"Coding/Python/Scripts/create_excel_files.html#updating_a_csv_files","text":"To update this type of file and in this case we will need to create a new function named \u2018 updater \u2018 that will just take the filename as a parameter. 1 2 3 4 5 6 7 def updater ( filename ): with open ( filename , newline = \"\" ) as file : readData = [ row for row in csv . DictReader ( file )] readData [ 0 ][ 'Rating' ] = '9.4' readHeader = readData [ 0 ] . keys () writer ( readHeader , readData , filename , \"update\" ) In this function: We open the file define in filename and we are going to called it \u2018file\u2019. Save all the information from that file in a variable called readData , cvs.DictReader the next line readData[0]['Rating'] = '9.4' is hard-coding the value 9.4 in the \u2018Rating\u2019 column. the last line will tell the function writer that we are executing an update ( this option is not yet define in the function writer that will be the next step) finally we add a call to the function uodater after the call to the function writer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 import csv filename = \"imdb_top_4.csv\" header = ( \"Rank\" , \"Rating\" , \"Title\" ) data = [ ( 1 , 9.2 , \"The Shawshank Redemption(1994)\" ), ( 2 , 9.2 , \"The Godfather(1972)\" ), ( 3 , 9 , \"The Godfather: Part II(1974)\" ), ( 4 , 8.9 , \"Pulp Fiction(1994)\" ) ] writer ( header , data , filename , \"write\" ) updater ( filename ) def writer ( header , data , filename ): with open ( filename , \"w\" , newline = \" \" ) as csvfile : movies = cvs . writer ( csvfile ) movies . writerow ( header ) for x in data : movies . writerow ( x ) # TODO option to update def updater ( filename ): with open ( filename , newline = \"\" ) as file : readData = [ row for row in csv . DictReader ( file )] readData [ 0 ][ 'Rating' ] = '9.4' readHeader = readData [ 0 ] . keys () writer ( readHeader , readData , filename , \"update\" ) if __name__ = \"__main__\" : main ()","title":"Updating a CSV files"},{"location":"Coding/Python/Scripts/create_excel_files.html#create_the_option_for_update","text":"Now, we need to modify the function writer to be able to receive an extra parameter, and inside, some modifications to handle these options for \u201c write \u201d and \u201c update \u201d. lets add the extra parameter, this will be called option 1 2 def writer ( header , data , filename , option ): ... Inside we are going to create a decision loop to execute some part of the code depending of the the parameter option 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def writer ( header , data , filename , option ): with open ( filename , \"w\" , newline = \"\" ) as csvfile : if option == \"write\" : movies = csv . writer ( csvfile ) movies . writerow ( headers ) for x in data : movies . writerow ( x ) elif option == \"update\" : writer = csv . DictWriter ( csvfile , fieldnames = headers ) writer . writeheader () write . writerow ( data ) else : print ( \"option is not know\" ) More information about DictWriter here but basically here is use to write a row with the field names. so with all this changes we will have a script that look like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import csv filename = \"imdb_top_4.csv\" header = ( \"Rank\" , \"Rating\" , \"Title\" ) data = [ ( 1 , 9.2 , \"The Shawshank Redemption(1994)\" ), ( 2 , 9.2 , \"The Godfather(1972)\" ), ( 3 , 9 , \"The Godfather: Part II(1974)\" ), ( 4 , 8.9 , \"Pulp Fiction(1994)\" ) ] writer ( header , data , filename , \"write\" ) updater ( filename ) def writer ( header , data , filename , option ): with open ( filename , \"w\" , newline = \"\" ) as csvfile : if option == \"write\" : movies = csv . writer ( csvfile ) movies . writerow ( headers ) for x in data : movies . writerow ( x ) elif option == \"update\" : writer = csv . DictWriter ( csvfile , fieldnames = headers ) writer . writeheader () write . writerow ( data ) else : print ( \"option is not know\" ) def updater ( filename ): with open ( filename , newline = \"\" ) as file : readData = [ row for row in csv . DictReader ( file )] readData [ 0 ][ 'Rating' ] = '9.4' readHeader = readData [ 0 ] . keys () writer ( readHeader , readData , filename , \"update\" ) if __name__ = \"__main__\" : main ()","title":"Create the option for update"},{"location":"Coding/Python/Scripts/create_excel_files.html#the_xlsx_file","text":"This will be an enhancement of the previous part, that doesn\u2019t mean previous part is not a solution, just that this solution will include openpyxl which is a all in one solution to work with worksheets, loading, updating m renaming and deleting them.","title":"The xlsx File"},{"location":"Coding/Python/Scripts/create_excel_files.html#basic_terminology","text":"WorkBook is the name for a an Excel file in openpyxl . A workbook consist of sheets( default is 1 sheet). sheets are referenced by their names. A Sheet consist of rows ( horizontal lines) starting from the number 1 and columns ( vertical lines) starting the letter A. Rows and columns result in a grid and form cells which may contain some data ( numerical or string value) or formulas.","title":"Basic terminology"},{"location":"Coding/Python/Scripts/create_excel_files.html#first_steps_with_openpyxl","text":"First we need to install openpyxl this can be done using pip 1 pip install openpyxl For the most basic usage, this been creating a new workbook with just one sheet","title":"First Steps with openpyxl"},{"location":"Coding/Python/Scripts/create_excel_files.html#create_a_workbook","text":"To create a workbook we can use the function Workbook() 1 2 from openpyxl import Workbook wb = Workbook () now, we need to create the first sheet, the following statement is just for the first sheet 1 2 3 4 from openpyxl import Workbook wb = Workbook () ws = wb . active by default the name of the sheet will be \u201csheet\u201d and a number, so the first sheet will be \u201csheet\u201d, the second \u201cSheet1\u201d, etc. Now, we have the workbook and the first sheet, what about the second sheet?, To create the second sheet and all the following sheets, we use create_sheet(\"name of the sheet\") 1 2 3 4 5 from openpyxl import Workbook wb = Workbook () ws = wb . active ws1 = wb . create_sheet ( \"second sheet\" ) We can use the create_sheet(\"name of the sheet\") and by defautl the sheet will be created after the previous sheet, but if we want to insert the sheet in a specific spot we can use create_sheet(\"name of the sheet\",0) in this case this sheet will be insert in the first spot. At any moment we can change the title of the sheet using .title as follow 1 2 3 4 5 6 7 8 from openpyxl import Workbook wb = Workbook () ws = wb . active ws1 = wb . create_sheet ( \"second sheet\" ) #change the name of the first sheet ws . title = \"first sheet\"","title":"Create a workbook"},{"location":"Coding/Python/Scripts/create_excel_files.html#save_the_workbook","text":"To save the workbook, first we need to give it a name dest_filename = \"name of the file\" , later, we use the function save(filename=\"name of the workbook\") 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from openpyxl import Workbook # Creating the workbook wb = Workbook () # the name of the file dest_filename = 'test_openpyxl.xlsx' ws = wb . active ws1 = wb . create_sheet ( \"second sheet\" ) #change the name of the first sheet ws . title = \"first sheet\" wb . save ( filename = dest_filename ) For more details or tutorial we can visit openpyxl documentation we just saw how to create a workbook, now, for this example, we are going to focus in manipulate a workbook that already exist, for that reason we will create one called \u201cCustomers1.xlsx\u201d and continue working on it","title":"Save the workbook"},{"location":"Coding/Python/Scripts/create_excel_files.html#working_with_xlsx_files","text":"Now, we are going to work with an existing file called \u201c customers1.xlsx \u201d, this look like this: The file contain 6 columns and 11 rows, We are going to import it and: Print the sheets name save the current sheet on the variable current_Sheet print the value of the second column and 4 row (B4). 1 2 3 4 import openpyxl as opxl theFile = opxl . load_workbook ( 'Customers1.xlsx' ) print ( theFile . sheetnames ) the result: We got the file, and display the sheets names, now step 2 and 3, Save the current sheet in a variable and print the value of the cell B4. 1 2 3 4 5 6 7 import openpyxl as opxl theFile = opxl . load_workbook ( 'Customers1.xlsx' ) print ( theFile . sheetnames ) current_sheet = theFile [ 'customers 1' ] print ( current_sheet [ 'B4' ] . value ) there is hardcoded values in this script which make it not that flexible, but we can make modifications in the future, for now we are going to do the following: Read the file Get all sheet names Loop through all sheets In the last step, the code will print values that are located in B4 fields of each found sheet inside the workbook. 1 2 3 4 5 6 7 8 9 10 import openpyxl as opxl the_File = opxl . load_workbook ( 'Customers1.xlsx' ) print ( the_File . sheetnames ) all_sheets = the_File . sheetnames for x in all_sheets : current_sheet = the_File [ x ] print ( current_sheet [ 'B4' ] . value ) Next, We will use the string 'ABCDEF' to create a loop through the content of each sheet. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import openpyxl as opxl the_File = opxl . load_workbook ( 'Customers1.xlsx' ) print ( the_File . sheetnames ) columns_marks = 'ABCDEF' all_sheets = the_File . sheetnames for x in all_sheets : current_sheet = the_File [ x ] for row in range ( 1 , current_sheet . max_row + 1 ): for column in columns_marks : cell_name = \"{}{}\" . format ( column , row ) # creat the structure B4 or C4 etc print ( \"Cell Position: {} has the value {}\" . format ( cell_name , current_sheet [ cell_name ] . value ))","title":"Working with xlsx files"},{"location":"Coding/Python/Scripts/create_excel_files.html#rudimentary_way_to_find_content_of_a_column","text":"The idea will be find the content of the column named \u201ctelephone\u201d and display its content. For that we will start by creating a function that will hold the loops, this loops will look row by row and column by column until we found the column named \u201ctelephone\u201d 1 2 3 4 5 6 def find_specific_cell (): for row in range ( 1 , current_sheet . max_row + 1 ): for column in columns_marks : cell_name = \"{}{}\" . format ( column , row ) if current_sheet [ cell_name ] . value == \"telephone\" : return cell_name now with the function, we can create a loop that iterate over the different sheets looking for the specific column that we are looking for 1 2 3 for sheet in all_sheets : current_sheet = the_File [ sheet ] print ( find_specific_cell ) so the script including this new function will be: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import openpyxl as opxl the_File = opxl . load_workbook ( 'Customers1.xlsx' ) print ( the_File . sheetnames ) columns_marks = 'ABCDEF' all_sheets = the_File . sheetnames def find_specific_cell (): for row in range ( 1 , current_sheet . max_row + 1 ): for column in columns_marks : cell_name = \"{}{}\" . format ( column , row ) # create the structure B4 or C4 etc if current_sheet [ cell_name ] . value == \"telephone\" : return cell_name for sheet in all_sheets : print ( \"Current sheet is: {}\" . format ( sheet )) current_sheet = the_File [ sheet ] print ( find_specific_cell ()) so, with few modification and the creation of the second functions we have 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import openpyxl as opxl the_File = opxl . load_workbook ( 'Customers1.xlsx' ) print ( the_File . sheetnames ) columns_marks = 'ABCDEF' #adding all the sheets to this variable all_sheets = the_File . sheetnames def find_specific_cell (): for row in range ( 1 , current_sheet . max_row + 1 ): for column in columns_marks : cell_name = \"{}{}\" . format ( column , row ) if current_sheet [ cell_name ] . value == \"telephone\" : return ( cell_name , column ) def print_all ( cell_name ): _ , column_letter = cell_name for row in range ( 1 , current_sheet . max_row + 1 ): cell = column_letter + str ( row ) print ( current_sheet [ cell ] . value ) for sheet in all_sheets : print ( \"Current sheet is: {}\" . format ( sheet )) current_sheet = the_File [ sheet ] cell = find_specific_cell () print ( print_all ( cell ))","title":"Rudimentary way to find content of a column"},{"location":"Coding/SQL/Advance_Funtion_SQL.html","text":"CASE function \u00b6 The CASE statement goes through conditions and return a value when the first condition is met (like an IF-THEN-ELSE statement). So, once a condition is true, it will stop reading and return the result. If no conditions are true, it will return the value in the ELSE clause. If there is no ELSE part and no conditions are true, it returns NULL. Syntax CASE \u00b6 1 2 3 4 5 6 CASE WHEN condition1 THEN result1 WHEN condition2 THEN result2 WHEN conditionN THEN resultN ELSE result END ; Example 1 2 3 4 5 6 7 SELECT CASE WHEN A + B > C AND A + C > B AND B + C > A THEN CASE WHEN A = B AND B = C THEN 'Equilateral' WHEN A = B OR B = C OR A = C THEN 'Isosceles' ELSE 'Scalene' END ELSE 'Not A Triangle' END FROM TRIANGLES ; IF() function \u00b6 The IF() function returns a value if a condition is TRUE, or another value if a condition is FALSE. Syntax IF() \u00b6 1 IF ( condition , value_if_true , value_if_false ) Example 1 SELECT IF ( 500 < 1000 , 5 , 10 ); 1 2 SELECT OrderID , Quantity , IF ( Quantity > 10 , \"MORE\" , \"LESS\" ) FROM OrderDetails ; SET @var_name User-defined Variables \u00b6 You can store a value in a user-defined variable in one statement and refer to it later in another statement. This enables you to pass values from one statement to another. User variables are written as @var_name, where the variable name var_name consists of alphanumeric characters, ., _ , and $. User variable names are not case-sensitive. Names have a maximum length of 64 characters. One way to set a user-defined variable is by issuing a SET statement: 1 SET @ var_name = expr [, @ var_name = expr ] ... For SET , either = or := can be used as the assignment operator. When making an assignment in this way, you must use := as the assignment operator; = is treated as the comparison operator in statements other than SET . Here ans example of the usage of this \u201cuser-defined variables\u201d, this is the solution to the challenge Occupations of Hacker Ranks 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SET @ dRow = 0 , @ pRow = 0 , @ sRow = 0 , @ aRow = 0 ; SELECT MIN ( Doctor ), MIN ( Professor ), MIN ( Singer ), MIN ( Actor ) FROM ( SELECT CASE Occupation WHEN 'Doctor' THEN @ dRow : = @ dRow + 1 WHEN 'Professor' THEN @ pRow : = @ pRow + 1 WHEN 'Singer' THEN @ sRow : = @ sRow + 1 WHEN 'Actor' THEN @ aRow : = @ aRow + 1 END AS row , IF ( Occupation = 'Doctor' , Name , NULL ) AS Doctor , IF ( Occupation = 'Professor' , Name , NULL ) AS Professor , IF ( Occupation = 'Singer' , Name , NULL ) AS Singer , IF ( Occupation = 'Actor' , Name , NULL ) AS Actor FROM OCCUPATIONS ORDER BY Name ) a GROUP BY row ;","title":"Advance Funtion SQL"},{"location":"Coding/SQL/Advance_Funtion_SQL.html#case_function","text":"The CASE statement goes through conditions and return a value when the first condition is met (like an IF-THEN-ELSE statement). So, once a condition is true, it will stop reading and return the result. If no conditions are true, it will return the value in the ELSE clause. If there is no ELSE part and no conditions are true, it returns NULL.","title":"CASE function"},{"location":"Coding/SQL/Advance_Funtion_SQL.html#syntax_case","text":"1 2 3 4 5 6 CASE WHEN condition1 THEN result1 WHEN condition2 THEN result2 WHEN conditionN THEN resultN ELSE result END ; Example 1 2 3 4 5 6 7 SELECT CASE WHEN A + B > C AND A + C > B AND B + C > A THEN CASE WHEN A = B AND B = C THEN 'Equilateral' WHEN A = B OR B = C OR A = C THEN 'Isosceles' ELSE 'Scalene' END ELSE 'Not A Triangle' END FROM TRIANGLES ;","title":"Syntax CASE"},{"location":"Coding/SQL/Advance_Funtion_SQL.html#if_function","text":"The IF() function returns a value if a condition is TRUE, or another value if a condition is FALSE.","title":"IF() function"},{"location":"Coding/SQL/Advance_Funtion_SQL.html#syntax_if","text":"1 IF ( condition , value_if_true , value_if_false ) Example 1 SELECT IF ( 500 < 1000 , 5 , 10 ); 1 2 SELECT OrderID , Quantity , IF ( Quantity > 10 , \"MORE\" , \"LESS\" ) FROM OrderDetails ;","title":"Syntax IF()"},{"location":"Coding/SQL/Advance_Funtion_SQL.html#set_var_name_user-defined_variables","text":"You can store a value in a user-defined variable in one statement and refer to it later in another statement. This enables you to pass values from one statement to another. User variables are written as @var_name, where the variable name var_name consists of alphanumeric characters, ., _ , and $. User variable names are not case-sensitive. Names have a maximum length of 64 characters. One way to set a user-defined variable is by issuing a SET statement: 1 SET @ var_name = expr [, @ var_name = expr ] ... For SET , either = or := can be used as the assignment operator. When making an assignment in this way, you must use := as the assignment operator; = is treated as the comparison operator in statements other than SET . Here ans example of the usage of this \u201cuser-defined variables\u201d, this is the solution to the challenge Occupations of Hacker Ranks 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SET @ dRow = 0 , @ pRow = 0 , @ sRow = 0 , @ aRow = 0 ; SELECT MIN ( Doctor ), MIN ( Professor ), MIN ( Singer ), MIN ( Actor ) FROM ( SELECT CASE Occupation WHEN 'Doctor' THEN @ dRow : = @ dRow + 1 WHEN 'Professor' THEN @ pRow : = @ pRow + 1 WHEN 'Singer' THEN @ sRow : = @ sRow + 1 WHEN 'Actor' THEN @ aRow : = @ aRow + 1 END AS row , IF ( Occupation = 'Doctor' , Name , NULL ) AS Doctor , IF ( Occupation = 'Professor' , Name , NULL ) AS Professor , IF ( Occupation = 'Singer' , Name , NULL ) AS Singer , IF ( Occupation = 'Actor' , Name , NULL ) AS Actor FROM OCCUPATIONS ORDER BY Name ) a GROUP BY row ;","title":"SET @var_name User-defined Variables"},{"location":"Coding/SQL/Some_SQL.html","text":"Random notes about SQL ( mostly MySQL) \u00b6 How to get different (unique) results \u00b6 1 2 SELECT DISTINCT column - name FROM table - name How to count different (unique) results \u00b6 1 2 SELECT COUNT ( DISTINCT column - name ) FROM table - name How to display different (unique) in alphabetical order \u00b6 1 2 3 SELECT DISTINCT Country FROM Supplier ORDER BY COUNTRY How to find the difference between unique values and all values \u00b6 1 SELECT ( COUNT ( CITY ) - COUNT ( DISTINCT CITY )) FROM STATION ; some reference","title":"Random notes about SQL"},{"location":"Coding/SQL/Some_SQL.html#random_notes_about_sql_mostly_mysql","text":"","title":"Random notes about SQL ( mostly MySQL)"},{"location":"Coding/SQL/Some_SQL.html#how_to_get_different_unique_results","text":"1 2 SELECT DISTINCT column - name FROM table - name","title":"How to get different (unique) results"},{"location":"Coding/SQL/Some_SQL.html#how_to_count_different_unique_results","text":"1 2 SELECT COUNT ( DISTINCT column - name ) FROM table - name","title":"How to count different (unique) results"},{"location":"Coding/SQL/Some_SQL.html#how_to_display_different_unique_in_alphabetical_order","text":"1 2 3 SELECT DISTINCT Country FROM Supplier ORDER BY COUNTRY","title":"How to display different (unique) in alphabetical order"},{"location":"Coding/SQL/Some_SQL.html#how_to_find_the_difference_between_unique_values_and_all_values","text":"1 SELECT ( COUNT ( CITY ) - COUNT ( DISTINCT CITY )) FROM STATION ; some reference","title":"How to find the difference between unique values and all values"},{"location":"Coding/SQL/String_Functions_SQL.html","text":"How to find the length of a string \u00b6 In this case let say that we want to print the size or length of a string 1 SELECT CITY , LENGTH ( CITY ) FROM STATION ORDER BY LENGTH ( CITY ) LEFT() and RIGHT() functions useful to make substrigns \u00b6 The LEFT() function extracts a number of characters from a string (starting from left. Syntax LEFT() \u00b6 1 LEFT ( string , number_of_chars ) Example: Extract 5 characters from the text in the \u201cCustomerName\u201d column (starting from left): 1 2 SELECT LEFT ( CustomerName , 5 ) AS ExtractString FROM Customers ; now, The RIGHT() function extracts a number of characters from a string (starting from right). Syntax RIGHT() \u00b6 1 RIGHT ( string , number_of_chars ) Example Extract 5 characters from the text in the \u201cCustomerName\u201d column (starting from right): 1 2 SELECT RIGHT ( CustomerName , 5 ) AS ExtractString FROM Customers ; CONCAT() function \u00b6 The CONCAT() function adds two or more expressions together. Syntax CONCAT() \u00b6 1 CONCAT ( expression1 , expression2 , expression3 ,...) Example: 1 2 SELECT CONCAT ( Address , \" \" , PostalCode , \" \" , City ) AS Address FROM Customers ; CONCAT_WS() function \u00b6 The CONCAT_WS() function adds two or more expressions together with a separator. Syntax CONCAT_WS() \u00b6 1 CONCAT_WS ( separator , expression1 , expression2 , expression3 ,...) Example 1 2 SELECT CONCAT_WS ( \" \" , Address , PostalCode , City ) AS Address FROM Customers ;","title":"String Funtions SQL"},{"location":"Coding/SQL/String_Functions_SQL.html#how_to_find_the_length_of_a_string","text":"In this case let say that we want to print the size or length of a string 1 SELECT CITY , LENGTH ( CITY ) FROM STATION ORDER BY LENGTH ( CITY )","title":"How to find the length of a string"},{"location":"Coding/SQL/String_Functions_SQL.html#left_and_right_functions_useful_to_make_substrigns","text":"The LEFT() function extracts a number of characters from a string (starting from left.","title":"LEFT() and RIGHT() functions useful to make substrigns"},{"location":"Coding/SQL/String_Functions_SQL.html#syntax_left","text":"1 LEFT ( string , number_of_chars ) Example: Extract 5 characters from the text in the \u201cCustomerName\u201d column (starting from left): 1 2 SELECT LEFT ( CustomerName , 5 ) AS ExtractString FROM Customers ; now, The RIGHT() function extracts a number of characters from a string (starting from right).","title":"Syntax LEFT()"},{"location":"Coding/SQL/String_Functions_SQL.html#syntax_right","text":"1 RIGHT ( string , number_of_chars ) Example Extract 5 characters from the text in the \u201cCustomerName\u201d column (starting from right): 1 2 SELECT RIGHT ( CustomerName , 5 ) AS ExtractString FROM Customers ;","title":"Syntax RIGHT()"},{"location":"Coding/SQL/String_Functions_SQL.html#concat_function","text":"The CONCAT() function adds two or more expressions together.","title":"CONCAT() function"},{"location":"Coding/SQL/String_Functions_SQL.html#syntax_concat","text":"1 CONCAT ( expression1 , expression2 , expression3 ,...) Example: 1 2 SELECT CONCAT ( Address , \" \" , PostalCode , \" \" , City ) AS Address FROM Customers ;","title":"Syntax CONCAT()"},{"location":"Coding/SQL/String_Functions_SQL.html#concat_ws_function","text":"The CONCAT_WS() function adds two or more expressions together with a separator.","title":"CONCAT_WS() function"},{"location":"Coding/SQL/String_Functions_SQL.html#syntax_concat_ws","text":"1 CONCAT_WS ( separator , expression1 , expression2 , expression3 ,...) Example 1 2 SELECT CONCAT_WS ( \" \" , Address , PostalCode , City ) AS Address FROM Customers ;","title":"Syntax CONCAT_WS()"},{"location":"Courses/Coursera/AI for Everyone/AI_for_Everyone.html","text":"Starting an AI projects \u00b6 Workflow of a project. Select AI project (Framework). Organizing data and teams for the project. Workflow of a Machine Learning project \u00b6 Collect data. Train the model. ( Iterate many times until achieve the result. ) Deploy model. ( Get back maintain and update. ) Workflow of a Data Science project. \u00b6 Unlike a machine learning project, the output of a data science project is often a set of actionable insights, a set of insights that may cause you to do things differently. Collect Data. Analyze Data. ( Iterate many times to get good insights ) Suggest Hypotheses/actions ( Deploy changes, re-analyze new data ) Build a AI company \u00b6 Case Study: Smart speaker To get a better idea about what is need it to build a AI company it is important to get and idea of what is need it in an AI system. Steps to process a command 1. Trigger word/wakeword detection (\u201cHello Device\u201d). 2. Speech recognition. 3. Intent recognition. 4. Execute joke. Case study: Self-driving car the key steps: 1. Car detection. 2. Pedestrian detection. 3. Motion Planning. Example roles of a AI team This roles might have other titles but the task that they perform will be similar: Software Engineer: those who develop the business logic, like the joke execution or those to ensure self-driving reliability. Machine Learning Engineer: those that create the model, that take data A and produce result B. Machine Learning Researcher: extend state-of the art in ML. Note: the last two roles can be, in some cases, just one and it is called \u201cApplied ML Scientist\u201d Data Scientist: Examine data and provide insights, make presentation to team and executives. Data Engineer: Organize data, make sure data is saved in a easily accessible, secure and cost effective way. AI Product Manager: Help decide what to build; what\u2019s feasible and valuable. AI transformation Playbook \u00b6 to create or move towards AI the company might need to restructure itself. 1. Execute pilot projects to gain momentum. * More important for the initial project to succeed rather than be the most valuable. * Show traction within 6-12 months. * Can be in-house or outsourced. 2. Build an in-house AI team It is better have a dedicated unit to work in this projects that a Business unit aside. 3. Provide broad AI Training Role What They Should learn Executives and senior business leaders 1. What AI can do for your enterprise. 2. AI strategy. 3. Resource allocation. Leaders of devisions working on AI projects 1. Set project direction (technical and business diligence) 2. Resource allocation. 3.Monitor Progress AI engineer trainees 1. Build and ship AI software. 2.Gather data. 3. Execute on specific AI projects 4 Develop an AI strategy Leverage AI to create an advantage specific to your industry sector. Design strategy aligned with the \u201cVirtuous Cycle of AI\u201d. Consider Creating a data strategy ( strategic data acquisition, unified data warehouse). 5. Develop internal and external communications Investor relations. Government relations. Consumer/user education. Talent/recruitment. Internal Communications. Some application of AI \u00b6 watch the video here Video about AI application","title":"AI for Everyone"},{"location":"Courses/Coursera/AI for Everyone/AI_for_Everyone.html#starting_an_ai_projects","text":"Workflow of a project. Select AI project (Framework). Organizing data and teams for the project.","title":"Starting an AI projects"},{"location":"Courses/Coursera/AI for Everyone/AI_for_Everyone.html#workflow_of_a_machine_learning_project","text":"Collect data. Train the model. ( Iterate many times until achieve the result. ) Deploy model. ( Get back maintain and update. )","title":"Workflow of a Machine Learning project"},{"location":"Courses/Coursera/AI for Everyone/AI_for_Everyone.html#workflow_of_a_data_science_project","text":"Unlike a machine learning project, the output of a data science project is often a set of actionable insights, a set of insights that may cause you to do things differently. Collect Data. Analyze Data. ( Iterate many times to get good insights ) Suggest Hypotheses/actions ( Deploy changes, re-analyze new data )","title":"Workflow of a Data Science project."},{"location":"Courses/Coursera/AI for Everyone/AI_for_Everyone.html#build_a_ai_company","text":"Case Study: Smart speaker To get a better idea about what is need it to build a AI company it is important to get and idea of what is need it in an AI system. Steps to process a command 1. Trigger word/wakeword detection (\u201cHello Device\u201d). 2. Speech recognition. 3. Intent recognition. 4. Execute joke. Case study: Self-driving car the key steps: 1. Car detection. 2. Pedestrian detection. 3. Motion Planning. Example roles of a AI team This roles might have other titles but the task that they perform will be similar: Software Engineer: those who develop the business logic, like the joke execution or those to ensure self-driving reliability. Machine Learning Engineer: those that create the model, that take data A and produce result B. Machine Learning Researcher: extend state-of the art in ML. Note: the last two roles can be, in some cases, just one and it is called \u201cApplied ML Scientist\u201d Data Scientist: Examine data and provide insights, make presentation to team and executives. Data Engineer: Organize data, make sure data is saved in a easily accessible, secure and cost effective way. AI Product Manager: Help decide what to build; what\u2019s feasible and valuable.","title":"Build a AI company"},{"location":"Courses/Coursera/AI for Everyone/AI_for_Everyone.html#ai_transformation_playbook","text":"to create or move towards AI the company might need to restructure itself. 1. Execute pilot projects to gain momentum. * More important for the initial project to succeed rather than be the most valuable. * Show traction within 6-12 months. * Can be in-house or outsourced. 2. Build an in-house AI team It is better have a dedicated unit to work in this projects that a Business unit aside. 3. Provide broad AI Training Role What They Should learn Executives and senior business leaders 1. What AI can do for your enterprise. 2. AI strategy. 3. Resource allocation. Leaders of devisions working on AI projects 1. Set project direction (technical and business diligence) 2. Resource allocation. 3.Monitor Progress AI engineer trainees 1. Build and ship AI software. 2.Gather data. 3. Execute on specific AI projects 4 Develop an AI strategy Leverage AI to create an advantage specific to your industry sector. Design strategy aligned with the \u201cVirtuous Cycle of AI\u201d. Consider Creating a data strategy ( strategic data acquisition, unified data warehouse). 5. Develop internal and external communications Investor relations. Government relations. Consumer/user education. Talent/recruitment. Internal Communications.","title":"AI transformation Playbook"},{"location":"Courses/Coursera/AI for Everyone/AI_for_Everyone.html#some_application_of_ai","text":"watch the video here Video about AI application","title":"Some application of AI"},{"location":"Courses/Coursera/Introduction tensorflow/Convolutional_Neural_network_Overview.html","text":"Convolutional Neural network Overview \u00b6 in this additional note we describe some concept mentioned about but in this case from the theory perspective, we make mentione of an architecture called LeNeT-5 which is a convolutional network designed for handwritten and machine-printed character recognition. From this architecture, we need to consider that we are not flatten the image in any way in order to maintain the spatial relationship between the pixels ( if we flatten we will lose that spatial information). Now for the input we need to think in different ways: in terms of 3D Volumes this means that the image has a depth associated with it. This is called the number of channels . For example, a color image or a RGB image of M * N M * N will have 3 channels ( one r, B, and G) so the full shape will be $ M * N * 3$ in contrast to a gray scale image that will be M * N * 1 M * N * 1 , this input are called image volumes. In this architecture we have three different kinds of layers: Convolutional Layer, pooling Layer, and fully-connected Layers. Convolution Layer \u00b6 This is the most important layer in CNNs: it gives the CNN its name, The convolution Layer, is where the feature learning happens, the idea is that we have a number of filters or kernels . These filters are just small patches that represent some kind of visual feature, \u201cweights\u201d and \u201cbiases\u201d of the CNN. We take each filter and convolve it over the input volume to get a single activation map , so in other words, we convolve a filter with an input volume to get back a activation map that tells us \u201cHow well\u201d parts of the input \u201crespond\u201d to the filter. Like in the example mentioned before about the horizontal lines and vertical lines, in the case of the horizontal line, we use a horizontal line filter, that will generate a activation map that indicates where the horizontal lines are in the input. The best part of CNN is that this filter are not hard-coded, they are learned, that means that we don\u2019t need to explicitly tell the CNN to look for horizontal lines, it will do all it by itself during the backprop. In this Convolution Layer (or CONV Layer), we need to specify at least the number of filters and their size (width and height). some additional parameters will be padding and stride (not cover here), in terms of input and outputs, suppose a CONV layer receives an input of size W_{in} * H_{in} * C_{in} W_{in} * H_{in} * C_{in} (assuming zero padding and stride 1), the output width and height of the output activation maps will be W_{out} = W_{in} - F_{w} + 1 W_{out} = W_{in} - F_{w} + 1 and H_{out} = H_{in} - F_{h} + 1 H_{out} = H_{in} - F_{h} + 1 where F_w F_w and F_h F_h are the width and height of the filters then the output will be $$ W_{out} * H_{out} * F $$ so for a an image volume of 32x32x1 with a filter size 5x5 and 6 of depth we will have an output action map of 28x28x6 Immediately following the CONV layer, we apply a non-linearity to each value in each activation map over the entire volume. The Rectified Linear Unit or ReLU is most freaquent used for CNNs f_{(x)} = max(0,x) f_{(x)} = max(0,x) it is zero for x<=0 x<=0 and x>0 x>0 , this is the activation use most frequently and that work well with this kind of scenarios. Pooling Layer \u00b6 This layer is primarily used to help reduce the computational complexity and extract prominent features, the pooling Layer (POOL) has no weights/parameters, unlike CONV layers. The result is smaller activation volume along the width and height. the depth of the input is still maintained, so if 12 activation maps go to the POOL layer, the output will also have 12 activation maps. For the POOL layer, we have to define the pool size, which tells us by how much we will reduce the width and height of the activation volume, if we want to halve the activation volume in width and height, we would choose a pool size of 2x2, if we wanted to reduce it by more, we should choose a larger pool size. The computation we do depends on the type of pooling: average or max. For max pooling, inside of the window, we just choose the maximum value in that window. This intuitively corresponds to choosing the most prominent features. For average pooling, we take the average of the values in the window. This produces smoother results than max pooling. In practice, max pooling is used more frequently than average pooling, and the most common pooling size is 2\u00d72. Fully-Connected Layer \u00b6 This layer is the common artificial neural network, the catch, we have a activation volume as output of the CONV and POOL Layers, and this layer accept just a vector, so we will need to flatten this volume. After flattening the volume, we can treat this layer just like a neural network! It is okay to flatten here since we\u2019ve already passed through all of the CONV layers and applied the filters. Reference","title":"Convolutional Neural network Overview"},{"location":"Courses/Coursera/Introduction tensorflow/Convolutional_Neural_network_Overview.html#convolutional_neural_network_overview","text":"in this additional note we describe some concept mentioned about but in this case from the theory perspective, we make mentione of an architecture called LeNeT-5 which is a convolutional network designed for handwritten and machine-printed character recognition. From this architecture, we need to consider that we are not flatten the image in any way in order to maintain the spatial relationship between the pixels ( if we flatten we will lose that spatial information). Now for the input we need to think in different ways: in terms of 3D Volumes this means that the image has a depth associated with it. This is called the number of channels . For example, a color image or a RGB image of M * N M * N will have 3 channels ( one r, B, and G) so the full shape will be $ M * N * 3$ in contrast to a gray scale image that will be M * N * 1 M * N * 1 , this input are called image volumes. In this architecture we have three different kinds of layers: Convolutional Layer, pooling Layer, and fully-connected Layers.","title":"Convolutional Neural network Overview"},{"location":"Courses/Coursera/Introduction tensorflow/Convolutional_Neural_network_Overview.html#convolution_layer","text":"This is the most important layer in CNNs: it gives the CNN its name, The convolution Layer, is where the feature learning happens, the idea is that we have a number of filters or kernels . These filters are just small patches that represent some kind of visual feature, \u201cweights\u201d and \u201cbiases\u201d of the CNN. We take each filter and convolve it over the input volume to get a single activation map , so in other words, we convolve a filter with an input volume to get back a activation map that tells us \u201cHow well\u201d parts of the input \u201crespond\u201d to the filter. Like in the example mentioned before about the horizontal lines and vertical lines, in the case of the horizontal line, we use a horizontal line filter, that will generate a activation map that indicates where the horizontal lines are in the input. The best part of CNN is that this filter are not hard-coded, they are learned, that means that we don\u2019t need to explicitly tell the CNN to look for horizontal lines, it will do all it by itself during the backprop. In this Convolution Layer (or CONV Layer), we need to specify at least the number of filters and their size (width and height). some additional parameters will be padding and stride (not cover here), in terms of input and outputs, suppose a CONV layer receives an input of size W_{in} * H_{in} * C_{in} W_{in} * H_{in} * C_{in} (assuming zero padding and stride 1), the output width and height of the output activation maps will be W_{out} = W_{in} - F_{w} + 1 W_{out} = W_{in} - F_{w} + 1 and H_{out} = H_{in} - F_{h} + 1 H_{out} = H_{in} - F_{h} + 1 where F_w F_w and F_h F_h are the width and height of the filters then the output will be $$ W_{out} * H_{out} * F $$ so for a an image volume of 32x32x1 with a filter size 5x5 and 6 of depth we will have an output action map of 28x28x6 Immediately following the CONV layer, we apply a non-linearity to each value in each activation map over the entire volume. The Rectified Linear Unit or ReLU is most freaquent used for CNNs f_{(x)} = max(0,x) f_{(x)} = max(0,x) it is zero for x<=0 x<=0 and x>0 x>0 , this is the activation use most frequently and that work well with this kind of scenarios.","title":"Convolution Layer"},{"location":"Courses/Coursera/Introduction tensorflow/Convolutional_Neural_network_Overview.html#pooling_layer","text":"This layer is primarily used to help reduce the computational complexity and extract prominent features, the pooling Layer (POOL) has no weights/parameters, unlike CONV layers. The result is smaller activation volume along the width and height. the depth of the input is still maintained, so if 12 activation maps go to the POOL layer, the output will also have 12 activation maps. For the POOL layer, we have to define the pool size, which tells us by how much we will reduce the width and height of the activation volume, if we want to halve the activation volume in width and height, we would choose a pool size of 2x2, if we wanted to reduce it by more, we should choose a larger pool size. The computation we do depends on the type of pooling: average or max. For max pooling, inside of the window, we just choose the maximum value in that window. This intuitively corresponds to choosing the most prominent features. For average pooling, we take the average of the values in the window. This produces smoother results than max pooling. In practice, max pooling is used more frequently than average pooling, and the most common pooling size is 2\u00d72.","title":"Pooling Layer"},{"location":"Courses/Coursera/Introduction tensorflow/Convolutional_Neural_network_Overview.html#fully-connected_layer","text":"This layer is the common artificial neural network, the catch, we have a activation volume as output of the CONV and POOL Layers, and this layer accept just a vector, so we will need to flatten this volume. After flattening the volume, we can treat this layer just like a neural network! It is okay to flatten here since we\u2019ve already passed through all of the CONV layers and applied the filters. Reference","title":"Fully-Connected Layer"},{"location":"Courses/Coursera/Introduction tensorflow/Enhancing_vision_with_convolutional_Neural_Networks.html","text":"What are convolutions and pooling? \u00b6 One thing, we can see from the previous exercise is that there is a lot of wasted space in each image. while there are only 784 pixels, it will be interesting if there is a way to condense this image to those important features that make a bag, a shoe or a bag, that\u2019s is where convolutions come in. What is convolution? \u00b6 To use an analogy, in image processing normally involve having a filter and passing that filter over the image in order to change the underlying image. The convolution will work in a similar way. The process will be a little bit like this: For every pixel, take its value, and take a look at the value of its neighbors. let say 192 in the image above. 1 2 3 4 5 | 0 | 64 | 128 | |--------|----------|--------| | 48 | 192 | 144 | |--------|----------|--------| | 142 | 226 | 168 | If the filter is 3x3 3x3 , then we can take a look at the immediate neighbor, so we will have a corresponding 3x3 3x3 grid. 1 2 3 4 5 | -1 | 0 | -2 | |--------|----------|--------| | 0.5 | 4.5 | -1.5 | |--------|----------|--------| | 1.5 | 2 | -3 | Now we can get the new value for the pixel, we simply multiply each neighbor by the corresponding value in the filter. Current pixel = 192 Current pixel = 192 New pixel = (-1 * 0)+(0 * 64)+(-2 * 128)+(0.5 * 48)+(4.5 * 192)+(-1.5 * 144)+(1.5 * 142)+(2 * 226)+(-3 * 168) New pixel = (-1 * 0)+(0 * 64)+(-2 * 128)+(0.5 * 48)+(4.5 * 192)+(-1.5 * 144)+(1.5 * 142)+(2 * 226)+(-3 * 168) we have the new pixel with the sum of each of the neighbor values multiplied by the corresponding filter value, and that\u2019s a convolution. The idea here is that some convolutions will change the image in such a way that certain features in the image get emphasized. So, for example, if you look at this filter. Then the vertical lines in the image really pop out. Now with this filter, the horizontal lines pop out. When convolution is combine with something call pooling they will become really powerful, a quick and easy way to do this, is to go over the image of four pixels at a time, of these four, pick the biggest value and keep just that. So, for example: So 16 pixels on the left are turned into the four pixels on the right, by looking at them in two-by-two grids and picking the biggest value. This will preserve the features that were highlighted by the convolution, while simultaneously quartering the size of the image. We have the horizontal and vertical axes. Class Conv2d \u00b6 This is the class that we are going to use to make the convolution, for more detailed information we can check the TesorFLow documentation about Conv2d . This layer creates a convolution kernel, which in or example was a 3x3, that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures or input_shape(128,128,1) for one gray scale 128x128 image. Syntax Convolution layer \u00b6 1 tf . keras . layers . Conv2D ( 64 ,( 3 , 3 ), activation + 'relu' , input_shape ( 28 , 28 , 1 )) This will be if this is the first layer of the model, we will need to add input_shape(28,28,1) because the image is a gray scale 28x28. for a layer that is not the first one it will be: 1 tf . keras . layers . Conv2D ( 64 ,( 3 , 3 ), activation = 'relu' ) Class MaxPooling2D \u00b6 Max pooling layer for 2D inputs (example an image) more information in the tensorFlow documentation about MaxPooling2D Syntax MaxPooling layer \u00b6 1 tf . keras . layers . MaxPooling2D ( 2 , 2 ) in this case we create a grid of 2x2. Example of a model with Convolutional and Maxpooling \u00b6 1 2 3 4 5 6 7 8 9 model = tf . keras . models . Sequencial ([ tf . keras . layers . Conv2D ( 64 ,( 3 , 3 ), activation = 'relu' , input_shape = ( 28 , 28 , 1 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Conv2D ( 64 ,( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) so in the first convolution layer we are asking keras to generate 64 filters for us, these filter are 3x3, their activation is relu , which mean that negative value will be throw away, finally the input shape is as before, the 28 by 28. that extra 1 just means that we have just 1 color depth. the second layer, it is the pooling layer, it is max-pooling because we\u2019re going to take the maximum value. so in the script we have a image that have being pass for 2 convolutional layers and 2 max-pooling, that means, that the image has been quarter and quarter again, so when we arrived to the flatten layer we have a greatly simplify image. now, we can use a good method call summary() like this 1 model . summary () this allow use to inspect the layers of the model and see the journey of the image through the convolution. the table is showing the layer and some details about them, including the output, one of the most important columns is the output shape, one things that we will notice is: the output shape isn\u2019t 28x28 instead 26x26, remember the filter is 3x3, so if we are trying to scan a picture we won\u2019t be able to scan form the top left corner, because it doesn\u2019t have any neighbors. we will need to start from one pixel down and one pixel to the right this means we cant use a one pixel margin all around the image, so the output of the convolution will be 2 pixel smaller in x and 2 pixel smaller in y , if we use a filter 5x5 the output will be smaller, but in a filter 3x3 the output shape of an input of 28x28 will be 26x26. The next, is the first max-pooling layer. We specified it to be two-by-two , thus turning four pixels into one, so now the output get reduced from 26 by 26, to 13 by 13. The next convolution will operate in this, losing one margin as before, and we are down to 11 by 11, add another two-by-two max-pooling layers, rounding down, and we when down to a image of 5 by 5 instead of 28 by 28. The Number of convolutions per image, in this case 64 (filters) and size five-by-five pixels are fed in to the Flatten, that will output 25 pixels times 64, which is 1600. Then the flattened layer will get 1,600 elements in it, as opposed to the 784 that you had previously. This number is impacted by the parameters that you set when defining the convolutional 2D layers.","title":"Enhancing vision with convolutional Neural Networks"},{"location":"Courses/Coursera/Introduction tensorflow/Enhancing_vision_with_convolutional_Neural_Networks.html#what_are_convolutions_and_pooling","text":"One thing, we can see from the previous exercise is that there is a lot of wasted space in each image. while there are only 784 pixels, it will be interesting if there is a way to condense this image to those important features that make a bag, a shoe or a bag, that\u2019s is where convolutions come in.","title":"What are convolutions and pooling?"},{"location":"Courses/Coursera/Introduction tensorflow/Enhancing_vision_with_convolutional_Neural_Networks.html#what_is_convolution","text":"To use an analogy, in image processing normally involve having a filter and passing that filter over the image in order to change the underlying image. The convolution will work in a similar way. The process will be a little bit like this: For every pixel, take its value, and take a look at the value of its neighbors. let say 192 in the image above. 1 2 3 4 5 | 0 | 64 | 128 | |--------|----------|--------| | 48 | 192 | 144 | |--------|----------|--------| | 142 | 226 | 168 | If the filter is 3x3 3x3 , then we can take a look at the immediate neighbor, so we will have a corresponding 3x3 3x3 grid. 1 2 3 4 5 | -1 | 0 | -2 | |--------|----------|--------| | 0.5 | 4.5 | -1.5 | |--------|----------|--------| | 1.5 | 2 | -3 | Now we can get the new value for the pixel, we simply multiply each neighbor by the corresponding value in the filter. Current pixel = 192 Current pixel = 192 New pixel = (-1 * 0)+(0 * 64)+(-2 * 128)+(0.5 * 48)+(4.5 * 192)+(-1.5 * 144)+(1.5 * 142)+(2 * 226)+(-3 * 168) New pixel = (-1 * 0)+(0 * 64)+(-2 * 128)+(0.5 * 48)+(4.5 * 192)+(-1.5 * 144)+(1.5 * 142)+(2 * 226)+(-3 * 168) we have the new pixel with the sum of each of the neighbor values multiplied by the corresponding filter value, and that\u2019s a convolution. The idea here is that some convolutions will change the image in such a way that certain features in the image get emphasized. So, for example, if you look at this filter. Then the vertical lines in the image really pop out. Now with this filter, the horizontal lines pop out. When convolution is combine with something call pooling they will become really powerful, a quick and easy way to do this, is to go over the image of four pixels at a time, of these four, pick the biggest value and keep just that. So, for example: So 16 pixels on the left are turned into the four pixels on the right, by looking at them in two-by-two grids and picking the biggest value. This will preserve the features that were highlighted by the convolution, while simultaneously quartering the size of the image. We have the horizontal and vertical axes.","title":"What is convolution?"},{"location":"Courses/Coursera/Introduction tensorflow/Enhancing_vision_with_convolutional_Neural_Networks.html#class_conv2d","text":"This is the class that we are going to use to make the convolution, for more detailed information we can check the TesorFLow documentation about Conv2d . This layer creates a convolution kernel, which in or example was a 3x3, that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None , it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures or input_shape(128,128,1) for one gray scale 128x128 image.","title":"Class Conv2d"},{"location":"Courses/Coursera/Introduction tensorflow/Enhancing_vision_with_convolutional_Neural_Networks.html#syntax_convolution_layer","text":"1 tf . keras . layers . Conv2D ( 64 ,( 3 , 3 ), activation + 'relu' , input_shape ( 28 , 28 , 1 )) This will be if this is the first layer of the model, we will need to add input_shape(28,28,1) because the image is a gray scale 28x28. for a layer that is not the first one it will be: 1 tf . keras . layers . Conv2D ( 64 ,( 3 , 3 ), activation = 'relu' )","title":"Syntax Convolution layer"},{"location":"Courses/Coursera/Introduction tensorflow/Enhancing_vision_with_convolutional_Neural_Networks.html#class_maxpooling2d","text":"Max pooling layer for 2D inputs (example an image) more information in the tensorFlow documentation about MaxPooling2D","title":"Class MaxPooling2D"},{"location":"Courses/Coursera/Introduction tensorflow/Enhancing_vision_with_convolutional_Neural_Networks.html#syntax_maxpooling_layer","text":"1 tf . keras . layers . MaxPooling2D ( 2 , 2 ) in this case we create a grid of 2x2.","title":"Syntax MaxPooling layer"},{"location":"Courses/Coursera/Introduction tensorflow/Enhancing_vision_with_convolutional_Neural_Networks.html#example_of_a_model_with_convolutional_and_maxpooling","text":"1 2 3 4 5 6 7 8 9 model = tf . keras . models . Sequencial ([ tf . keras . layers . Conv2D ( 64 ,( 3 , 3 ), activation = 'relu' , input_shape = ( 28 , 28 , 1 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Conv2D ( 64 ,( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) so in the first convolution layer we are asking keras to generate 64 filters for us, these filter are 3x3, their activation is relu , which mean that negative value will be throw away, finally the input shape is as before, the 28 by 28. that extra 1 just means that we have just 1 color depth. the second layer, it is the pooling layer, it is max-pooling because we\u2019re going to take the maximum value. so in the script we have a image that have being pass for 2 convolutional layers and 2 max-pooling, that means, that the image has been quarter and quarter again, so when we arrived to the flatten layer we have a greatly simplify image. now, we can use a good method call summary() like this 1 model . summary () this allow use to inspect the layers of the model and see the journey of the image through the convolution. the table is showing the layer and some details about them, including the output, one of the most important columns is the output shape, one things that we will notice is: the output shape isn\u2019t 28x28 instead 26x26, remember the filter is 3x3, so if we are trying to scan a picture we won\u2019t be able to scan form the top left corner, because it doesn\u2019t have any neighbors. we will need to start from one pixel down and one pixel to the right this means we cant use a one pixel margin all around the image, so the output of the convolution will be 2 pixel smaller in x and 2 pixel smaller in y , if we use a filter 5x5 the output will be smaller, but in a filter 3x3 the output shape of an input of 28x28 will be 26x26. The next, is the first max-pooling layer. We specified it to be two-by-two , thus turning four pixels into one, so now the output get reduced from 26 by 26, to 13 by 13. The next convolution will operate in this, losing one margin as before, and we are down to 11 by 11, add another two-by-two max-pooling layers, rounding down, and we when down to a image of 5 by 5 instead of 28 by 28. The Number of convolutions per image, in this case 64 (filters) and size five-by-five pixels are fed in to the Flatten, that will output 25 pixels times 64, which is 1600. Then the flattened layer will get 1,600 elements in it, as opposed to the 784 that you had previously. This number is impacted by the parameters that you set when defining the convolutional 2D layers.","title":"Example of a model with Convolutional and Maxpooling"},{"location":"Courses/Coursera/Introduction tensorflow/Introduction to computer vision.html","text":"Introduction to computer vision \u00b6 so in the previous notes we use the numpy array as a way to provide the arrays for training the model, although in a real scenario, hard coding the data won\u2019t be possible, in the following example we are going to use a data set call fashion-mnist and since this is a data set with 60.000 example for training and 10.000 examples for testing we will need to load the data in a different way. Importing fashion-mnist using keras \u00b6 Fortunately, it\u2019s still quite simple because Fashion-MNIST is available as a data set with an API call in TensorFlow. We simply declare an object of type MNIST loading it from the Keras database. 1 2 fashion_mnist = keras . dataset . fashion_mnist ( train_images , train_label ), ( test_images , test_labels ) = fashion_mnist . load_data () We simply declare an object of type MNIST loading it from the Keras database. On this object, if we call the load data method, it will return four lists to us. That\u2019s the training data, the training labels, the testing data, and the testing labels. Input Shape \u00b6 Here you saw how the data can be loaded into Python data structures that make it easy to train a neural network. The image is represented as a 28x28 array of greyscales, and how its label is a number. Using a number is a first step in avoiding bias \u2013 instead of labelling it with words in a specific language and excluding people who don\u2019t speak that language! Labels \u00b6 Each training and test example is assigned to one of the following labels: Label Description 0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot About the layers of the model \u00b6 we will look at the code for the neural network definition. Previously, we have just one layer, now we have three layers, it is important to look at are the first and the last layers. 1 2 3 4 model = keras . Sequential ([ keras . layers . Flatten ( input_shape = ( 28 , 28 )), keras . layers . Dense ( 128 , activation = tf . nn . relu ), keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) The last layer has 10 neurons in it because we have ten classes of clothing in the dataset. They should always match. The first layer is a flatten layer with the input shaping 28 by 28, this is because the images are 28X28, so we\u2019re specifying that this is the shape that we should expect the data to be in. Flatten takes this 28 by 28 square and turns it into a simple linear array. The interesting stuff happens in the middle layer, sometimes also called a hidden layer. This is a 128 neurons in it, we can think these neurons as variables in a function. Maybe call them x1, x2 x3, etc. if you then say the function was y equals w1 times x1 , plus w2 times x2 , plus w3 times x3 , all the way up to a w128 times x128 ( y = w_1x_1 +w_2x_2+w_3x_3+...+w_{128}x_{128} y = w_1x_1 +w_2x_2+w_3x_3+...+w_{128}x_{128} ). By figuring out the values of w , then y will be 9 , which is the category of the shoe. We can check the exercise in the colab A Computer Vision Example An Example of a script \u00b6 A full script will look like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import tensorflow as tfprint ( tf . __version__ ) #The Fashion MNIST data is available directly in the tf.keras datasets API. You load it like this: mnist = tf . keras . datasets . fashion_mnist ( traning_images . training_labels ),( test_images , test_labels ) = mnist . load_data () #You'll notice that all of the values in the number are between 0 and 255. If we are training a neural network, for various reasons it's easier if we treat all values as between 0 and 1, a process called 'normalizing'...and fortunately in Python it's easy to normalize a list like this without looping. You do it like this: training_images = training_images / 255.0 test_images = test_images / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 128 , activation = tf . nn . relu ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) model . compile ( optimizer = tf . train . AdamOptimizer (), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , epochs = 5 ) # now let see how good is the model model . evaluate ( test_images , test_labels ) Explanation of some Keywords \u00b6 Sequential: That defines a SEQUENCE of layers in the neural network Flatten: Remember earlier where our images were a square, when you printed them out? Flatten just takes that square and turns it into a 1 dimensional set. Dense: Adds a layer of neurons Each layer of neurons need an activation function to tell them what to do. There\u2019s lots of options, but just use these for now. Relu effectively means \u201cIf X>0 return X, else return 0\u201d \u2013 so what it does it it only passes values 0 or greater to the next layer in the network. Softmax takes a set of values, and effectively picks the biggest one, so, for example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], it saves you from fishing through it looking for the biggest value, and turns it into [0,0,0,0,1,0,0,0,0] \u2013 The goal is to save a lot of coding! Callback to stop the training \u00b6 first we will need to create the class myCallback 1 2 3 4 5 6 class myCallback ( tf . keras . callbacks . Callback ): \"\"\"docstring for myCallback\"\"\" def on_epoch_end ( self , epoch , logs = {}): if ( logs . get ( 'acc' ) > 0.6 ): print ( \" \\n Reached 60% accuracy so cancelling training!\" ) self . model . stop_training = True Instantiate the myCallback class 1 callbacks = myCallbacks () now we can make changes in the fit function to add the callback 1 model . fit ( x_training , y_training , epochs = 1 - , callbacks = [ callbacks ]) so the completed script will be: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import tensorflow as tf class myCallback ( tf . keras . callbacks . Callback ): def on_epoch_end ( self , epoch , logs = {}): if ( logs . get ( 'acc' ) > 0.6 ): print ( \" \\n Reached 60% accuracy so cancelling training!\" ) self . model . stop_training = True mnist = tf . keras . datasets . fashion_mnist ( x_train , y_train ),( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 callbacks = myCallback () model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( x_train , y_train , epochs = 10 , callbacks = [ callbacks ]) Now here ans example with the other data set, the original MNIST that contain handwritten numbers from 0 to 9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import tensorflow as tf mnist = tf . keras . datasets . mnist ( x_train , y_train ),( x_test , y_test ) = mnist . load_data () class myCallback ( tf . keras . callbacks . Callback ): def on_epoch_end ( self , epoch , logs = {}): if ( logs . get ( 'acc' ) > 0.99 ): print ( ' \\n Reached 99% accuracy so cancelling training!' ) self . model . stop_training = True callbacks = myCallback () x_train , x_test = x_train / 255.0 , x_test / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( x_train , y_train , epochs = 10 , callbacks = [ callbacks ])","title":"Introduction to Computer vision"},{"location":"Courses/Coursera/Introduction tensorflow/Introduction to computer vision.html#introduction_to_computer_vision","text":"so in the previous notes we use the numpy array as a way to provide the arrays for training the model, although in a real scenario, hard coding the data won\u2019t be possible, in the following example we are going to use a data set call fashion-mnist and since this is a data set with 60.000 example for training and 10.000 examples for testing we will need to load the data in a different way.","title":"Introduction to computer vision"},{"location":"Courses/Coursera/Introduction tensorflow/Introduction to computer vision.html#importing_fashion-mnist_using_keras","text":"Fortunately, it\u2019s still quite simple because Fashion-MNIST is available as a data set with an API call in TensorFlow. We simply declare an object of type MNIST loading it from the Keras database. 1 2 fashion_mnist = keras . dataset . fashion_mnist ( train_images , train_label ), ( test_images , test_labels ) = fashion_mnist . load_data () We simply declare an object of type MNIST loading it from the Keras database. On this object, if we call the load data method, it will return four lists to us. That\u2019s the training data, the training labels, the testing data, and the testing labels.","title":"Importing fashion-mnist using keras"},{"location":"Courses/Coursera/Introduction tensorflow/Introduction to computer vision.html#input_shape","text":"Here you saw how the data can be loaded into Python data structures that make it easy to train a neural network. The image is represented as a 28x28 array of greyscales, and how its label is a number. Using a number is a first step in avoiding bias \u2013 instead of labelling it with words in a specific language and excluding people who don\u2019t speak that language!","title":"Input Shape"},{"location":"Courses/Coursera/Introduction tensorflow/Introduction to computer vision.html#labels","text":"Each training and test example is assigned to one of the following labels: Label Description 0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot","title":"Labels"},{"location":"Courses/Coursera/Introduction tensorflow/Introduction to computer vision.html#about_the_layers_of_the_model","text":"we will look at the code for the neural network definition. Previously, we have just one layer, now we have three layers, it is important to look at are the first and the last layers. 1 2 3 4 model = keras . Sequential ([ keras . layers . Flatten ( input_shape = ( 28 , 28 )), keras . layers . Dense ( 128 , activation = tf . nn . relu ), keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) The last layer has 10 neurons in it because we have ten classes of clothing in the dataset. They should always match. The first layer is a flatten layer with the input shaping 28 by 28, this is because the images are 28X28, so we\u2019re specifying that this is the shape that we should expect the data to be in. Flatten takes this 28 by 28 square and turns it into a simple linear array. The interesting stuff happens in the middle layer, sometimes also called a hidden layer. This is a 128 neurons in it, we can think these neurons as variables in a function. Maybe call them x1, x2 x3, etc. if you then say the function was y equals w1 times x1 , plus w2 times x2 , plus w3 times x3 , all the way up to a w128 times x128 ( y = w_1x_1 +w_2x_2+w_3x_3+...+w_{128}x_{128} y = w_1x_1 +w_2x_2+w_3x_3+...+w_{128}x_{128} ). By figuring out the values of w , then y will be 9 , which is the category of the shoe. We can check the exercise in the colab A Computer Vision Example","title":"About the layers of the model"},{"location":"Courses/Coursera/Introduction tensorflow/Introduction to computer vision.html#an_example_of_a_script","text":"A full script will look like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import tensorflow as tfprint ( tf . __version__ ) #The Fashion MNIST data is available directly in the tf.keras datasets API. You load it like this: mnist = tf . keras . datasets . fashion_mnist ( traning_images . training_labels ),( test_images , test_labels ) = mnist . load_data () #You'll notice that all of the values in the number are between 0 and 255. If we are training a neural network, for various reasons it's easier if we treat all values as between 0 and 1, a process called 'normalizing'...and fortunately in Python it's easy to normalize a list like this without looping. You do it like this: training_images = training_images / 255.0 test_images = test_images / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 128 , activation = tf . nn . relu ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax )]) model . compile ( optimizer = tf . train . AdamOptimizer (), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , epochs = 5 ) # now let see how good is the model model . evaluate ( test_images , test_labels )","title":"An Example of a script"},{"location":"Courses/Coursera/Introduction tensorflow/Introduction to computer vision.html#explanation_of_some_keywords","text":"Sequential: That defines a SEQUENCE of layers in the neural network Flatten: Remember earlier where our images were a square, when you printed them out? Flatten just takes that square and turns it into a 1 dimensional set. Dense: Adds a layer of neurons Each layer of neurons need an activation function to tell them what to do. There\u2019s lots of options, but just use these for now. Relu effectively means \u201cIf X>0 return X, else return 0\u201d \u2013 so what it does it it only passes values 0 or greater to the next layer in the network. Softmax takes a set of values, and effectively picks the biggest one, so, for example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], it saves you from fishing through it looking for the biggest value, and turns it into [0,0,0,0,1,0,0,0,0] \u2013 The goal is to save a lot of coding!","title":"Explanation of some Keywords"},{"location":"Courses/Coursera/Introduction tensorflow/Introduction to computer vision.html#callback_to_stop_the_training","text":"first we will need to create the class myCallback 1 2 3 4 5 6 class myCallback ( tf . keras . callbacks . Callback ): \"\"\"docstring for myCallback\"\"\" def on_epoch_end ( self , epoch , logs = {}): if ( logs . get ( 'acc' ) > 0.6 ): print ( \" \\n Reached 60% accuracy so cancelling training!\" ) self . model . stop_training = True Instantiate the myCallback class 1 callbacks = myCallbacks () now we can make changes in the fit function to add the callback 1 model . fit ( x_training , y_training , epochs = 1 - , callbacks = [ callbacks ]) so the completed script will be: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import tensorflow as tf class myCallback ( tf . keras . callbacks . Callback ): def on_epoch_end ( self , epoch , logs = {}): if ( logs . get ( 'acc' ) > 0.6 ): print ( \" \\n Reached 60% accuracy so cancelling training!\" ) self . model . stop_training = True mnist = tf . keras . datasets . fashion_mnist ( x_train , y_train ),( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 callbacks = myCallback () model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( x_train , y_train , epochs = 10 , callbacks = [ callbacks ]) Now here ans example with the other data set, the original MNIST that contain handwritten numbers from 0 to 9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import tensorflow as tf mnist = tf . keras . datasets . mnist ( x_train , y_train ),( x_test , y_test ) = mnist . load_data () class myCallback ( tf . keras . callbacks . Callback ): def on_epoch_end ( self , epoch , logs = {}): if ( logs . get ( 'acc' ) > 0.99 ): print ( ' \\n Reached 99% accuracy so cancelling training!' ) self . model . stop_training = True callbacks = myCallback () x_train , x_test = x_train / 255.0 , x_test / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( x_train , y_train , epochs = 10 , callbacks = [ callbacks ])","title":"Callback to stop the training"},{"location":"Courses/Coursera/Introduction tensorflow/Step_by_step_Convolutions.html","text":"Let\u2019s look at the code again, and see, step by step how the Convolutions were built: Step 1: Gather the Data \u00b6 The training data needs to be reshaped, this is because the convolution layer is expecting a single Tensor but instead we have a 60,000 28x28x1 in a list, so what we need is to create a single 4D, the tensor mentioned before, a list that will look like 60000x28x28x1, and the same for the rest of the images. If you don\u2019t do this, you\u2019ll get an error when training as the Convolutions do not recognize the shape. 1 2 3 4 5 6 7 8 9 import tensorflow as tf mnist = tf . keras . datasets . fashion_mnist ( training_images , training_labels ), ( test_images , test_labels ) = mnist . load_data () #Here is where we reshape training_images = training_images . reshape ( 60000 , 28 , 28 , 1 ) #Here we normalize training_images = training_images / 255.0 test_images = test_images . reshape ( 10000 , 28 , 28 , 1 ) test_images = test_images / 255.0 Reshaping the list \u00b6 1 training_images = training_images . reshape ( 6000 , 28 , 28 , 1 ) where 60000 is the amount of pictures, 28,28 is the size of the pictures 28x28, and, finally 1 is the number of channels in this case is a gray scale pictures thus 1 channel. Normalize \u00b6 this is something we already mentioned before, this models work better with smaller number, there fore normalization of information is quite common, and recommended practice. 1 training_images = training_images / 255.0 Step 2: Define the model (Convolutional and Maxpooling Layers) \u00b6 Next is to define the models, the first type of model we saw, we started with the Flatten layer, but in this case we are going to start with the convolutional layer. Convolutional Layer \u00b6 So the Layer will have a series of parameters: The number of convolution, or filters (check here for more info), at this point, it is purely arbitrary, but the suggesting start is with something in the order of 32. The size of the Convolution, in this case a 3x3 grid. The activation to use in this case will the the Rectified Linear Unit or ReLU ( which it will return X when x>0, else return 0) In the first layer of the convolution, we need to add the input data. 1 tf . keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 28 , 28 , 1 )) MaxPooling Layer \u00b6 The next layer will be a MaxPooling layer which will compress the image, it will maintain the content of the features that were previously highlighted by the convolution, by specifying (2,2) for the Maxpooling (check here for more information about maxpooling), the effect is to quarter the size of the image. this layer create a 2x2 array of pixels, and picks the biggest one thus turning 4 pixels into 1, effectively reducing the image by 25%. 1 tf . keras . layers . MaxPooling2D ( 2 , 2 ) You can call model.summary() to see the size and shape of the network, and you\u2019ll notice that after every MaxPooling layer, the image size is reduced in this way. Add another convolution and Maxpooling 1 2 tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ) At this point we have part of the model, the first part that deal with the data preparation. 1 2 3 4 5 6 model = tf . keras . models . Sequential ([ tf . keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 28 , 28 , 1 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ) ... Step 3: Define the model (Deep neural network) \u00b6 Now flatten the output. After this you\u2019ll just have the same DNN structure as the non convolutional version 1 tf . keras . layers . Flatten (), The same 128 dense layers, and 10 output layers as in the pre-convolution example: 1 2 3 4 ... tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) so the full model will look like 1 2 3 4 5 6 7 8 9 model = tf . keras . models . Sequential ([ tf . keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 28 , 28 , 1 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) Step 4: Compile and Train the model \u00b6 Now compile the model, call the fit method to do the training, and evaluate the loss and accuracy from the test set. 1 2 3 4 model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , epochs = 5 ) test_loss , test_acc = model . evaluate ( test_images , test_labels ) print ( test_acc ) The whole model \u00b6 This model we use just one layer of convolution and Max pooling 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import tensorflow as tf print ( tf . __version__ ) #getting the Data set mnist = tf . keras . datasets . mnist #Load the Data ( training_images , training_labels ), ( test_images , test_labels ) = mnist . load_data () #Reshape the Training Data training_images = training_images . reshape ( 60000 , 28 , 28 , 1 ) #Normalizing the Training Data training_images = training_images / 255.0 #Reshape the Testing Data test_images = test_images . reshape ( 10000 , 28 , 28 , 1 ) #Normalizing the Test Data test_images = test_images / 255.0 #Build the model model = tf . keras . models . Sequential ([ tf . keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 28 , 28 , 1 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) #Compile the model model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) #Fit or train the model model . fit ( training_images , training_labels , epochs = 10 ) #Evaluate the model and get the accuracy test_loss , test_acc = model . evaluate ( test_images , test_labels ) print ( test_acc ) Additional Information \u00b6 Lode\u2019s Computer Graphics Tutorial Convolutions Sidebar (https://www.youtube.com/playlist?list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF)","title":"(Code) Step by step Convolutions"},{"location":"Courses/Coursera/Introduction tensorflow/Step_by_step_Convolutions.html#step_1_gather_the_data","text":"The training data needs to be reshaped, this is because the convolution layer is expecting a single Tensor but instead we have a 60,000 28x28x1 in a list, so what we need is to create a single 4D, the tensor mentioned before, a list that will look like 60000x28x28x1, and the same for the rest of the images. If you don\u2019t do this, you\u2019ll get an error when training as the Convolutions do not recognize the shape. 1 2 3 4 5 6 7 8 9 import tensorflow as tf mnist = tf . keras . datasets . fashion_mnist ( training_images , training_labels ), ( test_images , test_labels ) = mnist . load_data () #Here is where we reshape training_images = training_images . reshape ( 60000 , 28 , 28 , 1 ) #Here we normalize training_images = training_images / 255.0 test_images = test_images . reshape ( 10000 , 28 , 28 , 1 ) test_images = test_images / 255.0","title":"Step 1: Gather the Data"},{"location":"Courses/Coursera/Introduction tensorflow/Step_by_step_Convolutions.html#reshaping_the_list","text":"1 training_images = training_images . reshape ( 6000 , 28 , 28 , 1 ) where 60000 is the amount of pictures, 28,28 is the size of the pictures 28x28, and, finally 1 is the number of channels in this case is a gray scale pictures thus 1 channel.","title":"Reshaping the list"},{"location":"Courses/Coursera/Introduction tensorflow/Step_by_step_Convolutions.html#normalize","text":"this is something we already mentioned before, this models work better with smaller number, there fore normalization of information is quite common, and recommended practice. 1 training_images = training_images / 255.0","title":"Normalize"},{"location":"Courses/Coursera/Introduction tensorflow/Step_by_step_Convolutions.html#step_2_define_the_model_convolutional_and_maxpooling_layers","text":"Next is to define the models, the first type of model we saw, we started with the Flatten layer, but in this case we are going to start with the convolutional layer.","title":"Step 2: Define the model (Convolutional and Maxpooling Layers)"},{"location":"Courses/Coursera/Introduction tensorflow/Step_by_step_Convolutions.html#convolutional_layer","text":"So the Layer will have a series of parameters: The number of convolution, or filters (check here for more info), at this point, it is purely arbitrary, but the suggesting start is with something in the order of 32. The size of the Convolution, in this case a 3x3 grid. The activation to use in this case will the the Rectified Linear Unit or ReLU ( which it will return X when x>0, else return 0) In the first layer of the convolution, we need to add the input data. 1 tf . keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 28 , 28 , 1 ))","title":"Convolutional Layer"},{"location":"Courses/Coursera/Introduction tensorflow/Step_by_step_Convolutions.html#maxpooling_layer","text":"The next layer will be a MaxPooling layer which will compress the image, it will maintain the content of the features that were previously highlighted by the convolution, by specifying (2,2) for the Maxpooling (check here for more information about maxpooling), the effect is to quarter the size of the image. this layer create a 2x2 array of pixels, and picks the biggest one thus turning 4 pixels into 1, effectively reducing the image by 25%. 1 tf . keras . layers . MaxPooling2D ( 2 , 2 ) You can call model.summary() to see the size and shape of the network, and you\u2019ll notice that after every MaxPooling layer, the image size is reduced in this way. Add another convolution and Maxpooling 1 2 tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ) At this point we have part of the model, the first part that deal with the data preparation. 1 2 3 4 5 6 model = tf . keras . models . Sequential ([ tf . keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 28 , 28 , 1 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ) ...","title":"MaxPooling Layer"},{"location":"Courses/Coursera/Introduction tensorflow/Step_by_step_Convolutions.html#step_3_define_the_model_deep_neural_network","text":"Now flatten the output. After this you\u2019ll just have the same DNN structure as the non convolutional version 1 tf . keras . layers . Flatten (), The same 128 dense layers, and 10 output layers as in the pre-convolution example: 1 2 3 4 ... tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) so the full model will look like 1 2 3 4 5 6 7 8 9 model = tf . keras . models . Sequential ([ tf . keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 28 , 28 , 1 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ])","title":"Step 3: Define the model (Deep neural network)"},{"location":"Courses/Coursera/Introduction tensorflow/Step_by_step_Convolutions.html#step_4_compile_and_train_the_model","text":"Now compile the model, call the fit method to do the training, and evaluate the loss and accuracy from the test set. 1 2 3 4 model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( training_images , training_labels , epochs = 5 ) test_loss , test_acc = model . evaluate ( test_images , test_labels ) print ( test_acc )","title":"Step 4: Compile and Train the model"},{"location":"Courses/Coursera/Introduction tensorflow/Step_by_step_Convolutions.html#the_whole_model","text":"This model we use just one layer of convolution and Max pooling 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import tensorflow as tf print ( tf . __version__ ) #getting the Data set mnist = tf . keras . datasets . mnist #Load the Data ( training_images , training_labels ), ( test_images , test_labels ) = mnist . load_data () #Reshape the Training Data training_images = training_images . reshape ( 60000 , 28 , 28 , 1 ) #Normalizing the Training Data training_images = training_images / 255.0 #Reshape the Testing Data test_images = test_images . reshape ( 10000 , 28 , 28 , 1 ) #Normalizing the Test Data test_images = test_images / 255.0 #Build the model model = tf . keras . models . Sequential ([ tf . keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 28 , 28 , 1 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) #Compile the model model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) #Fit or train the model model . fit ( training_images , training_labels , epochs = 10 ) #Evaluate the model and get the accuracy test_loss , test_acc = model . evaluate ( test_images , test_labels ) print ( test_acc )","title":"The whole model"},{"location":"Courses/Coursera/Introduction tensorflow/Step_by_step_Convolutions.html#additional_information","text":"Lode\u2019s Computer Graphics Tutorial Convolutions Sidebar (https://www.youtube.com/playlist?list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF)","title":"Additional Information"},{"location":"Courses/Coursera/Introduction tensorflow/The Hello World of neural networks.html","text":"The \u2018Hello World\u2019 of neural networks \u00b6 A neural network is basically a set of functions which can learn patterns. The layers and the neurons \u00b6 The simplest possible neural network is one that has only one neuron in it, and that\u2019s what this line of code does. In keras, you use the word dense to define a layer of connected neurons. There\u2019s only one dense here 1 model = keras . Sequential ([ keras . layers . Dense ( unit = 1 , input_shape [ 1 ])]) There\u2019s only one dense here. So there\u2019s only one layer and there\u2019s only one unit in it, so it\u2019s a single neuron. Successive layers are defined in sequence, hence the word sequential. You define the shape of what\u2019s input to the neural network in the first and in this case the only layer, and you can see that our input shape is super simple. It\u2019s just one value The optimizer and the loss \u00b6 Here are two function roles that you should be aware of though and these are loss functions and optimizers. This code defines them. 1 model . compile ( optimizer = 'sgd' , loss = 'mean_squared_error' ) The loss function measures this and then gives the data to the optimizer which figures out the next guess. So the optimizer thinks about how good or how badly the guess was done using the data from the loss function. As the guesses get better and better, an accuracy approaches 100 percent, the term convergence is used. The data \u00b6 the next step is the data, in this case we use the library numpy 1 2 xs = np . array ([ - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 , 4.0 ], dtype = float ) xy = np . array ([ - 3.0 , - 1.0 , 1.0 , 3.0 , 5.0 , 9.0 ], dtype = float ) The training \u00b6 The training takes place in the fit command. 1 model . fit ( xs , ys , epochs = 500 ) The epochs equals 500 value means that it will go through the training loop 500 times.Then the model has finished training, it will then give you back values using the predict method. The prediction \u00b6 1 print ( model . predict ([ 10.0 ])) you\u2019ll see that it will return a value very close to 19 but not exactly 19. there are two main reasons. The first is that you trained it using very little data. There\u2019s only six points. The second main reason, when we use neural networks, as they try to figure out the answers for everything, they deal in probability. You\u2019ll see that a lot and you\u2019ll have to adjust how you handle answers to fit. Example: \u00b6 Imagine if house pricing was as easy as a house costs 50k + 50k per bedroom, so that a 1 bedroom house costs 100k, a 2 bedroom house costs 150k etc. How would you create a neural network that learns this relationship so that it would predict a 7 bedroom house as costing close to 400k etc. Hint: Your network might work better if you scale the house price down. You don\u2019t have to give the answer 400\u2026it might be better to create something that predicts the number 4, and then your answer is in the \u2018hundreds of thousands\u2019 etc. 1 2 3 4 5 6 7 8 9 import tensorflow as tf import numpy as np from tensorflow import keras model = tf . keras . Sequential ([ keras . layers . Dense ( units = 1 , input_shape = [ 1 ])]) model . compile ( optimizer = 'sgd' , loss = 'mean_squared_error' ) xs = np . array ([ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 6.0 ], dtype = float ) ys = np . array ([ 1.0 , 1.5 , 2.0 , 2.5 , 3.0 , 3.5 ], dtype = float ) model . fit ( xs , ys , epochs = 1000 ) print ( model . predict ([ 7.0 ])) Interesting info","title":"The Hello World of neural networks"},{"location":"Courses/Coursera/Introduction tensorflow/The Hello World of neural networks.html#the_hello_world_of_neural_networks","text":"A neural network is basically a set of functions which can learn patterns.","title":"The \u2018Hello World\u2019 of neural networks"},{"location":"Courses/Coursera/Introduction tensorflow/The Hello World of neural networks.html#the_layers_and_the_neurons","text":"The simplest possible neural network is one that has only one neuron in it, and that\u2019s what this line of code does. In keras, you use the word dense to define a layer of connected neurons. There\u2019s only one dense here 1 model = keras . Sequential ([ keras . layers . Dense ( unit = 1 , input_shape [ 1 ])]) There\u2019s only one dense here. So there\u2019s only one layer and there\u2019s only one unit in it, so it\u2019s a single neuron. Successive layers are defined in sequence, hence the word sequential. You define the shape of what\u2019s input to the neural network in the first and in this case the only layer, and you can see that our input shape is super simple. It\u2019s just one value","title":"The layers and the neurons"},{"location":"Courses/Coursera/Introduction tensorflow/The Hello World of neural networks.html#the_optimizer_and_the_loss","text":"Here are two function roles that you should be aware of though and these are loss functions and optimizers. This code defines them. 1 model . compile ( optimizer = 'sgd' , loss = 'mean_squared_error' ) The loss function measures this and then gives the data to the optimizer which figures out the next guess. So the optimizer thinks about how good or how badly the guess was done using the data from the loss function. As the guesses get better and better, an accuracy approaches 100 percent, the term convergence is used.","title":"The optimizer and the loss"},{"location":"Courses/Coursera/Introduction tensorflow/The Hello World of neural networks.html#the_data","text":"the next step is the data, in this case we use the library numpy 1 2 xs = np . array ([ - 1.0 , 0.0 , 1.0 , 2.0 , 3.0 , 4.0 ], dtype = float ) xy = np . array ([ - 3.0 , - 1.0 , 1.0 , 3.0 , 5.0 , 9.0 ], dtype = float )","title":"The data"},{"location":"Courses/Coursera/Introduction tensorflow/The Hello World of neural networks.html#the_training","text":"The training takes place in the fit command. 1 model . fit ( xs , ys , epochs = 500 ) The epochs equals 500 value means that it will go through the training loop 500 times.Then the model has finished training, it will then give you back values using the predict method.","title":"The training"},{"location":"Courses/Coursera/Introduction tensorflow/The Hello World of neural networks.html#the_prediction","text":"1 print ( model . predict ([ 10.0 ])) you\u2019ll see that it will return a value very close to 19 but not exactly 19. there are two main reasons. The first is that you trained it using very little data. There\u2019s only six points. The second main reason, when we use neural networks, as they try to figure out the answers for everything, they deal in probability. You\u2019ll see that a lot and you\u2019ll have to adjust how you handle answers to fit.","title":"The prediction"},{"location":"Courses/Coursera/Introduction tensorflow/The Hello World of neural networks.html#example","text":"Imagine if house pricing was as easy as a house costs 50k + 50k per bedroom, so that a 1 bedroom house costs 100k, a 2 bedroom house costs 150k etc. How would you create a neural network that learns this relationship so that it would predict a 7 bedroom house as costing close to 400k etc. Hint: Your network might work better if you scale the house price down. You don\u2019t have to give the answer 400\u2026it might be better to create something that predicts the number 4, and then your answer is in the \u2018hundreds of thousands\u2019 etc. 1 2 3 4 5 6 7 8 9 import tensorflow as tf import numpy as np from tensorflow import keras model = tf . keras . Sequential ([ keras . layers . Dense ( units = 1 , input_shape = [ 1 ])]) model . compile ( optimizer = 'sgd' , loss = 'mean_squared_error' ) xs = np . array ([ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 , 6.0 ], dtype = float ) ys = np . array ([ 1.0 , 1.5 , 2.0 , 2.5 , 3.0 , 3.5 ], dtype = float ) model . fit ( xs , ys , epochs = 1000 ) print ( model . predict ([ 7.0 ])) Interesting info","title":"Example:"},{"location":"Courses/Coursera/Introduction tensorflow/Using_Real_world_Image.html","text":"this is a video of an application to disease detection with the Cassava plant, here Understanding ImageGeneration \u00b6 On limitation that we face in the previous Notes was that it used a dataset with uniform images, Images of clothing that was staged and framed in 28 by 28. What happen when the subject are in different locations? for example: This dataset has images with different aspect ration, size and location. In some cases, there may even be multiple subjects. In addition to that, the earlier examples with a fashion data used a built-in dataset. All of the data, previously, was handily split into training and test sets for you and labels were available. In many scenarios, that\u2019s not going to be the case and you\u2019ll have to do it for yourself. we\u2019ll take a look at some of the APIs that are available to make that easier for you. In particular, the image generator in TensorFlow. One feature of the image generator is that you can point it at a directory and then the sub-directories of that will automatically generate labels for you. So for example, consider the directory structure in the image above, you have sub-directories for training and validation. When you put sub-directories in these for horses and humans and store the images in there, the image generator can create a feeder for those images and auto label them for you. let say i point the generator to the Training directory,the labels will be horses and humans and all of the images in each directory will be loaded and labeled accordingly. Image Generator in Code \u00b6 the Image generator class is available in Keras.preprocessing.image and we import it like this 1 2 from tensorflow.keras.preprocessing.image import ImageDataGenerator Now we can instantiate and image generator like this 1 train_datagen = imageDataGenerator ( rescale = 1. / 255 ) In this case we are passing rescale in order to normalize the data Now we need to load the images, so, we can then call the flow from directory method on it to get it to load images from that directory and its sub-directories. a common mistake that people point the generator at the sub-directory. It will fail in that circumstance. You should always point it at the directory that contains sub-directories that contain your images 1 2 3 4 5 train_generator = train_datagen . flow_from_directory ( train_dir , target_size = ( 300 , 300 ), batch_size = 128 , class_mode = 'binary' ) 1 2 3 4 5 6 7 # Flow training images in batches of 128 using train_datagen generator validation_generator = validation_datagen . flow_from_directory ( '/tmp/validation-horse-or-human/' , # This is the source directory for training images target_size = ( 300 , 300 ), # All images will be resized to 150x150 batch_size = 32 , # Since we use binary_crossentropy loss, we need binary labels class_mode = 'binary' ) The names of the sub-directories will be the labels for your images that are contained within them. Make sure the first parameter train_dir is pointing to the right directory. Now, images might come in all shapes and sizes and unfortunately for training a neural network, the input data all has to be the same size, so the images will need to be resized to make them consistent. The nice thing about this code is that the images are resized for you as they\u2019re loaded. So you don\u2019t need to preprocess thousands of images on your file system. The advantage of resize the data at runtime like this is that you can then experiment with different sizes without impacting your source data. The images will be loaded for training and validation in batches where it\u2019s more efficient than doing it one by one. Finally, there\u2019s the class mode. Now, this is a binary classifier i.e. it picks between two different things; horses and humans, so we specify that here. The validation generator should be exactly the same except of course it points at a different directory, the one containing the sub-directories containing the test images. Defining a ConvNet to use complex images \u00b6 Now we are going to see the model that will classify the human vs horses. this model is quite similar to the one that classify clothes, but with some minor differences that we are going to see. 1 2 3 4 5 6 7 8 9 10 11 model = tf . keras . models . Sequential ([ tf . keras . layers . Conv2D ( 16 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 300 , 300 , 3 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 512 , activation = 'relu' ), tf . keras . layers . Dense ( 1 , activation = 'sigmoid' ) ]) The differences \u00b6 The model is still sequential, but with some differences: 1. Three Convolutional and Maxpooling Layers This reflect the higher complexity of the model, in the previous model we started with 28x28 but in this case we started with 300x300. 2. Input Shape In this case we are dealing with color images or RGB images, which means we need 3 channels depth, so in this case the input shape is input_shape = (300,300,3) 3. Output Layer In the previous model we use 10 classes, so we have 10 neuron outputs, but in this case we are using just 1, but, we have two classes, how is this possible?, well this is because we are using a different activation in this case sigmoid which is the best activation for binary classification where one class will move towards 1 and the other towards 0. Training the ConvNet with fit_generator \u00b6 The compile function \u00b6 we have a loss function and an optimizer . When classifying the ten items of fashion, in that previous model the loss function was a categorical cross entropy . But because we\u2019re doing a binary choice here, let\u2019s pick a binary_crossentropy instead. Now the about the optimizer , we used an Adam optimizer in this case we use the RMSprop , where we can adjust the learning rate to experiment with performance. 1 2 3 from tensorflow.keras.optimizers import RMSprop model . compile ( loss = 'binary_crossentropy' , optimizer = RSSPROP ( lr = 0.001 ), metrics = [ 'acc' ]) The Training function (fit_generator) \u00b6 1 2 3 4 5 6 7 history = model . fit_generator ( train_enerator , steps_per_epoch = 8 , epochs = 15 , validation_data = validation_generator , validation_steps = 8 , verbose = 2 ) Because we are using generators instead the dataset, now you call model.fit_generator , now let see the parameters: The first parameter is the training generator that you set up earlier. This streams the images from the training directory. Remember the batch size you used when you created it, it was 128, that\u2019s important in the next step. There are 1,024 images in the training directory, so we\u2019re loading them in 128 at a time. So in order to load them all, we need to do 8 batches( 8 * 128 = 1024 8 * 128 = 1024 ). So we set the steps_per_epoch to cover that. Here we just set the number of epochs to train for. This is a bit more complex, so let\u2019s use, say, 15 epochs in this case. Here we just set the number of epochs to train for. This is a bit more complex, so let\u2019s use, say, 15 epochs in this case. It had 256 images, and we wanted to handle them in batches of 32, so we will do 8 steps. The verbose parameter specifies how much to display while training is going on. With verbose set to 2, we\u2019ll get a little less animation hiding the epoch progress. Running the Model \u00b6 Now, we will need to run the model, in this case we are going to use a code that will contain some special code for colab and the colab for this model (without validation) can be found Horse-or-Human-NoValidation . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np from google.colab import files from keras.preprocessing import image uploaded = files . upload () for fn in uploaded . keys (): # predicting images path = '/content/' + fn img = image . load_img ( path , target_size = ( 300 , 300 )) x = image . img_to_array ( img ) x = np . expand_dims ( x , axis = 0 ) images = np . vstack ([ x ]) classes = model . predict ( images , batch_size = 10 ) print ( classes [ 0 ]) if classes [ 0 ] > 0.5 : print ( fn + \" is a human\" ) else : print ( fn + \" is a horse\" ) Colab specific - Button to upload images \u00b6 Here we show the code that is specific for colab, this will give me a button that i can use to pick another image that I\u2019m going to use to make the prediction The images are will have a path, this image path is then loaded into this list called uploaded 1 uploaded = files . upload () Loop to iterate in the collection that holds the images \u00b6 now, we need a loop to \u201cread\u201d or iterate though all the images in the collection It is really important to set the target_size to the correct dimensions to match the input images that we specify designing the model. 1 2 3 ... img = image . load_img ( path , target_size = ( 300 , 300 )) ... predict Predictions \u00b6 after feed the images with the correct size we can start with the predictions, so we can call model.predict passing the details and we will get in return an array of classes. In the case of binary classification, this will only contain one item with a value close to 0 for one class and close to 1 for the other, that is why we use the if at the end.","title":"Using Real-World Images"},{"location":"Courses/Coursera/Introduction tensorflow/Using_Real_world_Image.html#understanding_imagegeneration","text":"On limitation that we face in the previous Notes was that it used a dataset with uniform images, Images of clothing that was staged and framed in 28 by 28. What happen when the subject are in different locations? for example: This dataset has images with different aspect ration, size and location. In some cases, there may even be multiple subjects. In addition to that, the earlier examples with a fashion data used a built-in dataset. All of the data, previously, was handily split into training and test sets for you and labels were available. In many scenarios, that\u2019s not going to be the case and you\u2019ll have to do it for yourself. we\u2019ll take a look at some of the APIs that are available to make that easier for you. In particular, the image generator in TensorFlow. One feature of the image generator is that you can point it at a directory and then the sub-directories of that will automatically generate labels for you. So for example, consider the directory structure in the image above, you have sub-directories for training and validation. When you put sub-directories in these for horses and humans and store the images in there, the image generator can create a feeder for those images and auto label them for you. let say i point the generator to the Training directory,the labels will be horses and humans and all of the images in each directory will be loaded and labeled accordingly.","title":"Understanding ImageGeneration"},{"location":"Courses/Coursera/Introduction tensorflow/Using_Real_world_Image.html#image_generator_in_code","text":"the Image generator class is available in Keras.preprocessing.image and we import it like this 1 2 from tensorflow.keras.preprocessing.image import ImageDataGenerator Now we can instantiate and image generator like this 1 train_datagen = imageDataGenerator ( rescale = 1. / 255 ) In this case we are passing rescale in order to normalize the data Now we need to load the images, so, we can then call the flow from directory method on it to get it to load images from that directory and its sub-directories. a common mistake that people point the generator at the sub-directory. It will fail in that circumstance. You should always point it at the directory that contains sub-directories that contain your images 1 2 3 4 5 train_generator = train_datagen . flow_from_directory ( train_dir , target_size = ( 300 , 300 ), batch_size = 128 , class_mode = 'binary' ) 1 2 3 4 5 6 7 # Flow training images in batches of 128 using train_datagen generator validation_generator = validation_datagen . flow_from_directory ( '/tmp/validation-horse-or-human/' , # This is the source directory for training images target_size = ( 300 , 300 ), # All images will be resized to 150x150 batch_size = 32 , # Since we use binary_crossentropy loss, we need binary labels class_mode = 'binary' ) The names of the sub-directories will be the labels for your images that are contained within them. Make sure the first parameter train_dir is pointing to the right directory. Now, images might come in all shapes and sizes and unfortunately for training a neural network, the input data all has to be the same size, so the images will need to be resized to make them consistent. The nice thing about this code is that the images are resized for you as they\u2019re loaded. So you don\u2019t need to preprocess thousands of images on your file system. The advantage of resize the data at runtime like this is that you can then experiment with different sizes without impacting your source data. The images will be loaded for training and validation in batches where it\u2019s more efficient than doing it one by one. Finally, there\u2019s the class mode. Now, this is a binary classifier i.e. it picks between two different things; horses and humans, so we specify that here. The validation generator should be exactly the same except of course it points at a different directory, the one containing the sub-directories containing the test images.","title":"Image Generator in Code"},{"location":"Courses/Coursera/Introduction tensorflow/Using_Real_world_Image.html#defining_a_convnet_to_use_complex_images","text":"Now we are going to see the model that will classify the human vs horses. this model is quite similar to the one that classify clothes, but with some minor differences that we are going to see. 1 2 3 4 5 6 7 8 9 10 11 model = tf . keras . models . Sequential ([ tf . keras . layers . Conv2D ( 16 , ( 3 , 3 ), activation = 'relu' , input_shape = ( 300 , 300 , 3 )), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Conv2D ( 32 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Conv2D ( 64 , ( 3 , 3 ), activation = 'relu' ), tf . keras . layers . MaxPooling2D ( 2 , 2 ), tf . keras . layers . Flatten (), tf . keras . layers . Dense ( 512 , activation = 'relu' ), tf . keras . layers . Dense ( 1 , activation = 'sigmoid' ) ])","title":"Defining a ConvNet to use complex images"},{"location":"Courses/Coursera/Introduction tensorflow/Using_Real_world_Image.html#the_differences","text":"The model is still sequential, but with some differences: 1. Three Convolutional and Maxpooling Layers This reflect the higher complexity of the model, in the previous model we started with 28x28 but in this case we started with 300x300. 2. Input Shape In this case we are dealing with color images or RGB images, which means we need 3 channels depth, so in this case the input shape is input_shape = (300,300,3) 3. Output Layer In the previous model we use 10 classes, so we have 10 neuron outputs, but in this case we are using just 1, but, we have two classes, how is this possible?, well this is because we are using a different activation in this case sigmoid which is the best activation for binary classification where one class will move towards 1 and the other towards 0.","title":"The differences"},{"location":"Courses/Coursera/Introduction tensorflow/Using_Real_world_Image.html#training_the_convnet_with_fit_generator","text":"","title":"Training the ConvNet with fit_generator"},{"location":"Courses/Coursera/Introduction tensorflow/Using_Real_world_Image.html#the_compile_function","text":"we have a loss function and an optimizer . When classifying the ten items of fashion, in that previous model the loss function was a categorical cross entropy . But because we\u2019re doing a binary choice here, let\u2019s pick a binary_crossentropy instead. Now the about the optimizer , we used an Adam optimizer in this case we use the RMSprop , where we can adjust the learning rate to experiment with performance. 1 2 3 from tensorflow.keras.optimizers import RMSprop model . compile ( loss = 'binary_crossentropy' , optimizer = RSSPROP ( lr = 0.001 ), metrics = [ 'acc' ])","title":"The compile function"},{"location":"Courses/Coursera/Introduction tensorflow/Using_Real_world_Image.html#the_training_function_fit_generator","text":"1 2 3 4 5 6 7 history = model . fit_generator ( train_enerator , steps_per_epoch = 8 , epochs = 15 , validation_data = validation_generator , validation_steps = 8 , verbose = 2 ) Because we are using generators instead the dataset, now you call model.fit_generator , now let see the parameters: The first parameter is the training generator that you set up earlier. This streams the images from the training directory. Remember the batch size you used when you created it, it was 128, that\u2019s important in the next step. There are 1,024 images in the training directory, so we\u2019re loading them in 128 at a time. So in order to load them all, we need to do 8 batches( 8 * 128 = 1024 8 * 128 = 1024 ). So we set the steps_per_epoch to cover that. Here we just set the number of epochs to train for. This is a bit more complex, so let\u2019s use, say, 15 epochs in this case. Here we just set the number of epochs to train for. This is a bit more complex, so let\u2019s use, say, 15 epochs in this case. It had 256 images, and we wanted to handle them in batches of 32, so we will do 8 steps. The verbose parameter specifies how much to display while training is going on. With verbose set to 2, we\u2019ll get a little less animation hiding the epoch progress.","title":"The Training function (fit_generator)"},{"location":"Courses/Coursera/Introduction tensorflow/Using_Real_world_Image.html#running_the_model","text":"Now, we will need to run the model, in this case we are going to use a code that will contain some special code for colab and the colab for this model (without validation) can be found Horse-or-Human-NoValidation . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np from google.colab import files from keras.preprocessing import image uploaded = files . upload () for fn in uploaded . keys (): # predicting images path = '/content/' + fn img = image . load_img ( path , target_size = ( 300 , 300 )) x = image . img_to_array ( img ) x = np . expand_dims ( x , axis = 0 ) images = np . vstack ([ x ]) classes = model . predict ( images , batch_size = 10 ) print ( classes [ 0 ]) if classes [ 0 ] > 0.5 : print ( fn + \" is a human\" ) else : print ( fn + \" is a horse\" )","title":"Running the Model"},{"location":"Courses/Coursera/Introduction tensorflow/Using_Real_world_Image.html#colab_specific_-_button_to_upload_images","text":"Here we show the code that is specific for colab, this will give me a button that i can use to pick another image that I\u2019m going to use to make the prediction The images are will have a path, this image path is then loaded into this list called uploaded 1 uploaded = files . upload ()","title":"Colab specific - Button to upload images"},{"location":"Courses/Coursera/Introduction tensorflow/Using_Real_world_Image.html#loop_to_iterate_in_the_collection_that_holds_the_images","text":"now, we need a loop to \u201cread\u201d or iterate though all the images in the collection It is really important to set the target_size to the correct dimensions to match the input images that we specify designing the model. 1 2 3 ... img = image . load_img ( path , target_size = ( 300 , 300 )) ...","title":"Loop to iterate in the collection that holds the images"},{"location":"Courses/Coursera/Introduction tensorflow/Using_Real_world_Image.html#predict_predictions","text":"after feed the images with the correct size we can start with the predictions, so we can call model.predict passing the details and we will get in return an array of classes. In the case of binary classification, this will only contain one item with a value close to 0 for one class and close to 1 for the other, that is why we use the if at the end.","title":"predict Predictions"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/ANN.html","text":"Artificial Neural Network","title":"Artificial Neural Network"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html","text":"This notes are notes for the Numpy crash course. Numpy is a linear library for Python, it is an essential building block for other PyData ecosystem ( Pandas, scipy, scikit-lean, etc) Using NumPy \u00b6 Normally numpy is import as np 1 import numpy as np NumPy Arrays \u00b6 NumPy came in vector and matrices, Vectors are 1 dimension, matrix are 2D dimensions. Create NumPy Arrays \u00b6 we can create an array from a python list 1 2 3 my_list = [ 1 , 2 , 3 ] np . array ( my_list ) # array([1, 2, 3]) and can be a list of list as well 1 2 3 4 5 my_matrix = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] np . array ( my_matrix ) #array([[1, 2, 3], # [4, 5, 6], # [7, 8, 9]]) Build-in Methods \u00b6 arange \u00b6 Return evenly spaced value within a given interval 1 2 3 4 np . arange ( 0 , 10 ) # array([0,1,2,3,4,5,6,7,8,9]) np . arange ( 0 , 11 , 2 ) # array([0, 2, 4, 6, 8, 10]) zeros and ones \u00b6 Generate vectors or matrix of zeros or ones 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 np . zero ( 3 ) # array([0. , 0. , 0.]) np . zero (( 5 , 5 )) # array([[0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.]]) np . ones ( 3 ) # array([1., 1., 1.]) np . ones (( 3 , 3 )) # array([[1., 1., 1.], # [1., 1., 1.], # [1., 1., 1.]]) linspace \u00b6 Return evenly spaced numbers over a specific interval 1 2 3 4 5 6 7 8 9 10 11 12 13 14 np . linspace ( 0 , 10 , 3 ) # array([0., 5., 10.]) np . linspace ( 0 , 5 , 20 ) # array([0. , 0.26315789, 0.52631579, 0.78947368, 1.05263158, # 1.31578947, 1.57894737, 1.84210526, 2.10526316, 2.36842105, # 2.63157895, 2.89473684, 3.15789474, 3.42105263, 3.68421053, # 3.94736842, 4.21052632, 4.47368421, 4.73684211, 5. ]) #Note that .linspace() includes the stop value. To obtain an array of common fractions, increase the number of items: np . linspace ( 0 , 5 , 21 ) # array([0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. , 2.25, 2.5 , # 2.75, 3. , 3.25, 3.5 , 3.75, 4. , 4.25, 4.5 , 4.75, 5. ]) eye identity matrix \u00b6 Create a identity matrix 1 2 3 4 5 np . eye ( 4 ) # array([[1., 0., 0., 0.], # [0., 1., 0., 0.], # [0., 0., 1., 0.], # [0., 0., 0., 1.]]) Random \u00b6 Here some of the ways we can create random numbers rand \u00b6 Creates an array of the given shape with random uniform distribution over [0,1) 1 2 3 4 5 6 7 8 np . random . rand ( 2 ) # array([0.37065108, 0.89813878]) np . random . rand ( 5 , 5 ) # array([[0.03932992, 0.80719137, 0.50145497, 0.68816102, 0.1216304 ], # [0.44966851, 0.92572848, 0.70802042, 0.10461719, 0.53768331], # [0.12201904, 0.5940684 , 0.89979774, 0.3424078 , 0.77421593], # [0.53191409, 0.0112285 , 0.3989947 , 0.8946967 , 0.2497392 ], # [0.5814085 , 0.37563686, 0.15266028, 0.42948309, 0.26434141]]) randint \u00b6 This generate a integer from low (inclusive) to high (exclusive) 1 2 3 4 np . random . randint ( 1 , 100 ) #61 np . random . randint ( 1 , 100 , 10 ) # array([39, 50, 72, 18,27, 59, 15, 97, 11, 14]) seed \u00b6 It is use to create a random state that can be reproducible, i means, the result will be the same everything we use the same seed 1 2 3 4 5 6 7 np . random . seed ( 42 ) np . random . rand ( 4 ) # array([0.37454012, 0.95071431, 0.73199394, 0.59865848]) np . random . seed ( 42 ) np . random . rand ( 4 ) # array([0.37454012, 0.95071431, 0.73199394, 0.59865848]) Array Attributes and Methods \u00b6 To explain the attributes and methods we need to create a vector and matrix 1 2 3 4 5 arr = np . arange ( 25 ) # array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, # 17, 18, 19, 20, 21, 22, 23, 24]) ranarr = np . random . randint ( 0 , 50 , 10 ) # array([38, 18, 22, 10, 10, 23, 35, 39, 23, 2]) Reshape - reshape \u00b6 Return the same data of the vector or matrix but in a different shape 1 2 3 4 5 6 arr . reshape ( 5 , 5 ) # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14], # [15, 16, 17, 18, 19], # [20, 21, 22, 23, 24]]) max, min, argmax, argmin - max , min , argmax , argmin \u00b6 Let start with ranarr 1 2 ranarr = np . random . randint ( 0 , 50 , 10 ) # array([38, 18, 22, 10, 10, 23, 35, 39, 23, 2]) the maximum number in the array 1 2 ranarr . max () # 39 the index of this maximum number 1 2 ranarr . argmax () # 7 now for the minimum 1 2 3 4 ranarr . min () # 2 ranarr . argmin () # 9 Shape - shape \u00b6 shape is an attribute and not a method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # Vector arr . shape #(25,) # Notice the two sets of brackets arr . reshape ( 1 , 25 ) # array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, # 16, 17, 18, 19, 20, 21, 22, 23, 24]]) arr . reshape ( 1 , 25 ) . shape # (1, 25) arr . reshape ( 25 , 1 ) #array([[ 0], # [ 1], # [ 2], # [ 3], # [ 4], # [ 5], # [ 6], # [ 7], # [ 8], # [ 9], # [10], # [11], # [12], # [13], # [14], # [15], # [16], # [17], # [18], # [19], # [20], # [21], # [22], # [23], # [24]]) arr . reshape ( 25 , 1 ) . shape # (25, 1) dtype \u00b6 In order to know the data type of the object 1 2 3 4 5 6 7 arr . dtype # dtype('int32') arr2 = np . array ([ 1.2 , 3.4 , 5.6 ]) arr2 . dtype #dtype('float64') Numpy Indexing and Selection \u00b6 To select an item in the array we can use a syntax similar to the one use to pick up elements of a list, in the following example we will: 1. Create an array \u00b6 1 2 import numpy as np #create an array arr = np . arange ( 0 , 11 ) 2. Select a single element \u00b6 1 2 arr [ 7 ] #7 3. Select a range of elements \u00b6 1 2 arr [ 0 : 5 ] #array([0,1,2,3,4])\\ Broadcasting \u00b6 The differences between Python list and Numpy arrays can be simplify as; python list you can only reassign values to part of the list with the same size and shape, if you want to replace X number of elements you will need to pass in a new x element list, this is explain better with an example. In the example: 1. Create an array. 2. Slice part of the array. 3. We will change the sliced array. 4. Display the original array. Notice the elements of the array, that belong to the sliced array, were change. This is because the data is not copied in order to avoid memory problems. 1 2 3 4 5 6 7 8 9 10 import numpy as np #create an array arr = np . arange ( 0 , 10 ) #slice the array sliced_arr = arr [ 0 : 6 ] #change the values in the slice sliced_arr [:] = 100 # print the original array to show the changes print ( arr ) #array([100, 100, 100, 100, 100, 100, 6, 7, 8, 9]) If you want to make a copy of the array you can use copy() like new_arr = arr.copy() Indexing 2d arrays (matrices) \u00b6 The syntax will be arr_2d[row][col] or arr_2d[row,col] , the latter the most common used. 1. Create the matrix \u00b6 1 2 import numpy as np arr_2d = np . arange ([ 5 , 10 , 15 ],[ 20 , 25 , 30 ],[ 35 , 40 , 45 ]) 2. Select base in index \u00b6 a row 1 2 arr_2d [ 1 ] #array([20,25,30]) a value 1 2 arr_2d [ 1 ][ 0 ] #20 3. select a matrix inside the matrix \u00b6 1 2 3 arr_2d [: 2 , 1 :] # top right corner # array([25,30], # [40,45]) Conditional selection \u00b6 We can select elements of the arrays base in a condition, let say we want to know what elements are bigger than 4. 1 2 3 4 5 import numpy as np arr = np . arange ( 0 , 10 ) print ( arr > 4 ) #array([False, False, False, False, True, True, True, True, True, # True]) we can save this array of boolean values and use it to slice or select elements of the original array base in this condition 1 2 3 4 5 bool_arr = arr > 4 arr [ bool_arr ] # array([ 5, 6, 7, 8, 9, 10]) arr [ arr > 4 ] # array([ 5, 6, 7, 8, 9, 10]) Operations \u00b6 Arithmetic \u00b6 Numpy allows operation including matrix with matrix and scalar with matrix. 1. Addition, multiplication , subtraction and division \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np arr = np . arange ( 0 , 10 ) arr #array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) #addition arr + arr #array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) # Multiplication arr * arr #array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81]) #Subtraction arr - arr #array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) #Division arr / arr #array([nan, 1., 1., 1., 1., 1., 1., 1., 1., 1.]) numpy will notify us when the division is not possible or the division by 0 1 2 3 1 / arr #array([ inf, 1. , 0.5 , 0.33333333, 0.25 , # 0.2 , 0.16666667, 0.14285714, 0.125 , 0.11111111]) and we have the exponential as well 1 2 arr ** 3 #array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729]) Universal Array function \u00b6 With Numpy we can perform different function to the matrices, square root, logarithmic and geometric functions. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 np . sqrt ( arr ) #array([0. , 1. , 1.41421356, 1.73205081, 2. , # 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ]) # Exponential (e^) np . exp ( arr ) #array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01, # 5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03, # 2.98095799e+03, 8.10308393e+03]) #Trigonometric np . sin ( arr ) #array([ 0. , 0.84147098, 0.90929743, 0.14112001, -0.7568025 , # -0.95892427, -0.2794155 , 0.6569866 , 0.98935825, 0.41211849]) #Natural Logarithm np . log ( arr ) #array([ -inf, 0. , 0.69314718, 1.09861229, 1.38629436, # 1.60943791, 1.79175947, 1.94591015, 2.07944154, 2.19722458]) Statistics \u00b6 as an example of the statistic function that can be perform in numpy we have sum, mean and max 1 2 3 4 5 6 7 8 9 10 arr = np . arange ( 0 , 10 ) arr . sum () #45 arr . mean () #4.5 arr . max () #9 and other examples of statistic functions Axis Logic \u00b6 Wen we work with 2D arrays (Matrix) the array term , axis 0 is the vertical axis ( rows ), and axis 1 is the horizontal ( columns ) so let do sum on the 0 axis, basically sum all the elements vertically, it make sense after the code. 1 2 3 4 5 6 7 8 9 10 11 12 arr_2d = np . array ([[ 1 , 2 , 3 , 4 ],[ 5 , 6 , 7 , 8 ],[ 9 , 10 , 11 , 12 ]]) arr_2d #array([[ 1, 2, 3, 4], # [ 5, 6, 7, 8], # [ 9, 10, 11, 12]]) arr_2d . sum ( axis = 0 ) #array([15, 18, 21, 24]) #[(1+5+9), (2+6+10), (3+7+11), (4+8+12)] arr_2d . sum ( axis = 1 ) #array([10, 26, 42])","title":"Numpy Crash course"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#using_numpy","text":"Normally numpy is import as np 1 import numpy as np","title":"Using NumPy"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#numpy_arrays","text":"NumPy came in vector and matrices, Vectors are 1 dimension, matrix are 2D dimensions.","title":"NumPy Arrays"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#create_numpy_arrays","text":"we can create an array from a python list 1 2 3 my_list = [ 1 , 2 , 3 ] np . array ( my_list ) # array([1, 2, 3]) and can be a list of list as well 1 2 3 4 5 my_matrix = [[ 1 , 2 , 3 ],[ 4 , 5 , 6 ],[ 7 , 8 , 9 ]] np . array ( my_matrix ) #array([[1, 2, 3], # [4, 5, 6], # [7, 8, 9]])","title":"Create NumPy Arrays"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#build-in_methods","text":"","title":"Build-in Methods"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#arange","text":"Return evenly spaced value within a given interval 1 2 3 4 np . arange ( 0 , 10 ) # array([0,1,2,3,4,5,6,7,8,9]) np . arange ( 0 , 11 , 2 ) # array([0, 2, 4, 6, 8, 10])","title":"arange"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#zeros_and_ones","text":"Generate vectors or matrix of zeros or ones 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 np . zero ( 3 ) # array([0. , 0. , 0.]) np . zero (( 5 , 5 )) # array([[0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.]]) np . ones ( 3 ) # array([1., 1., 1.]) np . ones (( 3 , 3 )) # array([[1., 1., 1.], # [1., 1., 1.], # [1., 1., 1.]])","title":"zeros and ones"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#linspace","text":"Return evenly spaced numbers over a specific interval 1 2 3 4 5 6 7 8 9 10 11 12 13 14 np . linspace ( 0 , 10 , 3 ) # array([0., 5., 10.]) np . linspace ( 0 , 5 , 20 ) # array([0. , 0.26315789, 0.52631579, 0.78947368, 1.05263158, # 1.31578947, 1.57894737, 1.84210526, 2.10526316, 2.36842105, # 2.63157895, 2.89473684, 3.15789474, 3.42105263, 3.68421053, # 3.94736842, 4.21052632, 4.47368421, 4.73684211, 5. ]) #Note that .linspace() includes the stop value. To obtain an array of common fractions, increase the number of items: np . linspace ( 0 , 5 , 21 ) # array([0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. , 2.25, 2.5 , # 2.75, 3. , 3.25, 3.5 , 3.75, 4. , 4.25, 4.5 , 4.75, 5. ])","title":"linspace"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#eye_identity_matrix","text":"Create a identity matrix 1 2 3 4 5 np . eye ( 4 ) # array([[1., 0., 0., 0.], # [0., 1., 0., 0.], # [0., 0., 1., 0.], # [0., 0., 0., 1.]])","title":"eye identity matrix"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#random","text":"Here some of the ways we can create random numbers","title":"Random"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#rand","text":"Creates an array of the given shape with random uniform distribution over [0,1) 1 2 3 4 5 6 7 8 np . random . rand ( 2 ) # array([0.37065108, 0.89813878]) np . random . rand ( 5 , 5 ) # array([[0.03932992, 0.80719137, 0.50145497, 0.68816102, 0.1216304 ], # [0.44966851, 0.92572848, 0.70802042, 0.10461719, 0.53768331], # [0.12201904, 0.5940684 , 0.89979774, 0.3424078 , 0.77421593], # [0.53191409, 0.0112285 , 0.3989947 , 0.8946967 , 0.2497392 ], # [0.5814085 , 0.37563686, 0.15266028, 0.42948309, 0.26434141]])","title":"rand"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#randint","text":"This generate a integer from low (inclusive) to high (exclusive) 1 2 3 4 np . random . randint ( 1 , 100 ) #61 np . random . randint ( 1 , 100 , 10 ) # array([39, 50, 72, 18,27, 59, 15, 97, 11, 14])","title":"randint"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#seed","text":"It is use to create a random state that can be reproducible, i means, the result will be the same everything we use the same seed 1 2 3 4 5 6 7 np . random . seed ( 42 ) np . random . rand ( 4 ) # array([0.37454012, 0.95071431, 0.73199394, 0.59865848]) np . random . seed ( 42 ) np . random . rand ( 4 ) # array([0.37454012, 0.95071431, 0.73199394, 0.59865848])","title":"seed"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#array_attributes_and_methods","text":"To explain the attributes and methods we need to create a vector and matrix 1 2 3 4 5 arr = np . arange ( 25 ) # array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, # 17, 18, 19, 20, 21, 22, 23, 24]) ranarr = np . random . randint ( 0 , 50 , 10 ) # array([38, 18, 22, 10, 10, 23, 35, 39, 23, 2])","title":"Array Attributes and Methods"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#reshape_-_reshape","text":"Return the same data of the vector or matrix but in a different shape 1 2 3 4 5 6 arr . reshape ( 5 , 5 ) # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14], # [15, 16, 17, 18, 19], # [20, 21, 22, 23, 24]])","title":"Reshape - reshape"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#max_min_argmax_argmin_-_maxminargmaxargmin","text":"Let start with ranarr 1 2 ranarr = np . random . randint ( 0 , 50 , 10 ) # array([38, 18, 22, 10, 10, 23, 35, 39, 23, 2]) the maximum number in the array 1 2 ranarr . max () # 39 the index of this maximum number 1 2 ranarr . argmax () # 7 now for the minimum 1 2 3 4 ranarr . min () # 2 ranarr . argmin () # 9","title":"max, min, argmax, argmin - max,min,argmax,argmin"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#shape_-_shape","text":"shape is an attribute and not a method 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # Vector arr . shape #(25,) # Notice the two sets of brackets arr . reshape ( 1 , 25 ) # array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, # 16, 17, 18, 19, 20, 21, 22, 23, 24]]) arr . reshape ( 1 , 25 ) . shape # (1, 25) arr . reshape ( 25 , 1 ) #array([[ 0], # [ 1], # [ 2], # [ 3], # [ 4], # [ 5], # [ 6], # [ 7], # [ 8], # [ 9], # [10], # [11], # [12], # [13], # [14], # [15], # [16], # [17], # [18], # [19], # [20], # [21], # [22], # [23], # [24]]) arr . reshape ( 25 , 1 ) . shape # (25, 1)","title":"Shape - shape"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#dtype","text":"In order to know the data type of the object 1 2 3 4 5 6 7 arr . dtype # dtype('int32') arr2 = np . array ([ 1.2 , 3.4 , 5.6 ]) arr2 . dtype #dtype('float64')","title":"dtype"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#numpy_indexing_and_selection","text":"To select an item in the array we can use a syntax similar to the one use to pick up elements of a list, in the following example we will:","title":"Numpy Indexing and Selection"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#1_create_an_array","text":"1 2 import numpy as np #create an array arr = np . arange ( 0 , 11 )","title":"1. Create an array"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#2_select_a_single_element","text":"1 2 arr [ 7 ] #7","title":"2. Select a single element"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#3_select_a_range_of_elements","text":"1 2 arr [ 0 : 5 ] #array([0,1,2,3,4])\\","title":"3. Select a range of elements"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#broadcasting","text":"The differences between Python list and Numpy arrays can be simplify as; python list you can only reassign values to part of the list with the same size and shape, if you want to replace X number of elements you will need to pass in a new x element list, this is explain better with an example. In the example: 1. Create an array. 2. Slice part of the array. 3. We will change the sliced array. 4. Display the original array. Notice the elements of the array, that belong to the sliced array, were change. This is because the data is not copied in order to avoid memory problems. 1 2 3 4 5 6 7 8 9 10 import numpy as np #create an array arr = np . arange ( 0 , 10 ) #slice the array sliced_arr = arr [ 0 : 6 ] #change the values in the slice sliced_arr [:] = 100 # print the original array to show the changes print ( arr ) #array([100, 100, 100, 100, 100, 100, 6, 7, 8, 9]) If you want to make a copy of the array you can use copy() like new_arr = arr.copy()","title":"Broadcasting"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#indexing_2d_arrays_matrices","text":"The syntax will be arr_2d[row][col] or arr_2d[row,col] , the latter the most common used.","title":"Indexing 2d arrays (matrices)"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#1_create_the_matrix","text":"1 2 import numpy as np arr_2d = np . arange ([ 5 , 10 , 15 ],[ 20 , 25 , 30 ],[ 35 , 40 , 45 ])","title":"1. Create the matrix"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#2_select_base_in_index","text":"a row 1 2 arr_2d [ 1 ] #array([20,25,30]) a value 1 2 arr_2d [ 1 ][ 0 ] #20","title":"2. Select base in index"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#3_select_a_matrix_inside_the_matrix","text":"1 2 3 arr_2d [: 2 , 1 :] # top right corner # array([25,30], # [40,45])","title":"3. select a matrix inside the matrix"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#conditional_selection","text":"We can select elements of the arrays base in a condition, let say we want to know what elements are bigger than 4. 1 2 3 4 5 import numpy as np arr = np . arange ( 0 , 10 ) print ( arr > 4 ) #array([False, False, False, False, True, True, True, True, True, # True]) we can save this array of boolean values and use it to slice or select elements of the original array base in this condition 1 2 3 4 5 bool_arr = arr > 4 arr [ bool_arr ] # array([ 5, 6, 7, 8, 9, 10]) arr [ arr > 4 ] # array([ 5, 6, 7, 8, 9, 10])","title":"Conditional selection"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#operations","text":"","title":"Operations"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#arithmetic","text":"Numpy allows operation including matrix with matrix and scalar with matrix.","title":"Arithmetic"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#1_addition_multiplication_subtraction_and_division","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np arr = np . arange ( 0 , 10 ) arr #array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) #addition arr + arr #array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) # Multiplication arr * arr #array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81]) #Subtraction arr - arr #array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) #Division arr / arr #array([nan, 1., 1., 1., 1., 1., 1., 1., 1., 1.]) numpy will notify us when the division is not possible or the division by 0 1 2 3 1 / arr #array([ inf, 1. , 0.5 , 0.33333333, 0.25 , # 0.2 , 0.16666667, 0.14285714, 0.125 , 0.11111111]) and we have the exponential as well 1 2 arr ** 3 #array([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729])","title":"1. Addition, multiplication , subtraction and division"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#universal_array_function","text":"With Numpy we can perform different function to the matrices, square root, logarithmic and geometric functions. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 np . sqrt ( arr ) #array([0. , 1. , 1.41421356, 1.73205081, 2. , # 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ]) # Exponential (e^) np . exp ( arr ) #array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01, # 5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03, # 2.98095799e+03, 8.10308393e+03]) #Trigonometric np . sin ( arr ) #array([ 0. , 0.84147098, 0.90929743, 0.14112001, -0.7568025 , # -0.95892427, -0.2794155 , 0.6569866 , 0.98935825, 0.41211849]) #Natural Logarithm np . log ( arr ) #array([ -inf, 0. , 0.69314718, 1.09861229, 1.38629436, # 1.60943791, 1.79175947, 1.94591015, 2.07944154, 2.19722458])","title":"Universal Array function"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#statistics","text":"as an example of the statistic function that can be perform in numpy we have sum, mean and max 1 2 3 4 5 6 7 8 9 10 arr = np . arange ( 0 , 10 ) arr . sum () #45 arr . mean () #4.5 arr . max () #9 and other examples of statistic functions","title":"Statistics"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Numpy Crash course.html#axis_logic","text":"Wen we work with 2D arrays (Matrix) the array term , axis 0 is the vertical axis ( rows ), and axis 1 is the horizontal ( columns ) so let do sum on the 0 axis, basically sum all the elements vertically, it make sense after the code. 1 2 3 4 5 6 7 8 9 10 11 12 arr_2d = np . array ([[ 1 , 2 , 3 , 4 ],[ 5 , 6 , 7 , 8 ],[ 9 , 10 , 11 , 12 ]]) arr_2d #array([[ 1, 2, 3, 4], # [ 5, 6, 7, 8], # [ 9, 10, 11, 12]]) arr_2d . sum ( axis = 0 ) #array([15, 18, 21, 24]) #[(1+5+9), (2+6+10), (3+7+11), (4+8+12)] arr_2d . sum ( axis = 1 ) #array([10, 26, 42])","title":"Axis Logic"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html","text":"For the review or refresh of pandas we will follow the structure bellow: Series. DataFrames. Missing Data. GroupBy. Operations. Data Input and Output. Series \u00b6 Series are similar to Numpy arrays ( they are build on top of Numpy arrays) and the difference is that SEris can have axis labels, that means that can be located not just by numbers but by labels, and they can hold much more than just numbers, they can hold any type of python object. We are going to create Series in different ways: Creating a series. \u00b6 First we will need to import numpy and pandas 1 2 import numpy as np import pandas as pd now we are going to create a list, a numpy array and a dictionary that later will be use to create the series 1 2 3 4 lables = [ 'a' , 'b' , 'c' ] my_list = [ 10 , 20 , 30 ] arr = np . array ([ 10 , 20 , 30 ]) d = { 'a' : 10 , 'b' : 20 , 'c' : 30 } 1. Using List \u00b6 1 2 3 4 5 pd . Series ( data = mylist ) #0 10 #1 20 #2 30 #dtype: int64 The about is a Series that use just number as labels, now let use the labels defined before with the list lables 1 2 3 4 5 pd . Series ( data = my_list , index = labels ) #a 10 #b 20 #c 30 #dtype: int64 now a shorter way 1 2 3 4 5 pd . Series ( my_list , labels ) #a 10 #b 20 #c 30 #dtype: int64 2. Using NumPy arrays \u00b6 1 2 3 4 5 pd . Series ( arr ) #0 10 #1 20 #2 30 #dtype: int64 now with the labels 1 2 3 4 5 pd . Series ( arr , labels ) #a 10 #b 20 #c 30 #dtype: int64 3. Using Dictionaries \u00b6 1 2 3 4 5 pd . Series ( d ) #a 10 #b 20 #c 30 #dtype: int64 Using Index \u00b6 Pandas use the index name or numbers which allow to access the information ( the index are the rows). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 sales_Q1 = pd . Series ( data = [ 250 , 450 , 200 , 150 ], index = [ 'USA' , 'China' , 'India' , 'Brazil' ]) Sales_Q1 #USA 250 #China 450 #India 200 #Brazil 150 #dtype: int64 sales_Q2 = pd . Series ([ 260 , 500 , 210 , 100 ], index = [ 'USA' , 'China' , 'India' , 'Japan' ]) sales_Q2 #USA 260 #China 500 #India 210 #Japan 100 #dtype: int64 sales_Q1 [ 'USA' ] #250 sales_Q1 + sales_Q2 #Brazil NaN #China 950.0 #India 410.0 #Japan NaN #USA 510.0 #dtype: float64 DataFrames \u00b6 Datafarames are inspire in R programming they look like a group of series put together to share the same index. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import pandas as pd import numpy as np from numpy.random import randint # define columns and index to use later in the DataFrame columns = [ 'W' , 'X' , 'Y' , 'Z' ] # four columns index = [ 'A' , 'B' , 'C' , 'D' , 'E' ] # five rows np . random . seed ( 42 ) data = randint ( - 100 , 100 ,( 5 , 4 )) # randint(low,high, size) print ( data ) #array([[ 2, 79, -8, -86], # [ 6, -29, 88, -80], # [ 2, 21, -26, -13], # [ 16, -1, 3, 51], # [ 30, 49, -48, -99]]) df = pd . DataFrame ( data , index , columns ) print ( df ) Selection and indexing \u00b6 We can select and grad columns or parts of the DataFrame Columns \u00b6 To grap a single column 1 2 3 4 5 6 7 print ( df [ 'W' ]) #A 2 #B 6 #C 2 #D 16 #E 30 #Name: W, dtype: int64 Grap multiple Columns 1 2 3 4 5 6 7 print [[ 'W' , 'Z' ]] # W Z # A 2 -86 # B 6 -80 # C 2 -13 # D 16 51 # E 30 -99 If we use the method type() we can see that the columns are just pandas series 1 2 print ( type ( df [ 'W' ])) # pandas.core.series.Series Create a new column \u00b6 1 2 df [ \"new\" ] = df [ 'W' ] + df [ 'Y' ] print ( df [ 'new' ]) Removing Columns \u00b6 for the removing of a column is important to understand that if it is not reassigned (df = de.drop()) the removal wont be save, see in the example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # axis=1 because its a column print ( df . drop ( 'new' , axis = 1 )) # W X Y Z # A 2 79 -8 -86 # B 6 -29 88 -80 # C 2 21 -26 -13 # D 16 -1 3 51 # E 30 49 -48 -99 print ( df ) # W X Y Z new # A 2 79 -8 -86 -6 # B 6 -29 88 -80 94 # C 2 21 -26 -13 -24 # D 16 -1 3 51 19 # E 30 49 -48 -99 -18 df = df . drop ( 'new' , axis = 1 ) Working with Rows \u00b6 Now to select a row we will need to use a bit different approach, in this case we will need to use df.loc[] we use loc and the name or number of the row. 1 2 3 4 5 6 print ( df . loc [ 'A' ]) #W 2 #X 79 #Y -8 #Z -86 #Name: A, dtype: int64 The selection by numerical index will be 1 2 3 4 5 6 print ( df . iloc [ 0 ]) #W 2 #X 79 #Y -8 #Z -86 #Name: A, dtype: int32 Multi-row selection \u00b6 Similar with columns we can select multiple rows at the same time 1 2 3 4 print ( df . loc [[ 'A' . 'C' ]]) # W X Y Z # A 2 79 -8 -86 # C 2 21 -26 -13 now by numerical index 1 2 3 4 5 print ( df . iloc [ 0 : 2 ]) # W X Y Z # A 2 79 -8 -86 # B 6 -29 88 -80 Now we can select a subset of rows and columns 1 2 3 4 print ( df . loc [[ 'A' , 'C' ],[ 'W' , 'Y' ]]) # W Y # A 2 -8 # C 2 -26 Removing a row \u00b6 To remove a row we use drop() but this time the parameter axis=0 , and in the same way with columns, if this is not reassigned the removal wont take place. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 print ( df . drop ( 'C' , axis = 0 )) # W X Y Z # A 2 79 -8 -86 # B 6 -29 88 -80 # D 16 -1 3 51 # E 30 49 -48 -99 print ( df ) # W X Y Z # A 2 79 -8 -86 # B 6 -29 88 -80 # C 2 21 -26 -13 # D 16 -1 3 51 # E 30 49 -48 -99 Conditional Selection \u00b6 Pandas allow the conditional selection similar to NumPy, for the examples of this we will use the DataFrame: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 print ( df > 0 ) # W X Y Z # A True True False False # B True False True False # C True True False False # D True False True True # E True True False False print ( df [ df > 0 ]) # W X Y Z # A 2 79.0 NaN NaN # B 6 NaN 88.0 NaN # C 2 21.0 NaN NaN # D 16 NaN 3.0 51.0 # E 30 49.0 NaN NaN More examples 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 print ( df [ 'X' ] > 0 ) #A True #B False #C True #D False #E True #Name: X, dtype: bool print ( df [ df [ 'X' ] > 0 ]) # W X Y Z #A 2 79 -8 -86 #C 2 21 -26 -13 #E 30 49 -48 -99 print ( df [ df [ 'X' ] > 0 ][ 'Y' ]) #A -8 #C -26 #E -48 #Name: Y, dtype: int64 print ( df [ df [ 'X' ] > 0 ][[ 'Y' , 'Z' ]]) # Y Z #A -8 -86 #C -26 -13 #E -48 -99 If we want to use more than one conditional we can us binary operator like \u201c|\u201d or \u201c&\u201d. 1 2 3 4 print ( df [( df [ 'W' ] > 0 ) & ( df [ 'Y' ] > 1 )]) # W X Y Z #B 6 -29 88 -80 #D 16 -1 3 51 More about index \u00b6 We can reset the index, this means change the index selected ( in this case the letters A to E) for the number (starting in 0), lit copy if this change is not reassigned it wont take place. Again we start with 1 2 3 4 5 6 7 8 9 # Reset to default 0,1...n index df . reset_index () # index W X Y Z #0 A 2 79 -8 -86 #1 B 6 -29 88 -80 #2 C 2 21 -26 -13 #3 D 16 -1 3 51 #4 E 30 49 -48 -99 now we can create new index, in the following example we will create a new index starting from a string 1 2 3 newid = 'CA NY WY OR CO' . split () df [ 'states' ] = newind df = df . set_index ( 'states' ) Summaries \u00b6 THere are 3 methods we can use to get information about the data in the DataFrames they are: describe() which will give a statistic description of the values 1 df . describe () info() overall description of what is in the table 1 df . info () dtypes() to display the data type Missing Data \u00b6 There are some methods in POandas that allow you to handle missing data in the DataFrames or series, First lets create a new DataFrame with missing data 1 2 3 4 5 6 import numpy as np import pandas as pd df = pd . DataFrame ({ 'A' :[ 1 , 2 , np , nan , 4 ], 'B' :[ 5 , np , nan , np , nan , 8 ], 'C' :[ 10 , 20 , 30 , 40 ]}) Removing missing data \u00b6 To remove horizontally 1 2 3 4 df . dropna () # A B C #0 1.0 5.0 10 #3 4.0 8.0 40 To do it Vertically 1 2 3 4 5 6 df . dropna ( axis = 1 ) # C # 0 10 # 1 20 # 2 30 # 3 40 Threshold \u00b6 We can set a threshold for each column or row, if the row or column has equal or more of specific number of non-NaN 1 2 3 4 5 df . dropna ( thresh = 2 ) # A B C #0 1.0 5.0 10 #1 2.0 NaN 20 #3 4.0 8.0 40 Filling missing data \u00b6 Now to fill the empty values we can use fillna(value=0) using as a argument for the parameter value either a string of an integer 1 2 3 4 5 6 7 df . fillna ( value = 'FILL VALUE' ) # A B C #0 1 5 10 #1 2 FILL VALUE 20 #2 FILL VALUE FILL VALUE 30 #3 4 8 40 1 2 3 4 5 6 7 df [ 'A' ] . fillna ( value = 0 ) #0 1.0 #1 2.0 #2 0.0 #3 4.0 #Name: A, dtype: float64 1 2 3 4 5 6 7 df [ 'A' ] . fillna ( df [ 'A' ] . mean ()) #0 1.000000 #1 2.000000 #2 2.333333 #3 4.000000 #Name: A, dtype: float64 Groupby \u00b6 the Method groupby() allow me to group different rows and call other functions ( aggregate functions) first we will need to choose the column that we are going to use as a categorical column, which is the same that we are going to use with groupby , Second choose the aggregated function (e.g. max,min, mean, std, etc\u2026) let make and example, we are going to read a file called \u201cUniversities.csv\u201d and use the column \u2018Year\u2019 to group. 1 2 3 4 5 6 7 import pandas as pd #read the csv file df = pd . read_csv ( 'Universities.cvs' ) #now we can show the first rows df . head () 1 2 3 4 5 6 7 #now group by Year df . groupby ( 'Year' ) #<pandas.core.groupby.generic.DataFrameGroupBy object at 0x00000220102EC048> # the groupby will be a DataFrame # we can use the the aggregated df . groupby ( 'Year' ), mean () As well as the groupby the result of df.groupby('Year').mean() will be a DataFrame as well. we can sort the results of mean() 1 df . groupby ( 'Year' ) . mean () . sort_index ( ascending = False ) Groupby by multiple columns \u00b6 We can make the grouping by multiple columns we just need to 1 df . groupby ([ 'Year' , 'Sector' ]) . mean () Now we are going to use describe() which will give use information about this dataFrame, but we will combine it with transpose() so we can change the columns for the rows and display it in a more readable way ( we are going to use just groupby with \u201cYear\u201d) 1 df . groupby ( 'Year' ) . describe () . transpose () Operations \u00b6 There are a set of operation that are useful but don\u2019t fall in a specific category for the next operation we will create the following DataFrame: 1 2 3 import pandas as pd df_one = pd . DataFrame ({ 'k1' :[ 'A' , 'A' , 'B' , 'B' , 'C' , 'C' ], 'col1' :[ 100 , 200 , 300 , 300 , 400 , 500 ], 'col2' :[ 'NY' , 'CA' , 'WA' , 'WA' , 'AK' , 'NV' ]}) Unique Values \u00b6 Get the unique values. 1 2 df_one [ 'col2' ] . unique () # array(['NY', 'CA', 'WA', 'AK', 'NV'], dtype=object) Get number of unique values. 1 2 df_one [ 'col2' ] . nunique () # 5 Count and Get the number of times a value is repeated 1 2 3 4 5 6 df_one [ 'col2' ] . value_counts () # WA 2 # CA 1 # NV 1 # NY 1 # AK 1 To remove duplicated rows 1 df_one [ 'col2' ] . drop_duplicates () Create new Columns with Operations and Functions \u00b6 1 def_one [ 'New col' ] = df_one [ 'col1' ] * 10 or we can use the method apply() and pass a customize function as a parameter 1 2 3 4 5 def grab_first_letter ( state ): return state [ 0 ] df_one [ 'first letter' ] = df_one [ 'col2' ] . apply ( grab_first_letter ) Be aware that we are not making a call of the function grab_first_letter() , there is not \u201c()\u201d. Mapping \u00b6 now we are going to mapped some value to other lets check the column \u201ck1\u201d 1 df_one [ 'k1' ] . map ({ 'A' : 1 , 'B' : 2 , 'C' : 3 }) Sorting, min, max, columns and index \u00b6 to get the max, min or their positions we use: To find max value: df_one['col1'].max() The index of the max value: df_one['col1'].idxmax() To find min value: df_one['col1'].min() The index of the min value: df_one['col1'].idxmin() Now for sorting: df_one.sort_values('col2') Concatenate DataFrames and Dummy variables \u00b6 to concatenate we need to be careful in which axis we want to work with, 1 2 3 4 5 features = pd . DataFrame ({ 'A' :[ 100 , 200 , 300 , 400 , 500 ], 'B' :[ 12 , 13 , 14 , 15 , 16 ]}) predictions = pd . DataFrame ({ 'pred' :[ 0 , 1 , 1 , 0 , 1 ]}) pd . concat ([ features , predictions ]) 1 pd . concat ([ features , predictions ], axis = 1 ) for the dummy variables, first, we are going to change the name of the columns 1 df_one . columns = [ 'C1' , 'C2' , 'C3' , 'C4' , 'C5' , 'C6' ] later we can make the dummy variable 1 pd . get_dummies ( df_one [ 'C1' ]) Data input and Data Output \u00b6 for more detailed information of the different type of files and please visit the pandas documentation CSV \u00b6 To be able to read this type of files we need two additional libraries: xlrd ( pip install xlrd ). openpyxl ( pip install openpyxl ). for reading CSV: 1 df = pd . read_csv ( 'example.csv' ) for the Output 1 df = df . to_csv ( 'example.csv' , index = False ) HTML \u00b6 Pandas is able to read table tabs of the HTML (if the firewall allow pandas to read the HTML) but it is necessary to add the following libraries: lxml ( pip install lxml ). html5lib ( pip install html5lib ). beautifulsoap4 ( pip install beautifulsoap4 ). for the input, the read_html function willl read tables of the webpage and return a list of DataFrame object: 1 2 tables = pd . read_html ( 'http://www.fdic.gov/bank/individual/failed/banklist.html' ) tables [ 0 ] . head ()","title":"Pandas Crash course"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#series","text":"Series are similar to Numpy arrays ( they are build on top of Numpy arrays) and the difference is that SEris can have axis labels, that means that can be located not just by numbers but by labels, and they can hold much more than just numbers, they can hold any type of python object. We are going to create Series in different ways:","title":"Series"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#creating_a_series","text":"First we will need to import numpy and pandas 1 2 import numpy as np import pandas as pd now we are going to create a list, a numpy array and a dictionary that later will be use to create the series 1 2 3 4 lables = [ 'a' , 'b' , 'c' ] my_list = [ 10 , 20 , 30 ] arr = np . array ([ 10 , 20 , 30 ]) d = { 'a' : 10 , 'b' : 20 , 'c' : 30 }","title":"Creating a series."},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#1_using_list","text":"1 2 3 4 5 pd . Series ( data = mylist ) #0 10 #1 20 #2 30 #dtype: int64 The about is a Series that use just number as labels, now let use the labels defined before with the list lables 1 2 3 4 5 pd . Series ( data = my_list , index = labels ) #a 10 #b 20 #c 30 #dtype: int64 now a shorter way 1 2 3 4 5 pd . Series ( my_list , labels ) #a 10 #b 20 #c 30 #dtype: int64","title":"1. Using List"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#2_using_numpy_arrays","text":"1 2 3 4 5 pd . Series ( arr ) #0 10 #1 20 #2 30 #dtype: int64 now with the labels 1 2 3 4 5 pd . Series ( arr , labels ) #a 10 #b 20 #c 30 #dtype: int64","title":"2. Using NumPy arrays"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#3_using_dictionaries","text":"1 2 3 4 5 pd . Series ( d ) #a 10 #b 20 #c 30 #dtype: int64","title":"3. Using Dictionaries"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#using_index","text":"Pandas use the index name or numbers which allow to access the information ( the index are the rows). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 sales_Q1 = pd . Series ( data = [ 250 , 450 , 200 , 150 ], index = [ 'USA' , 'China' , 'India' , 'Brazil' ]) Sales_Q1 #USA 250 #China 450 #India 200 #Brazil 150 #dtype: int64 sales_Q2 = pd . Series ([ 260 , 500 , 210 , 100 ], index = [ 'USA' , 'China' , 'India' , 'Japan' ]) sales_Q2 #USA 260 #China 500 #India 210 #Japan 100 #dtype: int64 sales_Q1 [ 'USA' ] #250 sales_Q1 + sales_Q2 #Brazil NaN #China 950.0 #India 410.0 #Japan NaN #USA 510.0 #dtype: float64","title":"Using Index"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#dataframes","text":"Datafarames are inspire in R programming they look like a group of series put together to share the same index. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import pandas as pd import numpy as np from numpy.random import randint # define columns and index to use later in the DataFrame columns = [ 'W' , 'X' , 'Y' , 'Z' ] # four columns index = [ 'A' , 'B' , 'C' , 'D' , 'E' ] # five rows np . random . seed ( 42 ) data = randint ( - 100 , 100 ,( 5 , 4 )) # randint(low,high, size) print ( data ) #array([[ 2, 79, -8, -86], # [ 6, -29, 88, -80], # [ 2, 21, -26, -13], # [ 16, -1, 3, 51], # [ 30, 49, -48, -99]]) df = pd . DataFrame ( data , index , columns ) print ( df )","title":"DataFrames"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#selection_and_indexing","text":"We can select and grad columns or parts of the DataFrame","title":"Selection and indexing"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#columns","text":"To grap a single column 1 2 3 4 5 6 7 print ( df [ 'W' ]) #A 2 #B 6 #C 2 #D 16 #E 30 #Name: W, dtype: int64 Grap multiple Columns 1 2 3 4 5 6 7 print [[ 'W' , 'Z' ]] # W Z # A 2 -86 # B 6 -80 # C 2 -13 # D 16 51 # E 30 -99 If we use the method type() we can see that the columns are just pandas series 1 2 print ( type ( df [ 'W' ])) # pandas.core.series.Series","title":"Columns"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#create_a_new_column","text":"1 2 df [ \"new\" ] = df [ 'W' ] + df [ 'Y' ] print ( df [ 'new' ])","title":"Create a new column"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#removing_columns","text":"for the removing of a column is important to understand that if it is not reassigned (df = de.drop()) the removal wont be save, see in the example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # axis=1 because its a column print ( df . drop ( 'new' , axis = 1 )) # W X Y Z # A 2 79 -8 -86 # B 6 -29 88 -80 # C 2 21 -26 -13 # D 16 -1 3 51 # E 30 49 -48 -99 print ( df ) # W X Y Z new # A 2 79 -8 -86 -6 # B 6 -29 88 -80 94 # C 2 21 -26 -13 -24 # D 16 -1 3 51 19 # E 30 49 -48 -99 -18 df = df . drop ( 'new' , axis = 1 )","title":"Removing Columns"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#working_with_rows","text":"Now to select a row we will need to use a bit different approach, in this case we will need to use df.loc[] we use loc and the name or number of the row. 1 2 3 4 5 6 print ( df . loc [ 'A' ]) #W 2 #X 79 #Y -8 #Z -86 #Name: A, dtype: int64 The selection by numerical index will be 1 2 3 4 5 6 print ( df . iloc [ 0 ]) #W 2 #X 79 #Y -8 #Z -86 #Name: A, dtype: int32","title":"Working with Rows"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#multi-row_selection","text":"Similar with columns we can select multiple rows at the same time 1 2 3 4 print ( df . loc [[ 'A' . 'C' ]]) # W X Y Z # A 2 79 -8 -86 # C 2 21 -26 -13 now by numerical index 1 2 3 4 5 print ( df . iloc [ 0 : 2 ]) # W X Y Z # A 2 79 -8 -86 # B 6 -29 88 -80 Now we can select a subset of rows and columns 1 2 3 4 print ( df . loc [[ 'A' , 'C' ],[ 'W' , 'Y' ]]) # W Y # A 2 -8 # C 2 -26","title":"Multi-row selection"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#removing_a_row","text":"To remove a row we use drop() but this time the parameter axis=0 , and in the same way with columns, if this is not reassigned the removal wont take place. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 print ( df . drop ( 'C' , axis = 0 )) # W X Y Z # A 2 79 -8 -86 # B 6 -29 88 -80 # D 16 -1 3 51 # E 30 49 -48 -99 print ( df ) # W X Y Z # A 2 79 -8 -86 # B 6 -29 88 -80 # C 2 21 -26 -13 # D 16 -1 3 51 # E 30 49 -48 -99","title":"Removing a row"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#conditional_selection","text":"Pandas allow the conditional selection similar to NumPy, for the examples of this we will use the DataFrame: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 print ( df > 0 ) # W X Y Z # A True True False False # B True False True False # C True True False False # D True False True True # E True True False False print ( df [ df > 0 ]) # W X Y Z # A 2 79.0 NaN NaN # B 6 NaN 88.0 NaN # C 2 21.0 NaN NaN # D 16 NaN 3.0 51.0 # E 30 49.0 NaN NaN More examples 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 print ( df [ 'X' ] > 0 ) #A True #B False #C True #D False #E True #Name: X, dtype: bool print ( df [ df [ 'X' ] > 0 ]) # W X Y Z #A 2 79 -8 -86 #C 2 21 -26 -13 #E 30 49 -48 -99 print ( df [ df [ 'X' ] > 0 ][ 'Y' ]) #A -8 #C -26 #E -48 #Name: Y, dtype: int64 print ( df [ df [ 'X' ] > 0 ][[ 'Y' , 'Z' ]]) # Y Z #A -8 -86 #C -26 -13 #E -48 -99 If we want to use more than one conditional we can us binary operator like \u201c|\u201d or \u201c&\u201d. 1 2 3 4 print ( df [( df [ 'W' ] > 0 ) & ( df [ 'Y' ] > 1 )]) # W X Y Z #B 6 -29 88 -80 #D 16 -1 3 51","title":"Conditional Selection"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#more_about_index","text":"We can reset the index, this means change the index selected ( in this case the letters A to E) for the number (starting in 0), lit copy if this change is not reassigned it wont take place. Again we start with 1 2 3 4 5 6 7 8 9 # Reset to default 0,1...n index df . reset_index () # index W X Y Z #0 A 2 79 -8 -86 #1 B 6 -29 88 -80 #2 C 2 21 -26 -13 #3 D 16 -1 3 51 #4 E 30 49 -48 -99 now we can create new index, in the following example we will create a new index starting from a string 1 2 3 newid = 'CA NY WY OR CO' . split () df [ 'states' ] = newind df = df . set_index ( 'states' )","title":"More about index"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#summaries","text":"THere are 3 methods we can use to get information about the data in the DataFrames they are: describe() which will give a statistic description of the values 1 df . describe () info() overall description of what is in the table 1 df . info () dtypes() to display the data type","title":"Summaries"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#missing_data","text":"There are some methods in POandas that allow you to handle missing data in the DataFrames or series, First lets create a new DataFrame with missing data 1 2 3 4 5 6 import numpy as np import pandas as pd df = pd . DataFrame ({ 'A' :[ 1 , 2 , np , nan , 4 ], 'B' :[ 5 , np , nan , np , nan , 8 ], 'C' :[ 10 , 20 , 30 , 40 ]})","title":"Missing Data"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#removing_missing_data","text":"To remove horizontally 1 2 3 4 df . dropna () # A B C #0 1.0 5.0 10 #3 4.0 8.0 40 To do it Vertically 1 2 3 4 5 6 df . dropna ( axis = 1 ) # C # 0 10 # 1 20 # 2 30 # 3 40","title":"Removing missing data"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#threshold","text":"We can set a threshold for each column or row, if the row or column has equal or more of specific number of non-NaN 1 2 3 4 5 df . dropna ( thresh = 2 ) # A B C #0 1.0 5.0 10 #1 2.0 NaN 20 #3 4.0 8.0 40","title":"Threshold"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#filling_missing_data","text":"Now to fill the empty values we can use fillna(value=0) using as a argument for the parameter value either a string of an integer 1 2 3 4 5 6 7 df . fillna ( value = 'FILL VALUE' ) # A B C #0 1 5 10 #1 2 FILL VALUE 20 #2 FILL VALUE FILL VALUE 30 #3 4 8 40 1 2 3 4 5 6 7 df [ 'A' ] . fillna ( value = 0 ) #0 1.0 #1 2.0 #2 0.0 #3 4.0 #Name: A, dtype: float64 1 2 3 4 5 6 7 df [ 'A' ] . fillna ( df [ 'A' ] . mean ()) #0 1.000000 #1 2.000000 #2 2.333333 #3 4.000000 #Name: A, dtype: float64","title":"Filling missing data"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#groupby","text":"the Method groupby() allow me to group different rows and call other functions ( aggregate functions) first we will need to choose the column that we are going to use as a categorical column, which is the same that we are going to use with groupby , Second choose the aggregated function (e.g. max,min, mean, std, etc\u2026) let make and example, we are going to read a file called \u201cUniversities.csv\u201d and use the column \u2018Year\u2019 to group. 1 2 3 4 5 6 7 import pandas as pd #read the csv file df = pd . read_csv ( 'Universities.cvs' ) #now we can show the first rows df . head () 1 2 3 4 5 6 7 #now group by Year df . groupby ( 'Year' ) #<pandas.core.groupby.generic.DataFrameGroupBy object at 0x00000220102EC048> # the groupby will be a DataFrame # we can use the the aggregated df . groupby ( 'Year' ), mean () As well as the groupby the result of df.groupby('Year').mean() will be a DataFrame as well. we can sort the results of mean() 1 df . groupby ( 'Year' ) . mean () . sort_index ( ascending = False )","title":"Groupby"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#groupby_by_multiple_columns","text":"We can make the grouping by multiple columns we just need to 1 df . groupby ([ 'Year' , 'Sector' ]) . mean () Now we are going to use describe() which will give use information about this dataFrame, but we will combine it with transpose() so we can change the columns for the rows and display it in a more readable way ( we are going to use just groupby with \u201cYear\u201d) 1 df . groupby ( 'Year' ) . describe () . transpose ()","title":"Groupby by multiple columns"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#operations","text":"There are a set of operation that are useful but don\u2019t fall in a specific category for the next operation we will create the following DataFrame: 1 2 3 import pandas as pd df_one = pd . DataFrame ({ 'k1' :[ 'A' , 'A' , 'B' , 'B' , 'C' , 'C' ], 'col1' :[ 100 , 200 , 300 , 300 , 400 , 500 ], 'col2' :[ 'NY' , 'CA' , 'WA' , 'WA' , 'AK' , 'NV' ]})","title":"Operations"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#unique_values","text":"Get the unique values. 1 2 df_one [ 'col2' ] . unique () # array(['NY', 'CA', 'WA', 'AK', 'NV'], dtype=object) Get number of unique values. 1 2 df_one [ 'col2' ] . nunique () # 5 Count and Get the number of times a value is repeated 1 2 3 4 5 6 df_one [ 'col2' ] . value_counts () # WA 2 # CA 1 # NV 1 # NY 1 # AK 1 To remove duplicated rows 1 df_one [ 'col2' ] . drop_duplicates ()","title":"Unique Values"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#create_new_columns_with_operations_and_functions","text":"1 def_one [ 'New col' ] = df_one [ 'col1' ] * 10 or we can use the method apply() and pass a customize function as a parameter 1 2 3 4 5 def grab_first_letter ( state ): return state [ 0 ] df_one [ 'first letter' ] = df_one [ 'col2' ] . apply ( grab_first_letter ) Be aware that we are not making a call of the function grab_first_letter() , there is not \u201c()\u201d.","title":"Create new Columns with Operations and Functions"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#mapping","text":"now we are going to mapped some value to other lets check the column \u201ck1\u201d 1 df_one [ 'k1' ] . map ({ 'A' : 1 , 'B' : 2 , 'C' : 3 })","title":"Mapping"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#sorting_min_max_columns_and_index","text":"to get the max, min or their positions we use: To find max value: df_one['col1'].max() The index of the max value: df_one['col1'].idxmax() To find min value: df_one['col1'].min() The index of the min value: df_one['col1'].idxmin() Now for sorting: df_one.sort_values('col2')","title":"Sorting, min, max, columns and index"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#concatenate_dataframes_and_dummy_variables","text":"to concatenate we need to be careful in which axis we want to work with, 1 2 3 4 5 features = pd . DataFrame ({ 'A' :[ 100 , 200 , 300 , 400 , 500 ], 'B' :[ 12 , 13 , 14 , 15 , 16 ]}) predictions = pd . DataFrame ({ 'pred' :[ 0 , 1 , 1 , 0 , 1 ]}) pd . concat ([ features , predictions ]) 1 pd . concat ([ features , predictions ], axis = 1 ) for the dummy variables, first, we are going to change the name of the columns 1 df_one . columns = [ 'C1' , 'C2' , 'C3' , 'C4' , 'C5' , 'C6' ] later we can make the dummy variable 1 pd . get_dummies ( df_one [ 'C1' ])","title":"Concatenate DataFrames and Dummy variables"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#data_input_and_data_output","text":"for more detailed information of the different type of files and please visit the pandas documentation","title":"Data input and Data Output"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#csv","text":"To be able to read this type of files we need two additional libraries: xlrd ( pip install xlrd ). openpyxl ( pip install openpyxl ). for reading CSV: 1 df = pd . read_csv ( 'example.csv' ) for the Output 1 df = df . to_csv ( 'example.csv' , index = False )","title":"CSV"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Pandas Crash course.html#html","text":"Pandas is able to read table tabs of the HTML (if the firewall allow pandas to read the HTML) but it is necessary to add the following libraries: lxml ( pip install lxml ). html5lib ( pip install html5lib ). beautifulsoap4 ( pip install beautifulsoap4 ). for the input, the read_html function willl read tables of the webpage and return a list of DataFrame object: 1 2 tables = pd . read_html ( 'http://www.fdic.gov/bank/individual/failed/banklist.html' ) tables [ 0 ] . head ()","title":"HTML"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Visualization_Crash_course.html","text":"Here we mentioned the very basics for the visualization tools, just enough to understand how Pandas plotting and Seaborn are built on top of Matplotlib. Matplotlib \u00b6 It is common to create an alias for Matplotlib as plt and that will in this way: 1 import matplotlib.pyplot as plt Now, since Jupyter notebooks is the most common tool it is important to mentione that we need to add an extra line after importing matplotlib.pyplot. so a common import session of a file will look like: 1 2 3 4 import numpy as np import pandas as pd import matplotlib , pyplot as plt Simple plot \u00b6 To simple plot we can use plot(x,y) but in jupyter notbooks we can add a \u201c;\u201d at the end so the matplotlib text wont be display 1 2 3 4 5 6 7 8 9 10 import numpy as np import pandas as pd import matplotlib , pyplot as plt x = [ 0 , 1 , 2 ] y = [ 100 , 200 , 300 ] plt . plot ( x , y ) #plt.plot(x,y); in a .py file we will need to add plt.show() in order to see the graph we will create a DataFrame that we can use to plot 1 2 housing = pd . DataFrame ({ 'rooms' :[ 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ], 'price' :[ 100 , 120 , 190 , 200 , 230 , 310 , 330 , 305 ]}) If we use the normal plot this will display some straight line but if we use the scatter we will have dots in the x and Y points 1 plt . scatter ( housing [ 'rooms' ], housing [ 'price' ]) Adding title and name to the axis \u00b6 we draw the plot plt.plot(x,y) we put the title plt.title('title') we name the axis plt.xlabel('X Label'), plt.ylabel('Y Label') 1 2 3 4 plt . plot ( x , y ) plt . title ( 'Title' ) plt . xlabel ( 'X Label' ) plt . ylabel ( 'Y Label' ) Adding limits or changing the axis scale \u00b6 We can limit or expand the limit of the graphic, in this case we want the previous plot axis to start 0 for X and 100 for Y and finish at 2 for X and 30 for Y. 1 2 3 4 5 6 7 8 9 10 ptl . plot ( x , y ) #axis and ticks plt . xlim ( 0 , 2 ) plt . ylin ( 100 , 300 ) #labeling plt . title ( 'Title' ) plt . xlabel ( 'X Label' ) plt . ylabel ( 'Y Label' ) Changing the markers \u00b6 We can change the color and style of the line, but also we can change the style of the markets 1 2 3 4 5 6 7 8 9 10 ptl . plot ( x , y , color = 'red' , marker = 'o' , markersize = 20 , linestyle = '--' ) # Axis and Ticks plt . xlim ( 0 , 20 ) plt . ylim ( 100 , 300 ) #labels plt . title ( 'Title' ) plt . xlabel ( 'X label' ) plt . ylabel ( 'y label' ) Seaborn \u00b6 Seaborn is a library on top of Matplotlib that allow the creation of different charts and graphics with less code for the examples we will use a Csv file 1 2 3 4 5 6 import pandas as pd import matplotlib.pyplot as plt import seaborn as sns df = pd . read_csv ( '../DATA/heart.csv' ) df . head () Distribution plots \u00b6 1 sns . displot ( df [ 'age' ]) Resizing and modify seaborn plots \u00b6 for resizing 1 2 plt . figure ( figsize = ( 12 , 8 )) sns . displot ( df [ 'age' ]) to remove the KDE (Kernel Density Estimates) 1 sns . distplot ( df [ 'age' ], kde = False ) similar to remove the histogram 1 sns . distplot ( df [ 'age' ], hist = False ) to change the color 1 sns . distplot ( df [ 'age' ], kde = False , bins = 40 , color = 'red' ) we can limit the axis in seaborn as we limit the axis in matplotlib 1 2 sns . distplot ( df [ 'age' ], kde = False , bins = 40 , color = 'green' ) plt . xlim ( 50 , 60 ) Count plot \u00b6 From the same csv file 1 sns . countplot ( x = 'sex' , data = df ) 1 sns . countplot ( x = 'cp' , data = df ) and we can use hue to add more information 1 sns . countplot ( x = 'cp' , data = df , hue = 'sex' ) we can change the color ( there are predefine color colormaps ) 1 sns . countplot ( x = 'cp' , data = df , palette = 'terrain' ) Box Plot \u00b6 1 sns . boxplot ( x = 'target' , y = 'thalach' , data = df ) and in the same way that with count plots we can use the hue to add more information to the Box plot 1 sns . boxplot ( x = 'target' , y = 'thalach' , data = df , hue = 'sex' ) Scatter plot \u00b6 This kind of plot is use to display the relationship between two continuous features we can use the hue and size to add extra dimension, and use palette to change the color more info 1 sns . scatterplot ( x = 'chol' , y = 'trestbps' , data = df , hue = 'sex' , size = 'age' ) Pairplots \u00b6 Pairplots perform scatterplots and histograms for every single column in your data set. This means it could be a huge plot for large datasets! Use with caution, as it could take a long time for large datasets and the figures could be too small! more info 1 2 iris = pd . read_csv ( '../DATA/iris.csv' ) iris . head () 1 sns . pairplot ( iris ) or just show the KDEs instead of histograms 1 sns . pairplot ( iris , hue = \"species\" )","title":"Visualization Crash course"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Visualization_Crash_course.html#matplotlib","text":"It is common to create an alias for Matplotlib as plt and that will in this way: 1 import matplotlib.pyplot as plt Now, since Jupyter notebooks is the most common tool it is important to mentione that we need to add an extra line after importing matplotlib.pyplot. so a common import session of a file will look like: 1 2 3 4 import numpy as np import pandas as pd import matplotlib , pyplot as plt","title":"Matplotlib"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Visualization_Crash_course.html#simple_plot","text":"To simple plot we can use plot(x,y) but in jupyter notbooks we can add a \u201c;\u201d at the end so the matplotlib text wont be display 1 2 3 4 5 6 7 8 9 10 import numpy as np import pandas as pd import matplotlib , pyplot as plt x = [ 0 , 1 , 2 ] y = [ 100 , 200 , 300 ] plt . plot ( x , y ) #plt.plot(x,y); in a .py file we will need to add plt.show() in order to see the graph we will create a DataFrame that we can use to plot 1 2 housing = pd . DataFrame ({ 'rooms' :[ 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ], 'price' :[ 100 , 120 , 190 , 200 , 230 , 310 , 330 , 305 ]}) If we use the normal plot this will display some straight line but if we use the scatter we will have dots in the x and Y points 1 plt . scatter ( housing [ 'rooms' ], housing [ 'price' ])","title":"Simple plot"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Visualization_Crash_course.html#adding_title_and_name_to_the_axis","text":"we draw the plot plt.plot(x,y) we put the title plt.title('title') we name the axis plt.xlabel('X Label'), plt.ylabel('Y Label') 1 2 3 4 plt . plot ( x , y ) plt . title ( 'Title' ) plt . xlabel ( 'X Label' ) plt . ylabel ( 'Y Label' )","title":"Adding title and name to the axis"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Visualization_Crash_course.html#adding_limits_or_changing_the_axis_scale","text":"We can limit or expand the limit of the graphic, in this case we want the previous plot axis to start 0 for X and 100 for Y and finish at 2 for X and 30 for Y. 1 2 3 4 5 6 7 8 9 10 ptl . plot ( x , y ) #axis and ticks plt . xlim ( 0 , 2 ) plt . ylin ( 100 , 300 ) #labeling plt . title ( 'Title' ) plt . xlabel ( 'X Label' ) plt . ylabel ( 'Y Label' )","title":"Adding limits or changing the axis scale"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Visualization_Crash_course.html#changing_the_markers","text":"We can change the color and style of the line, but also we can change the style of the markets 1 2 3 4 5 6 7 8 9 10 ptl . plot ( x , y , color = 'red' , marker = 'o' , markersize = 20 , linestyle = '--' ) # Axis and Ticks plt . xlim ( 0 , 20 ) plt . ylim ( 100 , 300 ) #labels plt . title ( 'Title' ) plt . xlabel ( 'X label' ) plt . ylabel ( 'y label' )","title":"Changing the markers"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Visualization_Crash_course.html#seaborn","text":"Seaborn is a library on top of Matplotlib that allow the creation of different charts and graphics with less code for the examples we will use a Csv file 1 2 3 4 5 6 import pandas as pd import matplotlib.pyplot as plt import seaborn as sns df = pd . read_csv ( '../DATA/heart.csv' ) df . head ()","title":"Seaborn"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Visualization_Crash_course.html#distribution_plots","text":"1 sns . displot ( df [ 'age' ])","title":"Distribution plots"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Visualization_Crash_course.html#resizing_and_modify_seaborn_plots","text":"for resizing 1 2 plt . figure ( figsize = ( 12 , 8 )) sns . displot ( df [ 'age' ]) to remove the KDE (Kernel Density Estimates) 1 sns . distplot ( df [ 'age' ], kde = False ) similar to remove the histogram 1 sns . distplot ( df [ 'age' ], hist = False ) to change the color 1 sns . distplot ( df [ 'age' ], kde = False , bins = 40 , color = 'red' ) we can limit the axis in seaborn as we limit the axis in matplotlib 1 2 sns . distplot ( df [ 'age' ], kde = False , bins = 40 , color = 'green' ) plt . xlim ( 50 , 60 )","title":"Resizing and modify seaborn plots"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Visualization_Crash_course.html#count_plot","text":"From the same csv file 1 sns . countplot ( x = 'sex' , data = df ) 1 sns . countplot ( x = 'cp' , data = df ) and we can use hue to add more information 1 sns . countplot ( x = 'cp' , data = df , hue = 'sex' ) we can change the color ( there are predefine color colormaps ) 1 sns . countplot ( x = 'cp' , data = df , palette = 'terrain' )","title":"Count plot"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Visualization_Crash_course.html#box_plot","text":"1 sns . boxplot ( x = 'target' , y = 'thalach' , data = df ) and in the same way that with count plots we can use the hue to add more information to the Box plot 1 sns . boxplot ( x = 'target' , y = 'thalach' , data = df , hue = 'sex' )","title":"Box Plot"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Visualization_Crash_course.html#scatter_plot","text":"This kind of plot is use to display the relationship between two continuous features we can use the hue and size to add extra dimension, and use palette to change the color more info 1 sns . scatterplot ( x = 'chol' , y = 'trestbps' , data = df , hue = 'sex' , size = 'age' )","title":"Scatter plot"},{"location":"Courses/Udemy/Complete Tensorflow 2 and Keras Deep Learning Bootcamp/Visualization_Crash_course.html#pairplots","text":"Pairplots perform scatterplots and histograms for every single column in your data set. This means it could be a huge plot for large datasets! Use with caution, as it could take a long time for large datasets and the figures could be too small! more info 1 2 iris = pd . read_csv ( '../DATA/iris.csv' ) iris . head () 1 sns . pairplot ( iris ) or just show the KDEs instead of histograms 1 sns . pairplot ( iris , hue = \"species\" )","title":"Pairplots"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Image Processing/blending_pasting_images.html","text":"","title":"Blending and Pasting Images"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Image Processing/color_mapping.html","text":"With OpenCV we can use different colorspaces other than RGB, this is the case of HSL and HSV, that stand for Hue-saturation-lightness and hue-saturation-value. Here how HSL will look like and here how HSV will look like Now in in openCV we can transform or convert the different pictures to a different colorspace. Import Libraries \u00b6 First the imports 1 2 3 4 import cv2 import matplotlib.pyplot as plt import numpy as np % matplotlib inline # this line is just for jypiter lab or notebooks now with the imports we read or get the image we are going to use to work with 1 img = cv2 . imread ( '../DATA/00-puppy.jpg' ) Converting to Different Colorspaces \u00b6 now, we did this before, we convert from BGR to RGB 1 2 img = cv2 . ctvColor ( img , cv2 . COLOR_BGR2RGB ) plt . imshow ( img ) Convert to HSV \u00b6 now, first HSV 1 2 img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2HSV ) plt . imshow ( img ) Convert to HSL \u00b6 the next one will be HSL 1 2 3 img = cv2 . imread ( '../DATA/00-puppy.jpg' ) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2HLS ) plt . imshow ( img )","title":"Color Mapping"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Image Processing/color_mapping.html#import_libraries","text":"First the imports 1 2 3 4 import cv2 import matplotlib.pyplot as plt import numpy as np % matplotlib inline # this line is just for jypiter lab or notebooks now with the imports we read or get the image we are going to use to work with 1 img = cv2 . imread ( '../DATA/00-puppy.jpg' )","title":"Import Libraries"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Image Processing/color_mapping.html#converting_to_different_colorspaces","text":"now, we did this before, we convert from BGR to RGB 1 2 img = cv2 . ctvColor ( img , cv2 . COLOR_BGR2RGB ) plt . imshow ( img )","title":"Converting to Different Colorspaces"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Image Processing/color_mapping.html#convert_to_hsv","text":"now, first HSV 1 2 img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2HSV ) plt . imshow ( img )","title":"Convert to HSV"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Image Processing/color_mapping.html#convert_to_hsl","text":"the next one will be HSL 1 2 3 img = cv2 . imread ( '../DATA/00-puppy.jpg' ) img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2HLS ) plt . imshow ( img )","title":"Convert to HSL"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Direct_drawing_with_mouse.html","text":"In this case we are going to create and script that will allow me to use the mouse to draw different shapes. Connecting a Function for Drawing \u00b6 We will need to create the \u2018canvas\u2019, later connect the functions to support hte mouse and a way to close the window. We will build the script step by step. Basic structure \u00b6 We are going to import the libraries Create the \u2018canvas\u2019 to draw A loop to keep the window open and a way to close it Destroy all the windows 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import cv2 import numpy as np #Create the 'Canvas' black_img = np . zeros (( 512 , 512 , 3 ), np . uint8 ) #Create a loop to keep the windows open and a way to close it while True : if cv2 . waitKey ( 20 ) & 0xFF == 27 : break #Destroy all windows cv2 . destroyAllWindows () Giving a name to the window \u00b6 giving a name to the window show the window 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import cv2 import numpy as np black_img = np . zeros (( 512 , 512 , 3 ), np . uint8 ) #giving a name to the window cv2 . namedWindow ( winname = 'my_drawing' ) while True : #Showing the window cv2 . imshow ( 'my_drawing' , black_img ) if cv2 . waitKey ( 20 ) & 0xFF == 27 : break #Destroy all windows cv2 . destroyAllWindows () link the Window to the event and set the mouse callback \u00b6 Set the mouse callback Create the function called for that callback Define the event, or what is the result of the event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import cv2 import numpy as np # 2. Create the function def draw_circle ( event , x , y , flags , param ): # 3. Define de event if event == cv2 . EVENT_LBUTTONDOWN : cv2 . circle ( black_img ,( x , y ), 100 , color = ( 0 , 255 , 0 ), - 1 ) black_img = np . zeros (( 512 , 512 , 3 ), np . uint8 ) cv2 . namedWindow ( winname = 'my_drawing' ) # 1. set the mouse callback cv2 . setMouseCallback ( 'my_drawing' , draw_circle ) while True : cv2 . imshow ( 'my_drawing' , black_img ) if cv2 . waitKey ( 20 ) & 0xFF == 27 : break cv2 . destroyAllWindows () the result Adding functionality with Event Choices \u00b6 Now, we are going to use other event to provide additional functionality, in this case we are going to use the right button to make a circle with other color. Add a elif Use other event to draw a circle with other color 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import cv2 import numpy as np def draw_circle ( event , x , y , flags , param ): if event == cv2 . EVENT_LBUTTONDOWN : cv2 . circle ( black_img ,( x , y ), 100 ,( 0 , 255 , 0 ), - 1 ) # Define new event elif event == cv2 . EVENT_RBUTTONDOWN : cv2 . circle ( black_img ,( x , y ), 50 ,( 0 , 0 , 255 ), - 1 ) black_img = np . zeros (( 512 , 512 , 3 ), np . uint8 ) cv2 . namedWindow ( winname = 'my_drawing' ) cv2 . setMouseCallback ( 'my_drawing' , draw_circle ) while True : cv2 . imshow ( 'my_drawing' , black_img ) if cv2 . waitKey ( 20 ) & 0xFF == 27 : break cv2 . destroyAllWindows () Dragging with Mouse \u00b6 In this case we are going to use the rectangle, we will need to create some variable to keep track of the status of the \u2018drawing\u2019, in other words, when the user stop drawing, also some variable for the initial x x and y y points. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import cv2 import numpy as np #create a function base in the CV2 events drawing = False # true if the mouse is press ix , iy = - 1 , - 1 # this variable will keep track of the initial point def draw_rectangle ( event , x , y , flags , param ): global ix , iy , drawing , mode if event == cv2 . EVENT_LBUTTONDOWN : drawing = True ix , iy = x , y # with this event we will see the the rect getting bigger or smaller when we drag it elif event == cv2 . EVENT_MOUSEMOVE : if drawing == True : cv2 . rectangle ( black_img ,( ix , iy ),( x , y ),( 0 , 255 , 0 ), - 1 ) # here we finished drawing elif event == cv2 . EVENT_LBUTTONUP : drawing = False cv2 . rectangle ( black_img ,( ix , iy ),( x , y ),( 0 , 255 , 0 ), - 1 ) # Create the 'canvas' black_img = np . zeros (( 512 , 512 , 3 ), np . uint8 ) cv2 . namedWindow ( winname = 'my_drawing' ) cv2 . setMouseCallback ( 'my_drawing' , draw_rectangle ) while True : cv2 . imshow ( 'my_drawing' , black_img ) if cv2 . waitKey ( 1 ) & 0xFF == 27 : break cv2 . destroyAllWindows () The result is:","title":"Drawing with mouse"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Direct_drawing_with_mouse.html#connecting_a_function_for_drawing","text":"We will need to create the \u2018canvas\u2019, later connect the functions to support hte mouse and a way to close the window. We will build the script step by step.","title":"Connecting a Function for Drawing"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Direct_drawing_with_mouse.html#basic_structure","text":"We are going to import the libraries Create the \u2018canvas\u2019 to draw A loop to keep the window open and a way to close it Destroy all the windows 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import cv2 import numpy as np #Create the 'Canvas' black_img = np . zeros (( 512 , 512 , 3 ), np . uint8 ) #Create a loop to keep the windows open and a way to close it while True : if cv2 . waitKey ( 20 ) & 0xFF == 27 : break #Destroy all windows cv2 . destroyAllWindows ()","title":"Basic structure"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Direct_drawing_with_mouse.html#giving_a_name_to_the_window","text":"giving a name to the window show the window 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import cv2 import numpy as np black_img = np . zeros (( 512 , 512 , 3 ), np . uint8 ) #giving a name to the window cv2 . namedWindow ( winname = 'my_drawing' ) while True : #Showing the window cv2 . imshow ( 'my_drawing' , black_img ) if cv2 . waitKey ( 20 ) & 0xFF == 27 : break #Destroy all windows cv2 . destroyAllWindows ()","title":"Giving a name to the window"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Direct_drawing_with_mouse.html#link_the_window_to_the_event_and_set_the_mouse_callback","text":"Set the mouse callback Create the function called for that callback Define the event, or what is the result of the event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import cv2 import numpy as np # 2. Create the function def draw_circle ( event , x , y , flags , param ): # 3. Define de event if event == cv2 . EVENT_LBUTTONDOWN : cv2 . circle ( black_img ,( x , y ), 100 , color = ( 0 , 255 , 0 ), - 1 ) black_img = np . zeros (( 512 , 512 , 3 ), np . uint8 ) cv2 . namedWindow ( winname = 'my_drawing' ) # 1. set the mouse callback cv2 . setMouseCallback ( 'my_drawing' , draw_circle ) while True : cv2 . imshow ( 'my_drawing' , black_img ) if cv2 . waitKey ( 20 ) & 0xFF == 27 : break cv2 . destroyAllWindows () the result","title":"link the Window to the event and set the mouse callback"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Direct_drawing_with_mouse.html#adding_functionality_with_event_choices","text":"Now, we are going to use other event to provide additional functionality, in this case we are going to use the right button to make a circle with other color. Add a elif Use other event to draw a circle with other color 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import cv2 import numpy as np def draw_circle ( event , x , y , flags , param ): if event == cv2 . EVENT_LBUTTONDOWN : cv2 . circle ( black_img ,( x , y ), 100 ,( 0 , 255 , 0 ), - 1 ) # Define new event elif event == cv2 . EVENT_RBUTTONDOWN : cv2 . circle ( black_img ,( x , y ), 50 ,( 0 , 0 , 255 ), - 1 ) black_img = np . zeros (( 512 , 512 , 3 ), np . uint8 ) cv2 . namedWindow ( winname = 'my_drawing' ) cv2 . setMouseCallback ( 'my_drawing' , draw_circle ) while True : cv2 . imshow ( 'my_drawing' , black_img ) if cv2 . waitKey ( 20 ) & 0xFF == 27 : break cv2 . destroyAllWindows ()","title":"Adding functionality with Event Choices"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Direct_drawing_with_mouse.html#dragging_with_mouse","text":"In this case we are going to use the rectangle, we will need to create some variable to keep track of the status of the \u2018drawing\u2019, in other words, when the user stop drawing, also some variable for the initial x x and y y points. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import cv2 import numpy as np #create a function base in the CV2 events drawing = False # true if the mouse is press ix , iy = - 1 , - 1 # this variable will keep track of the initial point def draw_rectangle ( event , x , y , flags , param ): global ix , iy , drawing , mode if event == cv2 . EVENT_LBUTTONDOWN : drawing = True ix , iy = x , y # with this event we will see the the rect getting bigger or smaller when we drag it elif event == cv2 . EVENT_MOUSEMOVE : if drawing == True : cv2 . rectangle ( black_img ,( ix , iy ),( x , y ),( 0 , 255 , 0 ), - 1 ) # here we finished drawing elif event == cv2 . EVENT_LBUTTONUP : drawing = False cv2 . rectangle ( black_img ,( ix , iy ),( x , y ),( 0 , 255 , 0 ), - 1 ) # Create the 'canvas' black_img = np . zeros (( 512 , 512 , 3 ), np . uint8 ) cv2 . namedWindow ( winname = 'my_drawing' ) cv2 . setMouseCallback ( 'my_drawing' , draw_rectangle ) while True : cv2 . imshow ( 'my_drawing' , black_img ) if cv2 . waitKey ( 1 ) & 0xFF == 27 : break cv2 . destroyAllWindows () The result is:","title":"Dragging with Mouse"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Drawing_on_Images.html","text":"We can draw different things on the images, we can draw basic forms such as circle and rectangles, etc, we can put text and more complex figures all using cv2 First we will need the imports 1 2 3 4 5 import numpy as np import matplotlib.pyplot as plt # this parts is jut for jupyter lab % matplotlib inline import cv2 Create the \u2018Canvas\u2019 for drawing \u00b6 We can draw in any images, but in this case we will use a black canvas so it will be easy 1 2 3 4 black_img = np . zeros ( shape = ( 512 , 512 , 3 ), dtype = np . int16 ) black_img . shape # (512,512,3) plt . imshow ( black_img ) we are using the Numpy function to create an array of zeros, and since the shape is 512x512x3 this means that is an image of 512x512 with 3 channels of color, and all this colors are 0 0 , so if we display it will be a black image of 512x512. Shapes \u00b6 Rectangles \u00b6 the rectangle function will have some parameters: img: the image where we are going to draw pt1: the vertex of the rectangle, basically the top left corner pt2: the vertex opposite to pt1, means the lower right corner Color of rectangle: the color or brightness(in case of grayscale img) RGB format thickness: the thickness of the lines that make up the rectangle ( if you want to fill all the rectangle or figure you can use -1 as value) the are two parameters more, the linetype, and shift that we are not going to address here. 1 2 #pt1 top left , pt2 lower right corner cv2 . rectangle ( img = black_img , pt1 = ( 384 , 0 ), pt2 = ( 510 , 128 ), color = ( 0 , 255 , 0 ), thickness = 5 ) if we check the previous statement we will have a Numpy array, but if we use imshow we will have the image with the rectangle in the points we set up 1 cv2 . imshow ( black_img ) now one in the center of the canvas 1 2 cv2 . rectangle ( black_img , pt1 = ( 200 , 200 ), pt2 = ( 300 , 300 ), color = ( 0 , 0 , 255 ), thickness = 5 ) plt . imshow ( black_img ) Circle \u00b6 For the circle the parameter change from vertex to center point and radius, so to draw a circle 1 2 cv2 . circle ( img = black_img , center = ( 100 , 100 ), radius = 50 , color = ( 255 , 0 , 0 ), thickness = 5 ) plt . imshow ( black_img ) now let\u2019s change the value of thickness to -1 -1 so we filled in the circle 1 cv2 . circle ( img = black_img , center = ( 400 , 400 ), radius = 50 , color = ( 255 , 0 , 0 ), thickness =- 1 ) Lines \u00b6 We can create lines in a similar way we create the rectangle, in this case pt1 and pt2 are starting and ending point instead of vertex. 1 2 cv2 . line ( black_img , pt1 = ( 0 , 0 ), pt2 = ( 511 , 511 ), color = ( 102 , 255 , 255 ), thickness = 5 ) plt . imshow ( black_img ) Text \u00b6 now for the text we need first to select the font we are going to use, in this case we are limited for the fonts already in cv2 we have other arguments: text: the text we want to display org: the origin point, where the text will start fontFace: the font we are going to use fontScale: the size of the font linetype, for now we are going to use cv2.LINE_AA 1 2 3 font = cv2 . FONT_HERSHEY_SIMPLEX cv2 . putText ( balck_img , text = 'Hello' , org = ( 10 , 500 ), fontFace = font , fontScale = 4 , color = ( 255 , 255 , 255 ), thickness = 2 , lineType = cv2 . LINE_AA ) plt . imshow ( black_img ) Polygons \u00b6 For the polygons we will have some changes, first we will need to create an array with the vertex and later reshape this array to ROWSx1X2 and this should be a int32 array. the new parameter will be isClosed that we need to set as True to close the figure 1 2 3 4 5 black_img = np . zeros ( shape = ( 512 , 512 , 3 ), dtype = np . int32 ) vertices = np . array ([[ 100 , 300 ],[ 200 , 200 ],[ 400 , 300 ],[ 200 , 400 ]], np . int32 ) pts = vertices . reshape (( - 1 , 1 , 2 )) cv2 . polylines ( black_img ,[ pts ], isClosed = True , color = ( 255 , 0 , 0 ), thickness = 5 ) plt . imshow ( black_img )","title":"Drawing on Images"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Drawing_on_Images.html#create_the_canvas_for_drawing","text":"We can draw in any images, but in this case we will use a black canvas so it will be easy 1 2 3 4 black_img = np . zeros ( shape = ( 512 , 512 , 3 ), dtype = np . int16 ) black_img . shape # (512,512,3) plt . imshow ( black_img ) we are using the Numpy function to create an array of zeros, and since the shape is 512x512x3 this means that is an image of 512x512 with 3 channels of color, and all this colors are 0 0 , so if we display it will be a black image of 512x512.","title":"Create the 'Canvas' for drawing"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Drawing_on_Images.html#shapes","text":"","title":"Shapes"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Drawing_on_Images.html#rectangles","text":"the rectangle function will have some parameters: img: the image where we are going to draw pt1: the vertex of the rectangle, basically the top left corner pt2: the vertex opposite to pt1, means the lower right corner Color of rectangle: the color or brightness(in case of grayscale img) RGB format thickness: the thickness of the lines that make up the rectangle ( if you want to fill all the rectangle or figure you can use -1 as value) the are two parameters more, the linetype, and shift that we are not going to address here. 1 2 #pt1 top left , pt2 lower right corner cv2 . rectangle ( img = black_img , pt1 = ( 384 , 0 ), pt2 = ( 510 , 128 ), color = ( 0 , 255 , 0 ), thickness = 5 ) if we check the previous statement we will have a Numpy array, but if we use imshow we will have the image with the rectangle in the points we set up 1 cv2 . imshow ( black_img ) now one in the center of the canvas 1 2 cv2 . rectangle ( black_img , pt1 = ( 200 , 200 ), pt2 = ( 300 , 300 ), color = ( 0 , 0 , 255 ), thickness = 5 ) plt . imshow ( black_img )","title":"Rectangles"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Drawing_on_Images.html#circle","text":"For the circle the parameter change from vertex to center point and radius, so to draw a circle 1 2 cv2 . circle ( img = black_img , center = ( 100 , 100 ), radius = 50 , color = ( 255 , 0 , 0 ), thickness = 5 ) plt . imshow ( black_img ) now let\u2019s change the value of thickness to -1 -1 so we filled in the circle 1 cv2 . circle ( img = black_img , center = ( 400 , 400 ), radius = 50 , color = ( 255 , 0 , 0 ), thickness =- 1 )","title":"Circle"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Drawing_on_Images.html#lines","text":"We can create lines in a similar way we create the rectangle, in this case pt1 and pt2 are starting and ending point instead of vertex. 1 2 cv2 . line ( black_img , pt1 = ( 0 , 0 ), pt2 = ( 511 , 511 ), color = ( 102 , 255 , 255 ), thickness = 5 ) plt . imshow ( black_img )","title":"Lines"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Drawing_on_Images.html#text","text":"now for the text we need first to select the font we are going to use, in this case we are limited for the fonts already in cv2 we have other arguments: text: the text we want to display org: the origin point, where the text will start fontFace: the font we are going to use fontScale: the size of the font linetype, for now we are going to use cv2.LINE_AA 1 2 3 font = cv2 . FONT_HERSHEY_SIMPLEX cv2 . putText ( balck_img , text = 'Hello' , org = ( 10 , 500 ), fontFace = font , fontScale = 4 , color = ( 255 , 255 , 255 ), thickness = 2 , lineType = cv2 . LINE_AA ) plt . imshow ( black_img )","title":"Text"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Drawing_on_Images.html#polygons","text":"For the polygons we will have some changes, first we will need to create an array with the vertex and later reshape this array to ROWSx1X2 and this should be a int32 array. the new parameter will be isClosed that we need to set as True to close the figure 1 2 3 4 5 black_img = np . zeros ( shape = ( 512 , 512 , 3 ), dtype = np . int32 ) vertices = np . array ([[ 100 , 300 ],[ 200 , 200 ],[ 400 , 300 ],[ 200 , 400 ]], np . int32 ) pts = vertices . reshape (( - 1 , 1 , 2 )) cv2 . polylines ( black_img ,[ pts ], isClosed = True , color = ( 255 , 0 , 0 ), thickness = 5 ) plt . imshow ( black_img )","title":"Polygons"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Images_and_Numpy.html","text":"in this case i will use Jupyter lab for the code, so the code in this document will have some parts specific for Jupyter lab, additionally the course gave me some images that i will use, so i will refer to those images too. First we need to remember that python alone is not able to handle the images, it needs a library to do so, in this case we are going to use PLI or pillow Get the image with Python \u00b6 so in this case we are going to use the function open() to get the image, in the next step we will transfor the image in a Numpy array 1 2 3 import numpy as np import mathplotlib.pyplot as plt % matplotlib inline #--> this line is just for Jupyter Lab, in order to disply images We imported the mathplotlib in order to display the image in the Jypyter lab, now we are going to import Image from PIL 1 from PIL import Image next, we need to load the image 1 2 3 4 5 6 import numpy as np import mathplotlib.pyplot as plt % matplotlib inline #--> this line is just for Jupyter Lab, in order to disply images from PIL import Image pic = Image . open ( 'path-to/the-image.jpg' ) Transform the image to a Numpy array \u00b6 At this point the image is load but it 1 2 3 pic = Image . open ( 'Computer-Vision-with-Python/DATA/00-puppy.jpg' ) type ( pic ) # PIL.JpegImagePlugin.JpegImageFile in this case we have a Jpeg Image file, now we need to transform it to Numpy array 1 2 3 pic_arr = np . asarray ( pic ) type ( pic_arr ) # numpy.ndarray with the function called asarray we transform this image field to a Numpy array. Display the image with imshow \u00b6 1 2 pic_arr . shape ( 1300 , 1950 , 3 ) in this case we have an array with 1300x1950 with 3 channels, this means, that the image is a color image, so, now lest display this array as an image 1 plt . imshow ( pic_arr ) you will get plt.imshow(image_numpy_array) the plt.imshow is a special function from matplotlib use to display images that are in a Numpy array format. Color mapping one channel to grayscale \u00b6 We know that the image is a color image, that means, it has 3 channels, and we confirm this when we ask for its shape (pic_arr.shape) which result was (1300, 1950, 3), so first we are going to make a copy and later slice the one of the channels 1 2 3 pic_red = pic_arr . copy () pic_red = pic_red [:,:, 0 ] plt . imshow ( pic_red ) the result will be 1 2 pic_red . shape # (1300, 1950) how it looks is due to how , matplotlib handle the colors, in this case is displaying the image in a format that will be special for people with an specific color blindness. We can display the image in gray scale, but the question will be, Gray?, we are going to map the color red to a gray-scale 1 plt . imshow ( pic_red , cmap = 'gray' ) we can see the difference when we get the other colors, green and blue Blue 1 2 3 pic_green = pic_arr . copy () pic_green = pic_green [:,:, 1 ] plt . imshow ( pic_green , cmap = 'gray' ) Green 1 2 3 pic_blue = pic_arr . copy () pic_blue = pic_blue [:,:, 2 ] plt . imshow ( pic_blue , cmap = 'gray' ) now comparing the 3 images Then we can say that in each channel, the closest is the pixel to the color of the channel, closest to 255, and closest to white, for example, in the image of the red channel, the parts of the picture that are more white means that they contain more red, and those that are black means that contain no red. This is mapping the color to a gray scale, but we are not removing the contribution of the colors. Removing contribution of the channels \u00b6 In this part we are going to remove the contribution of the channels Blue and Green so we can have an image with the tree channels but with 0 contribution in two of those channels. 1 2 3 4 pic_red_real = pic_arr . copy () pic_red_real [:,:, 1 ] = 0 pic_red_real [:,:, 2 ] = 0 plt . imshow ( pic_red_real ) and if we check the shape 1 2 pic_red_real . shape # (1300, 1950, 3)","title":"Images and Numpy"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Images_and_Numpy.html#get_the_image_with_python","text":"so in this case we are going to use the function open() to get the image, in the next step we will transfor the image in a Numpy array 1 2 3 import numpy as np import mathplotlib.pyplot as plt % matplotlib inline #--> this line is just for Jupyter Lab, in order to disply images We imported the mathplotlib in order to display the image in the Jypyter lab, now we are going to import Image from PIL 1 from PIL import Image next, we need to load the image 1 2 3 4 5 6 import numpy as np import mathplotlib.pyplot as plt % matplotlib inline #--> this line is just for Jupyter Lab, in order to disply images from PIL import Image pic = Image . open ( 'path-to/the-image.jpg' )","title":"Get the image with Python"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Images_and_Numpy.html#transform_the_image_to_a_numpy_array","text":"At this point the image is load but it 1 2 3 pic = Image . open ( 'Computer-Vision-with-Python/DATA/00-puppy.jpg' ) type ( pic ) # PIL.JpegImagePlugin.JpegImageFile in this case we have a Jpeg Image file, now we need to transform it to Numpy array 1 2 3 pic_arr = np . asarray ( pic ) type ( pic_arr ) # numpy.ndarray with the function called asarray we transform this image field to a Numpy array.","title":"Transform the image to a Numpy array"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Images_and_Numpy.html#display_the_image_with_imshow","text":"1 2 pic_arr . shape ( 1300 , 1950 , 3 ) in this case we have an array with 1300x1950 with 3 channels, this means, that the image is a color image, so, now lest display this array as an image 1 plt . imshow ( pic_arr ) you will get plt.imshow(image_numpy_array) the plt.imshow is a special function from matplotlib use to display images that are in a Numpy array format.","title":"Display the image with imshow"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Images_and_Numpy.html#color_mapping_one_channel_to_grayscale","text":"We know that the image is a color image, that means, it has 3 channels, and we confirm this when we ask for its shape (pic_arr.shape) which result was (1300, 1950, 3), so first we are going to make a copy and later slice the one of the channels 1 2 3 pic_red = pic_arr . copy () pic_red = pic_red [:,:, 0 ] plt . imshow ( pic_red ) the result will be 1 2 pic_red . shape # (1300, 1950) how it looks is due to how , matplotlib handle the colors, in this case is displaying the image in a format that will be special for people with an specific color blindness. We can display the image in gray scale, but the question will be, Gray?, we are going to map the color red to a gray-scale 1 plt . imshow ( pic_red , cmap = 'gray' ) we can see the difference when we get the other colors, green and blue Blue 1 2 3 pic_green = pic_arr . copy () pic_green = pic_green [:,:, 1 ] plt . imshow ( pic_green , cmap = 'gray' ) Green 1 2 3 pic_blue = pic_arr . copy () pic_blue = pic_blue [:,:, 2 ] plt . imshow ( pic_blue , cmap = 'gray' ) now comparing the 3 images Then we can say that in each channel, the closest is the pixel to the color of the channel, closest to 255, and closest to white, for example, in the image of the red channel, the parts of the picture that are more white means that they contain more red, and those that are black means that contain no red. This is mapping the color to a gray scale, but we are not removing the contribution of the colors.","title":"Color mapping one channel to grayscale"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Images_and_Numpy.html#removing_contribution_of_the_channels","text":"In this part we are going to remove the contribution of the channels Blue and Green so we can have an image with the tree channels but with 0 contribution in two of those channels. 1 2 3 4 pic_red_real = pic_arr . copy () pic_red_real [:,:, 1 ] = 0 pic_red_real [:,:, 2 ] = 0 plt . imshow ( pic_red_real ) and if we check the shape 1 2 pic_red_real . shape # (1300, 1950, 3)","title":"Removing contribution of the channels"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Images_and_Opencv.html","text":"OpenCV or Open source computer vision is a library written in C++ although for the purpose of this notes we are going to use the python bindings. Importing and possible errors \u00b6 To import the library we can use cv2 1 import cv2 now to load an image we need to have the correct path to it, otherwise we can generate an error that will be difficult to fine. If for some reason we add the wrong path, we wont get an error message instead None 1 2 3 4 5 img = cv2 . imread ( 'some/wrong/path.png' ) print ( img ) # None type ( img ) # NoneType now, lets load from a correct path 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 img = cv2 . imread ( '../DATA/00-puppy.jpg' ) type ( img ) # numpy.ndarray img array ([[[ 78 , 81 , 95 ], [ 80 , 83 , 97 ], [ 81 , 84 , 98 ], ... , [ 22 , 27 , 25 ], [ 22 , 27 , 25 ], [ 22 , 27 , 25 ]], [[ 78 , 81 , 95 ], [ 79 , 82 , 96 ], [ 79 , 82 , 96 ], ... , [ 22 , 30 , 23 ], [ 23 , 31 , 24 ], [ 23 , 31 , 24 ]]], dtype = uint8 ) img . shape # (1300, 1950, 3) so this imread gave me a Numpy.narray with 3 color channels Color Correction \u00b6 Now that we have the correct image, or rather the correct path, we have the image loaded, but when we display it 1 2 img_bgr = cv2 . imread ( '../DATA/00-puppy.jpg' ) plt . imshow ( img_bgr ) The image has been correctly loaded by openCV as a numpy array, but the color of each pixel has been sorted as BGR. Matplotlib\u2019s plot expects an RGB image so, for a correct display of the image, it is necessary to swap those channels for this, we can use a build-in function 1 img_rgb = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) in this case we are going to use cvtColor to transform from BGR to RGB, we pass first the image img , later the type of conversion cv2.COLOR_BGR2RGB Load Image to a Specific Color Map \u00b6 We can use opencv to load an image to a specific color channel, in this case we want to import it as a gray scale image 1 2 img_gray = cv2 . imread ( '../DATA/00-puppy.jpg' , cv2 . IMREAD_GRAYSCALE ) plt . imshow ( img_gray ) so in this case we have have the default color mapping but this is not the gray scale we wanted, we need to be specific to plt to map the image to gray-scale 1 2 img_gray = cv2 . imread ( '../DATA/00-puppy.jpg' , cv2 . IMREAD_GRAYSCALE ) plt . imshow ( img_gray , cmap = 'gray' ) Resizing and Flipping the Image \u00b6 First we need to remember what is the shape of the image 1 2 img_rgb . shape # width, height, color channels now, we can use resize to change the size of the image 1 2 img = cv2 . resize ( img_rgb ,( 1300 , 275 )) plt . imshow ( img ) By Ratio \u00b6 Here we will use the ratio 1 2 3 4 w_ratio = 0.5 h_ratio = 0.5 new_img = cv2 . resize ( img_rgb ,( 0 , 0 ), img , w_ratio , h_ratio ) plt . imshow ( new_img ) Flipping Images \u00b6 1 2 3 4 5 6 7 # Along central x axis new_img = cv2 . flip ( new_img , 0 ) # Along central y axis new_img = cv2 . flip ( new_img , 1 ) # Along both axis new_img = cv2 . flip ( new_img , - 1 ) plt . imshow ( new_img ) Saving Image Files \u00b6 1 2 3 4 type ( new_img ) #numpy.ndarray cv2 . imwrite ( 'my_new_picture.jpg' , new_img ) #True Keep in mind, the above stored the BGR version of the image. Drawing Images \u00b6 We start by making a canvas, we are going to create an array that will serve as it. 1 2 3 4 5 6 7 8 import numpy as np import matplotlib.pyplot as plt % matplotlib inline import cv2 blank_img = np . zeros ( shape = ( 512 , 512 , 3 ), dtype = np . int16 ) blank_img . shape plt . imshow ( blank_img )","title":"Images and Opencv"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Images_and_Opencv.html#importing_and_possible_errors","text":"To import the library we can use cv2 1 import cv2 now to load an image we need to have the correct path to it, otherwise we can generate an error that will be difficult to fine. If for some reason we add the wrong path, we wont get an error message instead None 1 2 3 4 5 img = cv2 . imread ( 'some/wrong/path.png' ) print ( img ) # None type ( img ) # NoneType now, lets load from a correct path 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 img = cv2 . imread ( '../DATA/00-puppy.jpg' ) type ( img ) # numpy.ndarray img array ([[[ 78 , 81 , 95 ], [ 80 , 83 , 97 ], [ 81 , 84 , 98 ], ... , [ 22 , 27 , 25 ], [ 22 , 27 , 25 ], [ 22 , 27 , 25 ]], [[ 78 , 81 , 95 ], [ 79 , 82 , 96 ], [ 79 , 82 , 96 ], ... , [ 22 , 30 , 23 ], [ 23 , 31 , 24 ], [ 23 , 31 , 24 ]]], dtype = uint8 ) img . shape # (1300, 1950, 3) so this imread gave me a Numpy.narray with 3 color channels","title":"Importing and possible errors"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Images_and_Opencv.html#color_correction","text":"Now that we have the correct image, or rather the correct path, we have the image loaded, but when we display it 1 2 img_bgr = cv2 . imread ( '../DATA/00-puppy.jpg' ) plt . imshow ( img_bgr ) The image has been correctly loaded by openCV as a numpy array, but the color of each pixel has been sorted as BGR. Matplotlib\u2019s plot expects an RGB image so, for a correct display of the image, it is necessary to swap those channels for this, we can use a build-in function 1 img_rgb = cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB ) in this case we are going to use cvtColor to transform from BGR to RGB, we pass first the image img , later the type of conversion cv2.COLOR_BGR2RGB","title":"Color Correction"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Images_and_Opencv.html#load_image_to_a_specific_color_map","text":"We can use opencv to load an image to a specific color channel, in this case we want to import it as a gray scale image 1 2 img_gray = cv2 . imread ( '../DATA/00-puppy.jpg' , cv2 . IMREAD_GRAYSCALE ) plt . imshow ( img_gray ) so in this case we have have the default color mapping but this is not the gray scale we wanted, we need to be specific to plt to map the image to gray-scale 1 2 img_gray = cv2 . imread ( '../DATA/00-puppy.jpg' , cv2 . IMREAD_GRAYSCALE ) plt . imshow ( img_gray , cmap = 'gray' )","title":"Load Image to a Specific Color Map"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Images_and_Opencv.html#resizing_and_flipping_the_image","text":"First we need to remember what is the shape of the image 1 2 img_rgb . shape # width, height, color channels now, we can use resize to change the size of the image 1 2 img = cv2 . resize ( img_rgb ,( 1300 , 275 )) plt . imshow ( img )","title":"Resizing and Flipping the Image"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Images_and_Opencv.html#by_ratio","text":"Here we will use the ratio 1 2 3 4 w_ratio = 0.5 h_ratio = 0.5 new_img = cv2 . resize ( img_rgb ,( 0 , 0 ), img , w_ratio , h_ratio ) plt . imshow ( new_img )","title":"By Ratio"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Images_and_Opencv.html#flipping_images","text":"1 2 3 4 5 6 7 # Along central x axis new_img = cv2 . flip ( new_img , 0 ) # Along central y axis new_img = cv2 . flip ( new_img , 1 ) # Along both axis new_img = cv2 . flip ( new_img , - 1 ) plt . imshow ( new_img )","title":"Flipping Images"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Images_and_Opencv.html#saving_image_files","text":"1 2 3 4 type ( new_img ) #numpy.ndarray cv2 . imwrite ( 'my_new_picture.jpg' , new_img ) #True Keep in mind, the above stored the BGR version of the image.","title":"Saving Image Files"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Images_and_Opencv.html#drawing_images","text":"We start by making a canvas, we are going to create an array that will serve as it. 1 2 3 4 5 6 7 8 import numpy as np import matplotlib.pyplot as plt % matplotlib inline import cv2 blank_img = np . zeros ( shape = ( 512 , 512 , 3 ), dtype = np . int16 ) blank_img . shape plt . imshow ( blank_img )","title":"Drawing Images"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html","text":"This session will cover numpy, this is specially useful since the images are going to be represented as numpy arrays most of the time, here the numpy documentation The Basics \u00b6 NumPy\u2019s main object is the homogeneous multidimensional array. It is a table of elements (usually numbers), all of the same type, indexed by a tuple of positive integers. In NumPy dimensions are called axes. For example, the coordinates of a point in 3D space [1, 2, 1] has one axis. That axis has 3 elements in it, so we say it has a length of 3. In the example pictured below, the array has 2 axes. 1 2 [[ 1. , 0. , 0. ], [ 0. , 1. , 2. ]] The NumPy\u2019s class is call ndarray also know by its alias array , but make sure not confuse it with the Standard Python library class array.array , in this case the NumPy is numpy.array , the Standard python array only handles one-dimensional arrays and offers less functionality. the Important objects \u00b6 The most importation objects for the ndarray : ndarray.ndim : the number of axis (dimensions) of the array ndarray.shape : this is a tuple that indicate the size of the array in each direction, for example, shape of (m,n) will be a matrix with n rows and m columns, the length of the tuple is the numbers of axis, ndim . ndarray.size : total number of elements in the array, this is equal to the product of the elements in the shape. ndarray.dtype : an object describing the type of the elements in the array. ndarray.itemsize : the size in bytes of each element on the array. ndarray.data :the buffer that contain the actual elements of the array, normally is not use since we access the elements using the indexing facilities The code Example \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import numpy as np a = np . arange ( 15 ) . reshape ( 3 , 5 ) # a # array([ [0, 1, 2, 3, 4,], # [5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a . shape # (3,5) a , ndim # 2 a . dtype . name #'int64' a . itemsize #8 a . size #15 type ( a ) # <type 'numpy.ndarray'> Numpy arrays \u00b6 The Creation or a NumPy array \u00b6 you can create a NumPy array with a simple python list 1 2 3 4 5 6 import numpy as np a = np . array ([ 1 , 2 , 3 ]) # array([1, 2, 3]) a . dtype #dtype('int64') A frequent error consists in calling array with multiple numeric arguments, rather than providing a single list of numbers as an argument. 1 2 a = np . array ( 1 , 2 , 3 , 4 ) # WRONG a = np . array ([ 1 , 2 , 3 , 4 ]) # RIGHT now, if you use np.array in a sequence of sequence, example [(1,2,3), (4,5,6)] you will get a two-dimension array 1 2 3 b = np . array ([( 1 , 2 , 3 ),( 4 , 5 , 6 )]) # array([[1, 2, 3], # [4, 5, 6]]) Creation Functions \u00b6 Often, the elements of an array are originally unknown, but its size is known. Hence, NumPy offers several functions to create arrays with initial placeholder content. These minimize the necessity of growing arrays, an expensive operation Create array of zeros with zeros Function \u00b6 The function zeros will create an array and will be use as placeholder 0. this means a float 0, at least you specify the dtype Syntax 1 np . zeros ( shape = ( n , m ), dtype = np type ) shape is optional, instead we can use just the tuple (n,m), and the dtype if it is not specify it will use the float. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 c = np . zeros ( shape = ( 3 , 5 )) #array([[0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.]]) d = np . zeros (( 3 , 5 )) #array([[0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.]]) type ( d ) # numpy.ndarray e = np . zeros (( 3 , 5 ), dtype = np . int16 ) #array([[0, 0, 0, 0, 0], # [0, 0, 0, 0, 0], # [0, 0, 0, 0, 0]], dtype=int16) Create array of ones with ones Function \u00b6 The function ones in the same way that zeros will create and array but instead of use 0 it will use the placeholder 1 . Syntax 1 np . ones ( shape = ( n , m ), dtype = np type ) shape is optional, instead we can use just the tuple (n,m), and the dtype if it is not specify it will use the float. 1 2 3 4 5 6 7 8 9 c = np . ones ( shape = ( 3 , 5 )) #array([[1., 1., 1., 1., 1.], # [1., 1., 1., 1., 1.], # [1., 1., 1., 1., 1.]]) e = np . ones (( 3 , 5 ), dtype = np . int16 ) #array([[1, 1, 1, 1, 1], # [1, 1, 1, 1, 1], # [1, 1, 1, 1, 1]], dtype=int16) Create array of random numbers with empty Function \u00b6 This will create an array with random numbers, the numbers will depend of the state of the memory in that moment 1 2 3 np . empty ( ( 2 , 3 ) ) #array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], # [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]]) Create sequence of numbers ( arange , linspace ) \u00b6 To create sequences of numbers, NumPy provides a function analogous to range that returns arrays instead of lists. Syntax 1 np . arange ( start , end , step ) Example: 1 2 3 4 np . arange ( 10 , 30 , 5 ) # array([10, 15, 20, 25]) np . arange ( 0 , 2 , 0.3 ) # it accepts float arguments # array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) Although when we want to be completely sure of the number o elements that we want we can use linspace Syntax 1 np . linspace ( start , end , number_of_elements ) Example: 1 2 np . linspace ( 0 , 2 , 9 ) # 9 numbers from 0 to 2 # array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ]) Numpy Useful methods for data manipulation \u00b6 Here will be a couple of operation that will be useful in some cases Random numbers \u00b6 To start the generation of random numbers we can start by creating a seed 1 2 # Start making a seed np . random . seed ( 101 ) the 101 can be different in this case we use his to keep constant the numbers with the course followed Get the number \u00b6 Syntax 1 np . random . randint ( starting , ending , step ) 1 2 3 np . random . seed ( 101 ) arr = np . random . randint ( 0 , 100 , 10 ) #array([95, 11, 81, 70, 63, 87, 75, 9, 77, 40]) Find the max and min and it location \u00b6 It is always useful to find the min and the max values of the array and where they are, their index Find max value and its index \u00b6 assuming the array is: 1 2 arr = np . random . randint ( 0 , 100 , 10 ) # array([95, 11, 81, 70, 63, 87, 75, 9, 77, 40]) then Syntax 1 2 3 4 arr . max () # 95 arr . argmax () # 0 Find the min and its index \u00b6 assuming the array is: 1 2 arr = np . random . randint ( 0 , 100 , 10 ) # array([95, 11, 81, 70, 63, 87, 75, 9, 77, 40]) then Syntax 1 2 3 4 arr . min () # 9 arr . argmin () # 7 Average value \u00b6 assuming the array is: 1 2 arr = np . random . randint ( 0 , 100 , 10 ) # array([95, 11, 81, 70, 63, 87, 75, 9, 77, 40]) then Syntax 1 2 arr . mean () # 60.8","title":"Numpy Basic (Part 1)"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#the_basics","text":"NumPy\u2019s main object is the homogeneous multidimensional array. It is a table of elements (usually numbers), all of the same type, indexed by a tuple of positive integers. In NumPy dimensions are called axes. For example, the coordinates of a point in 3D space [1, 2, 1] has one axis. That axis has 3 elements in it, so we say it has a length of 3. In the example pictured below, the array has 2 axes. 1 2 [[ 1. , 0. , 0. ], [ 0. , 1. , 2. ]] The NumPy\u2019s class is call ndarray also know by its alias array , but make sure not confuse it with the Standard Python library class array.array , in this case the NumPy is numpy.array , the Standard python array only handles one-dimensional arrays and offers less functionality.","title":"The Basics"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#the_important_objects","text":"The most importation objects for the ndarray : ndarray.ndim : the number of axis (dimensions) of the array ndarray.shape : this is a tuple that indicate the size of the array in each direction, for example, shape of (m,n) will be a matrix with n rows and m columns, the length of the tuple is the numbers of axis, ndim . ndarray.size : total number of elements in the array, this is equal to the product of the elements in the shape. ndarray.dtype : an object describing the type of the elements in the array. ndarray.itemsize : the size in bytes of each element on the array. ndarray.data :the buffer that contain the actual elements of the array, normally is not use since we access the elements using the indexing facilities","title":"the Important objects"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#the_code_example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import numpy as np a = np . arange ( 15 ) . reshape ( 3 , 5 ) # a # array([ [0, 1, 2, 3, 4,], # [5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a . shape # (3,5) a , ndim # 2 a . dtype . name #'int64' a . itemsize #8 a . size #15 type ( a ) # <type 'numpy.ndarray'>","title":"The code Example"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#numpy_arrays","text":"","title":"Numpy arrays"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#the_creation_or_a_numpy_array","text":"you can create a NumPy array with a simple python list 1 2 3 4 5 6 import numpy as np a = np . array ([ 1 , 2 , 3 ]) # array([1, 2, 3]) a . dtype #dtype('int64') A frequent error consists in calling array with multiple numeric arguments, rather than providing a single list of numbers as an argument. 1 2 a = np . array ( 1 , 2 , 3 , 4 ) # WRONG a = np . array ([ 1 , 2 , 3 , 4 ]) # RIGHT now, if you use np.array in a sequence of sequence, example [(1,2,3), (4,5,6)] you will get a two-dimension array 1 2 3 b = np . array ([( 1 , 2 , 3 ),( 4 , 5 , 6 )]) # array([[1, 2, 3], # [4, 5, 6]])","title":"The Creation or a NumPy array"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#creation_functions","text":"Often, the elements of an array are originally unknown, but its size is known. Hence, NumPy offers several functions to create arrays with initial placeholder content. These minimize the necessity of growing arrays, an expensive operation","title":"Creation Functions"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#create_array_of_zeros_with_zeros_function","text":"The function zeros will create an array and will be use as placeholder 0. this means a float 0, at least you specify the dtype Syntax 1 np . zeros ( shape = ( n , m ), dtype = np type ) shape is optional, instead we can use just the tuple (n,m), and the dtype if it is not specify it will use the float. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 c = np . zeros ( shape = ( 3 , 5 )) #array([[0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.]]) d = np . zeros (( 3 , 5 )) #array([[0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.], # [0., 0., 0., 0., 0.]]) type ( d ) # numpy.ndarray e = np . zeros (( 3 , 5 ), dtype = np . int16 ) #array([[0, 0, 0, 0, 0], # [0, 0, 0, 0, 0], # [0, 0, 0, 0, 0]], dtype=int16)","title":"Create array of zeros with zeros Function"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#create_array_of_ones_with_ones_function","text":"The function ones in the same way that zeros will create and array but instead of use 0 it will use the placeholder 1 . Syntax 1 np . ones ( shape = ( n , m ), dtype = np type ) shape is optional, instead we can use just the tuple (n,m), and the dtype if it is not specify it will use the float. 1 2 3 4 5 6 7 8 9 c = np . ones ( shape = ( 3 , 5 )) #array([[1., 1., 1., 1., 1.], # [1., 1., 1., 1., 1.], # [1., 1., 1., 1., 1.]]) e = np . ones (( 3 , 5 ), dtype = np . int16 ) #array([[1, 1, 1, 1, 1], # [1, 1, 1, 1, 1], # [1, 1, 1, 1, 1]], dtype=int16)","title":"Create array of ones with ones Function"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#create_array_of_random_numbers_with_empty_function","text":"This will create an array with random numbers, the numbers will depend of the state of the memory in that moment 1 2 3 np . empty ( ( 2 , 3 ) ) #array([[ 3.73603959e-262, 6.02658058e-154, 6.55490914e-260], # [ 5.30498948e-313, 3.14673309e-307, 1.00000000e+000]])","title":"Create array of random numbers with empty Function"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#create_sequence_of_numbers_arangelinspace","text":"To create sequences of numbers, NumPy provides a function analogous to range that returns arrays instead of lists. Syntax 1 np . arange ( start , end , step ) Example: 1 2 3 4 np . arange ( 10 , 30 , 5 ) # array([10, 15, 20, 25]) np . arange ( 0 , 2 , 0.3 ) # it accepts float arguments # array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) Although when we want to be completely sure of the number o elements that we want we can use linspace Syntax 1 np . linspace ( start , end , number_of_elements ) Example: 1 2 np . linspace ( 0 , 2 , 9 ) # 9 numbers from 0 to 2 # array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ])","title":"Create sequence of numbers (arange,linspace)"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#numpy_useful_methods_for_data_manipulation","text":"Here will be a couple of operation that will be useful in some cases","title":"Numpy Useful methods for data manipulation"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#random_numbers","text":"To start the generation of random numbers we can start by creating a seed 1 2 # Start making a seed np . random . seed ( 101 ) the 101 can be different in this case we use his to keep constant the numbers with the course followed","title":"Random numbers"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#get_the_number","text":"Syntax 1 np . random . randint ( starting , ending , step ) 1 2 3 np . random . seed ( 101 ) arr = np . random . randint ( 0 , 100 , 10 ) #array([95, 11, 81, 70, 63, 87, 75, 9, 77, 40])","title":"Get the number"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#find_the_max_and_min_and_it_location","text":"It is always useful to find the min and the max values of the array and where they are, their index","title":"Find the max and min and it location"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#find_max_value_and_its_index","text":"assuming the array is: 1 2 arr = np . random . randint ( 0 , 100 , 10 ) # array([95, 11, 81, 70, 63, 87, 75, 9, 77, 40]) then Syntax 1 2 3 4 arr . max () # 95 arr . argmax () # 0","title":"Find max value and its index"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#find_the_min_and_its_index","text":"assuming the array is: 1 2 arr = np . random . randint ( 0 , 100 , 10 ) # array([95, 11, 81, 70, 63, 87, 75, 9, 77, 40]) then Syntax 1 2 3 4 arr . min () # 9 arr . argmin () # 7","title":"Find the min  and its index"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_1.html#average_value","text":"assuming the array is: 1 2 arr = np . random . randint ( 0 , 100 , 10 ) # array([95, 11, 81, 70, 63, 87, 75, 9, 77, 40]) then Syntax 1 2 arr . mean () # 60.8","title":"Average value"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_2.html","text":"Shape and Reshape \u00b6 Something really useful is be able to change or create an array in a specific shape, we will start with the same array that before: 1 2 3 np . random . seed ( 101 ) arr = np . random . randint ( 0 , 100 , 10 ) #array([95, 11, 81, 70, 63, 87, 75, 9, 77, 40]) Find the shape \u00b6 now we can use shape to find what shape our array has in this moment 1 2 arr . shape # (10,) this mean that is an array, or, better a vector with 10 items. How to reshape \u00b6 we can use the function reshape to change the previous vector Syntax 1 2 3 arr . reshape (( 2 , 5 )) # array([[95, 11, 81, 70, 63], # [87, 75, 9, 77, 40]]) we need to reshape to values that make sense, in this case 2 rows and 5 columns does, because 2*5 = 10 2*5 = 10 if i try to create a shape that is not correct i will get a valueError exception, like here 1 2 3 4 5 6 7 arr . reshape (( 2 , 10 )) --------------------------------------------------------------------------- ValueError Traceback ( most recent call last ) < ipython - input - 35 - ef8eb80c29ad > in < module > () ----> 1 arr . reshape (( 2 , 10 )) ValueError : cannot reshape array of size 10 into shape ( 2 , 10 ) Indexing \u00b6 it is important to be able to get back rows, columns or slice of the matrix we create, in this case we enter to scope of indexing. 1 2 3 4 5 6 7 8 9 10 11 mat = np . arange ( 0 , 100 ) . reshape ( 10 , 10 ) # array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], # [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], # [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], # [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], # [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], # [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], # [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], # [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], # [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]) Find a single digit in the matrix \u00b6 let say that i want to get the element in row = 0 and col = 1 Syntax 1 2 mat [ row , col ] # 1 we don\u2019t need to define the variables \u2018row\u2019 and \u2018col\u2019 we can simply 1 2 mat [ 5 , 5 ] # 55 How to slice a matrix \u00b6 let say that we want 1. Get all the values in a row or column we want to get all the values in the row 1 Syntax 1 2 mat [:, 1 ] # array([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91]) now for the column 1 2 mat [ 1 ,:] # array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]) we can change the shape of the new array 1 2 3 4 5 6 7 8 9 10 11 mat [ 1 ,:] . reshape ( 10 , 1 ) # array([[10], # [11], # [12], # [13], # [14], # [15], # [16], # [17], # [18], # [19]]) 2. Get all the values from n rows and m columns We want to get back part of the original matrix 1 2 3 4 mat [: 3 ,: 3 ] # array([[ 0, 1, 2], # [10, 11, 12], # [20, 21, 22]]) we can replace part of the matrix as well 1 2 3 4 5 6 7 8 9 10 11 mat [ 0 : 4 , 0 : 4 ] = 0 #array([[ 0, 0, 0, 0, 4, 5, 6, 7, 8, 9], # [ 0, 0, 0, 0, 14, 15, 16, 17, 18, 19], # [ 0, 0, 0, 0, 24, 25, 26, 27, 28, 29], # [ 0, 0, 0, 0, 34, 35, 36, 37, 38, 39], # [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], # [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], # [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], # [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], # [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], # [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]])","title":"Numpy Basic (Part 2)"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_2.html#shape_and_reshape","text":"Something really useful is be able to change or create an array in a specific shape, we will start with the same array that before: 1 2 3 np . random . seed ( 101 ) arr = np . random . randint ( 0 , 100 , 10 ) #array([95, 11, 81, 70, 63, 87, 75, 9, 77, 40])","title":"Shape and Reshape"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_2.html#find_the_shape","text":"now we can use shape to find what shape our array has in this moment 1 2 arr . shape # (10,) this mean that is an array, or, better a vector with 10 items.","title":"Find the shape"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_2.html#how_to_reshape","text":"we can use the function reshape to change the previous vector Syntax 1 2 3 arr . reshape (( 2 , 5 )) # array([[95, 11, 81, 70, 63], # [87, 75, 9, 77, 40]]) we need to reshape to values that make sense, in this case 2 rows and 5 columns does, because 2*5 = 10 2*5 = 10 if i try to create a shape that is not correct i will get a valueError exception, like here 1 2 3 4 5 6 7 arr . reshape (( 2 , 10 )) --------------------------------------------------------------------------- ValueError Traceback ( most recent call last ) < ipython - input - 35 - ef8eb80c29ad > in < module > () ----> 1 arr . reshape (( 2 , 10 )) ValueError : cannot reshape array of size 10 into shape ( 2 , 10 )","title":"How to reshape"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_2.html#indexing","text":"it is important to be able to get back rows, columns or slice of the matrix we create, in this case we enter to scope of indexing. 1 2 3 4 5 6 7 8 9 10 11 mat = np . arange ( 0 , 100 ) . reshape ( 10 , 10 ) # array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], # [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], # [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], # [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], # [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], # [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], # [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], # [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], # [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]])","title":"Indexing"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_2.html#find_a_single_digit_in_the_matrix","text":"let say that i want to get the element in row = 0 and col = 1 Syntax 1 2 mat [ row , col ] # 1 we don\u2019t need to define the variables \u2018row\u2019 and \u2018col\u2019 we can simply 1 2 mat [ 5 , 5 ] # 55","title":"Find a single digit in the matrix"},{"location":"Courses/Udemy/Python Computer Vision and OpenCV/Images Basic with OpenCV/Numpy_Basic_part_2.html#how_to_slice_a_matrix","text":"let say that we want 1. Get all the values in a row or column we want to get all the values in the row 1 Syntax 1 2 mat [:, 1 ] # array([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91]) now for the column 1 2 mat [ 1 ,:] # array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]) we can change the shape of the new array 1 2 3 4 5 6 7 8 9 10 11 mat [ 1 ,:] . reshape ( 10 , 1 ) # array([[10], # [11], # [12], # [13], # [14], # [15], # [16], # [17], # [18], # [19]]) 2. Get all the values from n rows and m columns We want to get back part of the original matrix 1 2 3 4 mat [: 3 ,: 3 ] # array([[ 0, 1, 2], # [10, 11, 12], # [20, 21, 22]]) we can replace part of the matrix as well 1 2 3 4 5 6 7 8 9 10 11 mat [ 0 : 4 , 0 : 4 ] = 0 #array([[ 0, 0, 0, 0, 4, 5, 6, 7, 8, 9], # [ 0, 0, 0, 0, 14, 15, 16, 17, 18, 19], # [ 0, 0, 0, 0, 24, 25, 26, 27, 28, 29], # [ 0, 0, 0, 0, 34, 35, 36, 37, 38, 39], # [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], # [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], # [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], # [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], # [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], # [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]])","title":"How to slice a matrix"},{"location":"Machine Learning/Other_key_Terminology.html","text":"Key concepts about dataset \u00b6 Feature: The input(s) to our model. Examples: An input/output pair used for training (Output, are the labels we mentioned in other notes). Labels: the output of the model. Layer: A collection of nodes connected together within a neural network. Key concept about the model \u00b6 Model: The representation of your neural network. Dense and Fully Connected (FC): Each node in one layer is connected to each node in the previous layer. Weights and Biases: the internal variable of model. Loss: The discrepancy between the desired output and the actual output. MSE: Mean squared error, a type of loss function that counts a small number of large discrepancies as worse than a large number of small ones. Gradient Descent: An algorithm that changes the internal variables a bit at a time to gradually reduce the loss function. Optimizer: A specific implementation of the gradient descent algorithm.( There are many algorithms for this. in this course we will only use the \u201cAdam\u201d Optimizer, which stands for ADAptive with Momentum it is considered the best-practice optimizer. Learning rate: The \u2018step size\u201d for loss improvement during gradient descent. Batch: The set of examples used during training of the neural network. Epoch: A full pass over the entire training dataset. Forward pass: The computation of output values from input. Backward pass (back-propagation): The calculation of internal variable adjustments according to the optimizer algorithm, starting from the output layer and working back through each layer to the input. (it is the tunning process) Different type of models \u00b6 Regression: A model that outputs a single value. For example, an estimate of a house\u2019s value. Classification: A model that outputs a probability distribution across several categories. For example, in Fashion MNIST, the output was 10 probabilities, one for each of the different types of clothing. Remember, we use Softmax as the activation function in our last Dense layer to create this probability Distribution. + Classification Regression Output List of numbers that represent probabilities for each class Single Number Example Fashion MNIST Celsius to Fahrenheit Loss Spare categorical cross-entropy Mean squared error Last Layer Activation Function Softmax None Key concepts in the model and the Dense function \u00b6 Flattening: The process of converting a 2d image into 1d vector. ReLU: An activation function that allows a model to solve nonlinear problems. Softmax: A function that provides probabilities for each possible output class. Classification: A machine learning model used for distinguishing among two or more output categories. A convolution is the process of applying a filter (\u201ckernel\u201d) to an image. Max pooling is the process of reducing the size of the image through downsampling. Key concept about convolutional networks \u00b6 CNNs: Convolutional neural network, that is, a network which has at least one convolutional layer. A typical CNN also includes other types of Layers, such as pooling layers and dense layers. Convolution: The process of applying a kernel (filter) to an image. Kernel / Filter: A matrix which is smaller than the input, used to transform the input into chunks. Padding: adding pixels of some value usually 0, around the input image. Pooling: The process of reducing the size of an image through downsampling. There are several types of pooling layers. For example, average pooling converts many values into a single value by talking the average. However, maxpooling is the most common. Maxpooling: A pooling process in which many values are converted into a single value by talking the maximum value from among them. Stride:**the number of pixels to slide the kernel (filter) across the image. **Downsampling: The act of reducing the size of an image. Example of a script with convolutional and fashion MNIST data set \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 #Imports import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt #import dataset fashion_mnist = keras . datasets . fashion_mnist ( train_image , train_label ), ( test_image , test_labels ) = fashion_mnist . load_data () #classes name ( name of the classes of clothes) class_name = [ 't-shirt/top' , 'trouser' , 'pullover' , 'dress' , 'coat' , 'sandal' , 'shirt' , 'sneakers' , 'bag' , 'Ankle boot' ] #Explore data/ process data train_image = train_image / 255 test_image = test_image / 255 #build a model #setup layer model = keras . Sequencial ([ keras . layers . Flatten ( input_shape ( 28 , 28 )), keras . layers . Dense ( 128 , activation = tf . np . relu ), keras . layers . Dense ( 10 , activation = tf . np . softmax ) ]) #compile the model model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) #train the model model . fit ( train_image , train_label , epochs = 5 ) #Evaluate accuracy test_loss , test_acc = model . evaluate ( test_image , test_labels ) print ( 'Test accuracy: ' , test_acc ) #make predictions predictions = model . predict ( test_image ) predictions [ 0 ] np . argmax ( predictions [ 0 ]) test_labels [ 0 ]","title":"Other Key Terminology (TensorFlow)"},{"location":"Machine Learning/Other_key_Terminology.html#key_concepts_about_dataset","text":"Feature: The input(s) to our model. Examples: An input/output pair used for training (Output, are the labels we mentioned in other notes). Labels: the output of the model. Layer: A collection of nodes connected together within a neural network.","title":"Key concepts about dataset"},{"location":"Machine Learning/Other_key_Terminology.html#key_concept_about_the_model","text":"Model: The representation of your neural network. Dense and Fully Connected (FC): Each node in one layer is connected to each node in the previous layer. Weights and Biases: the internal variable of model. Loss: The discrepancy between the desired output and the actual output. MSE: Mean squared error, a type of loss function that counts a small number of large discrepancies as worse than a large number of small ones. Gradient Descent: An algorithm that changes the internal variables a bit at a time to gradually reduce the loss function. Optimizer: A specific implementation of the gradient descent algorithm.( There are many algorithms for this. in this course we will only use the \u201cAdam\u201d Optimizer, which stands for ADAptive with Momentum it is considered the best-practice optimizer. Learning rate: The \u2018step size\u201d for loss improvement during gradient descent. Batch: The set of examples used during training of the neural network. Epoch: A full pass over the entire training dataset. Forward pass: The computation of output values from input. Backward pass (back-propagation): The calculation of internal variable adjustments according to the optimizer algorithm, starting from the output layer and working back through each layer to the input. (it is the tunning process)","title":"Key concept about the model"},{"location":"Machine Learning/Other_key_Terminology.html#different_type_of_models","text":"Regression: A model that outputs a single value. For example, an estimate of a house\u2019s value. Classification: A model that outputs a probability distribution across several categories. For example, in Fashion MNIST, the output was 10 probabilities, one for each of the different types of clothing. Remember, we use Softmax as the activation function in our last Dense layer to create this probability Distribution. + Classification Regression Output List of numbers that represent probabilities for each class Single Number Example Fashion MNIST Celsius to Fahrenheit Loss Spare categorical cross-entropy Mean squared error Last Layer Activation Function Softmax None","title":"Different type of models"},{"location":"Machine Learning/Other_key_Terminology.html#key_concepts_in_the_model_and_the_dense_function","text":"Flattening: The process of converting a 2d image into 1d vector. ReLU: An activation function that allows a model to solve nonlinear problems. Softmax: A function that provides probabilities for each possible output class. Classification: A machine learning model used for distinguishing among two or more output categories. A convolution is the process of applying a filter (\u201ckernel\u201d) to an image. Max pooling is the process of reducing the size of the image through downsampling.","title":"Key concepts in the model and the Dense function"},{"location":"Machine Learning/Other_key_Terminology.html#key_concept_about_convolutional_networks","text":"CNNs: Convolutional neural network, that is, a network which has at least one convolutional layer. A typical CNN also includes other types of Layers, such as pooling layers and dense layers. Convolution: The process of applying a kernel (filter) to an image. Kernel / Filter: A matrix which is smaller than the input, used to transform the input into chunks. Padding: adding pixels of some value usually 0, around the input image. Pooling: The process of reducing the size of an image through downsampling. There are several types of pooling layers. For example, average pooling converts many values into a single value by talking the average. However, maxpooling is the most common. Maxpooling: A pooling process in which many values are converted into a single value by talking the maximum value from among them. Stride:**the number of pixels to slide the kernel (filter) across the image. **Downsampling: The act of reducing the size of an image.","title":"Key concept about convolutional networks"},{"location":"Machine Learning/Other_key_Terminology.html#example_of_a_script_with_convolutional_and_fashion_mnist_data_set","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 #Imports import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt #import dataset fashion_mnist = keras . datasets . fashion_mnist ( train_image , train_label ), ( test_image , test_labels ) = fashion_mnist . load_data () #classes name ( name of the classes of clothes) class_name = [ 't-shirt/top' , 'trouser' , 'pullover' , 'dress' , 'coat' , 'sandal' , 'shirt' , 'sneakers' , 'bag' , 'Ankle boot' ] #Explore data/ process data train_image = train_image / 255 test_image = test_image / 255 #build a model #setup layer model = keras . Sequencial ([ keras . layers . Flatten ( input_shape ( 28 , 28 )), keras . layers . Dense ( 128 , activation = tf . np . relu ), keras . layers . Dense ( 10 , activation = tf . np . softmax ) ]) #compile the model model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) #train the model model . fit ( train_image , train_label , epochs = 5 ) #Evaluate accuracy test_loss , test_acc = model . evaluate ( test_image , test_labels ) print ( 'Test accuracy: ' , test_acc ) #make predictions predictions = model . predict ( test_image ) predictions [ 0 ] np . argmax ( predictions [ 0 ]) test_labels [ 0 ]","title":"Example of  a script with convolutional and fashion MNIST data set"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html","text":"Common ML Problem \u00b6 In basic terms, ML is the process of training a piece of software, called a model, to make useful predictions using a data set. There are two common paradigms mentioned in ML, Supervised and Unsupervised training. What is Supervised Learning? \u00b6 Supervised learning is a type of ML where the model is provided with labeled training data, this means that we feed the model with features ( x x If you want) and the answer or so call label ( y y ) and it will learn the relationship between these two. What is Unsupervised Learning? \u00b6 In unsupervised learning, the goal is to identify meaningful patterns in the data. To accomplish this, the machine must learn from an unlabeled data set. In other words, the model has no hints how to categorize each piece of data and must infer its own rules for doing so. Types of ML Problems \u00b6 There are several subclass of ML, depending of the prediction task type of ML Problem Description Example Classification Pick one of N labels Cat, dog, horse, or bear Regression Predict numerical values Click-through rate Clustering Group similar examples Most relevant documents (unsupervised) Association rule learning Infer likely association patterns in data If you buy hamburger buns, you\u2019re likely to buy hamburgers (unsupervised) Structured output Create complex output Natural language parse trees, image recognition bounding boxes Ranking Identify position on a scale or status Search result ranking The ML Mindset \u00b6 In traditional software, you can trick, tune and reason to find the design that fit the requirements, but in machine learning, more often than not, it will be necessary to experiment to find the correct or rather the workable model. ML produce models that interpret signals in a different way ( compare with humans), for example a Neural network might interpret the words \u201ctree\u201d liek something like this [0.37,0.24,0.2] and \u201ccar\u201d as [0.1,0.78, 0.9] the Neural network might use this interpretation to do an accurate translation or a sentiment analysis, but humans looking to this embeddings would find them very hard to understand, this can make machine learning difficult but not impossible for humans to evaluate and understand. Experimental Design Prime \u00b6 Get Comfortable with Some Uncertainty \u00b6 One of the difference between ML and the traditional programming, is that in traditional programming you will end with a set of parameters that you understand and you know how they should behave, but with ML, the non-coding work can be very complicated, but the code usually far less code. you might get the code correctly and expect a result, but the result you will find suitable might be obtain after several changes and tunning that you might not fully understand. Scientific Method \u00b6 It is useful to think ML process as an experiment where we run test after test to converge on a workable model. Step Example 1. Set the research goal. I want to predict how heavy traffic will be on a given day 2. Make a hypothesis. I think the weather forecast is an informative signal. 3. Collect the data. Collect historical traffic data and weather on each day. 4. Test your hypothesis. Train a model using this data. 5. Analyze your results. Is this model better than existing systems? 6. Reach a conclusion. I should (not) use this model to make predictions, because of X, Y, and Z. 7. Refine hypothesis and repeat. Time of year could be a helpful signal. Identifying Good Problems for ML \u00b6 Clear Use Case \u00b6 Start with the problem, not the solution. Make sure you aren\u2019t treating ML as a hammer for your problems. Ask yourself the following question in order: What problem is my product facing? Would it be a good problem for ML? Know the Problem Before Focusing on the Data \u00b6 Be prepared to have your assumptions challenged. If you understand the problem clearly, you should be able to list some potential solutions to test in order to generate the best model. Understand that you will likely have to try out a few solutions before you land on a good working model. Predictive Power \u00b6 You should not try to make ML do the hard work of discovering which features are relevant for you. If you simply throw everything at the model and see what looks useful, your model will likely wind up overly complicated, expensive, and filled with unimportant features. Predictions vs. Decisions \u00b6 Make sure your predictions allow you to take a useful action. For example, a model that predicts the likelihood of clicking certain videos could allow a system to prefetch the videos most likely to be clicked. Conversely, a model that predicts the probability that someone will click \u201cthumbs down\u201d for a specific YouTube video might be interesting, but we can\u2019t do anything useful with that knowledge. Prediction Decision What video the learner wants to watch next. Show those videos in the recommendation bar. Probability someone will click on a search result. If P(click) > 0.12, prefetch the web page. What fraction of a video ad the user will watch. If a small fraction, don\u2019t show the user the ad. Success and Failure Metrics \u00b6 How I will measure the success or failure of the model, the success or failure metric are different than metrics such as precision, recall, etc, rather the specific anticipated outcome. As an example, let say i want a video recommendations model, the success metric might be \u201cA success metric is the number of popular videos properly predicted by the model. Success means predicting 95% of the most popular videos as measured by watch time within 28 days of being uploaded.\u201d and the failure metric will be \u201cFailure means the number of popular videos properly predicted is no better than current heuristics\u201d Are the Metrics Measurable? \u00b6 Ask the following: How will you measure your metrics? When can you measure your metrics? How long will it take to know whether your new ML system is a success or failure? What Output Would You like the ML Model to Produce? \u00b6 Revisiting this table, which type of output are you looking for: a number, a label, a cluster, or something else? Type of ML Problem Description Example Classification Pick one of N labels cat, dog, horse, or bear Regression Predict numerical values click-through rate Clustering Group similar examples most relevant documents (unsupervised) Association rule learning Infer likely association patterns in data If you buy hamburger buns, you\u2019re likely to buy hamburgers (unsupervised) Structured output Create complex output natural language parse trees, image recognition bounding boxes Heuristics \u00b6 How might you solve your problem without ML? Suppose you need to deliver a product tomorrow, and you have only time enough to hard-code the business logic. You could try a heuristic (non-ML solution) like the following: Example Consider people who uploaded popular videos in the past. Assume that new videos uploaded by these people will also become popular. The preceding heuristic might not be the world\u2019s greatest heuristic, but it does provide a baseline. Never launch a fancy ML model that can\u2019t beat a heuristic. The exercise of making a heuristic often will help you identify good signals in your ML model. Non-ML solutions can sometimes be simpler to maintain than ML solutions. Formulate YourProblem as an ML Problem \u00b6 We are to follow the suggested approach for framing the ML problem: Articulate your problem. Start simple. Identify Your Data Sources. Design your data for the model. Determine where data comes from. Determine easily obtain inputs. Ability to Learn. Think About Potential Bias. Articulate Your Problem \u00b6 There are several subtype of classification an regression, the following flowchart can give help to define which can be use. Our problem is best framed as: Binary classification Unidimensional regression Multi-class single-label classification Multi-class multi-label classification Multidimensional regression Clustering (unsupervised) Other (translation, parsing, bounding box id, etc.) Start Simple \u00b6 Can you simplify your problem? First, simplify your modeling task. State your given problem as a binary classification or a unidimensional regression problem Then, for that task, use the simplest model possible. A simple model is easier to implement and understand. Once you have a full ML pipeline, you can iterate on the simple model with greater ease. The biggest gain from ML tends to be the first launch, since that\u2019s when you can first leverage your data. Further tuning still gives wins, but, generally, the biggest gain is at the start so it\u2019s good to pick well-tested methods to make the process easier. Identify Your Data Sources \u00b6 Provide answers to the following questions about your labels: How much labeled data do you have? What is the source of your label? Is your label closely connected to the decision you will be making? Design your Data for the Model \u00b6 Identify the data that your ML system should use to make predictions (input -> output), If an input is not a scalar or 1D list, consider whether that is the best representation for your data. Determine Where Data Comes From \u00b6 Assess how much work it will be to develop a data pipeline to construct each column for a row. When does the example output become available for training purposes? Determine Easily Obtained Inputs \u00b6 Pick 1-3 inputs that are easy to obtain and that you believe would produce a reasonable, initial outcome. Which inputs would be useful for implementing heuristics mentioned previously? Ability to Learn \u00b6 Will the ML model be able to learn? List aspects of your problem that might cause difficulty learning. For example: The data set doesn\u2019t contain enough positive labels. The training data doesn\u2019t contain enough examples. The labels are too noisy. The system memorizes the training data, but has difficulty generalizing to new cases Think About Potential Bias \u00b6 Many dataset are biased in some way. These biases may adversely affect training and the predictions made.","title":"Problem Framing"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#common_ml_problem","text":"In basic terms, ML is the process of training a piece of software, called a model, to make useful predictions using a data set. There are two common paradigms mentioned in ML, Supervised and Unsupervised training.","title":"Common ML Problem"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#what_is_supervised_learning","text":"Supervised learning is a type of ML where the model is provided with labeled training data, this means that we feed the model with features ( x x If you want) and the answer or so call label ( y y ) and it will learn the relationship between these two.","title":"What is Supervised Learning?"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#what_is_unsupervised_learning","text":"In unsupervised learning, the goal is to identify meaningful patterns in the data. To accomplish this, the machine must learn from an unlabeled data set. In other words, the model has no hints how to categorize each piece of data and must infer its own rules for doing so.","title":"What is Unsupervised Learning?"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#types_of_ml_problems","text":"There are several subclass of ML, depending of the prediction task type of ML Problem Description Example Classification Pick one of N labels Cat, dog, horse, or bear Regression Predict numerical values Click-through rate Clustering Group similar examples Most relevant documents (unsupervised) Association rule learning Infer likely association patterns in data If you buy hamburger buns, you\u2019re likely to buy hamburgers (unsupervised) Structured output Create complex output Natural language parse trees, image recognition bounding boxes Ranking Identify position on a scale or status Search result ranking","title":"Types of ML Problems"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#the_ml_mindset","text":"In traditional software, you can trick, tune and reason to find the design that fit the requirements, but in machine learning, more often than not, it will be necessary to experiment to find the correct or rather the workable model. ML produce models that interpret signals in a different way ( compare with humans), for example a Neural network might interpret the words \u201ctree\u201d liek something like this [0.37,0.24,0.2] and \u201ccar\u201d as [0.1,0.78, 0.9] the Neural network might use this interpretation to do an accurate translation or a sentiment analysis, but humans looking to this embeddings would find them very hard to understand, this can make machine learning difficult but not impossible for humans to evaluate and understand.","title":"The ML Mindset"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#experimental_design_prime","text":"","title":"Experimental Design Prime"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#get_comfortable_with_some_uncertainty","text":"One of the difference between ML and the traditional programming, is that in traditional programming you will end with a set of parameters that you understand and you know how they should behave, but with ML, the non-coding work can be very complicated, but the code usually far less code. you might get the code correctly and expect a result, but the result you will find suitable might be obtain after several changes and tunning that you might not fully understand.","title":"Get Comfortable with Some Uncertainty"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#scientific_method","text":"It is useful to think ML process as an experiment where we run test after test to converge on a workable model. Step Example 1. Set the research goal. I want to predict how heavy traffic will be on a given day 2. Make a hypothesis. I think the weather forecast is an informative signal. 3. Collect the data. Collect historical traffic data and weather on each day. 4. Test your hypothesis. Train a model using this data. 5. Analyze your results. Is this model better than existing systems? 6. Reach a conclusion. I should (not) use this model to make predictions, because of X, Y, and Z. 7. Refine hypothesis and repeat. Time of year could be a helpful signal.","title":"Scientific Method"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#identifying_good_problems_for_ml","text":"","title":"Identifying Good Problems for ML"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#clear_use_case","text":"Start with the problem, not the solution. Make sure you aren\u2019t treating ML as a hammer for your problems. Ask yourself the following question in order: What problem is my product facing? Would it be a good problem for ML?","title":"Clear Use Case"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#know_the_problem_before_focusing_on_the_data","text":"Be prepared to have your assumptions challenged. If you understand the problem clearly, you should be able to list some potential solutions to test in order to generate the best model. Understand that you will likely have to try out a few solutions before you land on a good working model.","title":"Know the Problem Before Focusing on the Data"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#predictive_power","text":"You should not try to make ML do the hard work of discovering which features are relevant for you. If you simply throw everything at the model and see what looks useful, your model will likely wind up overly complicated, expensive, and filled with unimportant features.","title":"Predictive Power"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#predictions_vs_decisions","text":"Make sure your predictions allow you to take a useful action. For example, a model that predicts the likelihood of clicking certain videos could allow a system to prefetch the videos most likely to be clicked. Conversely, a model that predicts the probability that someone will click \u201cthumbs down\u201d for a specific YouTube video might be interesting, but we can\u2019t do anything useful with that knowledge. Prediction Decision What video the learner wants to watch next. Show those videos in the recommendation bar. Probability someone will click on a search result. If P(click) > 0.12, prefetch the web page. What fraction of a video ad the user will watch. If a small fraction, don\u2019t show the user the ad.","title":"Predictions vs. Decisions"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#success_and_failure_metrics","text":"How I will measure the success or failure of the model, the success or failure metric are different than metrics such as precision, recall, etc, rather the specific anticipated outcome. As an example, let say i want a video recommendations model, the success metric might be \u201cA success metric is the number of popular videos properly predicted by the model. Success means predicting 95% of the most popular videos as measured by watch time within 28 days of being uploaded.\u201d and the failure metric will be \u201cFailure means the number of popular videos properly predicted is no better than current heuristics\u201d","title":"Success and Failure Metrics"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#are_the_metrics_measurable","text":"Ask the following: How will you measure your metrics? When can you measure your metrics? How long will it take to know whether your new ML system is a success or failure?","title":"Are the Metrics Measurable?"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#what_output_would_you_like_the_ml_model_to_produce","text":"Revisiting this table, which type of output are you looking for: a number, a label, a cluster, or something else? Type of ML Problem Description Example Classification Pick one of N labels cat, dog, horse, or bear Regression Predict numerical values click-through rate Clustering Group similar examples most relevant documents (unsupervised) Association rule learning Infer likely association patterns in data If you buy hamburger buns, you\u2019re likely to buy hamburgers (unsupervised) Structured output Create complex output natural language parse trees, image recognition bounding boxes","title":"What Output Would You like the ML Model to Produce?"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#heuristics","text":"How might you solve your problem without ML? Suppose you need to deliver a product tomorrow, and you have only time enough to hard-code the business logic. You could try a heuristic (non-ML solution) like the following: Example Consider people who uploaded popular videos in the past. Assume that new videos uploaded by these people will also become popular. The preceding heuristic might not be the world\u2019s greatest heuristic, but it does provide a baseline. Never launch a fancy ML model that can\u2019t beat a heuristic. The exercise of making a heuristic often will help you identify good signals in your ML model. Non-ML solutions can sometimes be simpler to maintain than ML solutions.","title":"Heuristics"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#formulate_yourproblem_as_an_ml_problem","text":"We are to follow the suggested approach for framing the ML problem: Articulate your problem. Start simple. Identify Your Data Sources. Design your data for the model. Determine where data comes from. Determine easily obtain inputs. Ability to Learn. Think About Potential Bias.","title":"Formulate YourProblem as an ML Problem"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#articulate_your_problem","text":"There are several subtype of classification an regression, the following flowchart can give help to define which can be use. Our problem is best framed as: Binary classification Unidimensional regression Multi-class single-label classification Multi-class multi-label classification Multidimensional regression Clustering (unsupervised) Other (translation, parsing, bounding box id, etc.)","title":"Articulate Your Problem"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#start_simple","text":"Can you simplify your problem? First, simplify your modeling task. State your given problem as a binary classification or a unidimensional regression problem Then, for that task, use the simplest model possible. A simple model is easier to implement and understand. Once you have a full ML pipeline, you can iterate on the simple model with greater ease. The biggest gain from ML tends to be the first launch, since that\u2019s when you can first leverage your data. Further tuning still gives wins, but, generally, the biggest gain is at the start so it\u2019s good to pick well-tested methods to make the process easier.","title":"Start Simple"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#identify_your_data_sources","text":"Provide answers to the following questions about your labels: How much labeled data do you have? What is the source of your label? Is your label closely connected to the decision you will be making?","title":"Identify Your Data Sources"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#design_your_data_for_the_model","text":"Identify the data that your ML system should use to make predictions (input -> output), If an input is not a scalar or 1D list, consider whether that is the best representation for your data.","title":"Design your Data for the Model"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#determine_where_data_comes_from","text":"Assess how much work it will be to develop a data pipeline to construct each column for a row. When does the example output become available for training purposes?","title":"Determine Where Data Comes From"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#determine_easily_obtained_inputs","text":"Pick 1-3 inputs that are easy to obtain and that you believe would produce a reasonable, initial outcome. Which inputs would be useful for implementing heuristics mentioned previously?","title":"Determine Easily Obtained Inputs"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#ability_to_learn","text":"Will the ML model be able to learn? List aspects of your problem that might cause difficulty learning. For example: The data set doesn\u2019t contain enough positive labels. The training data doesn\u2019t contain enough examples. The labels are too noisy. The system memorizes the training data, but has difficulty generalizing to new cases","title":"Ability to Learn"},{"location":"Machine Learning/Google Crash Course/Problem_Framing.html#think_about_potential_bias","text":"Many dataset are biased in some way. These biases may adversely affect training and the predictions made.","title":"Think About Potential Bias"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/First_steps_TensorFlow.html","text":"TensorFlow provide different type of toolkits to construct models in a variety of levels of abstraction. It can be a lower_level with mathematical operations or a higher level with predefine architectures. Tollkit Description Estimator (tf.estimator) High-level, OOP API. tf.layers/tf.losses/tf.metrics Libraries for common model components. TensorFlow Lower-level APIs Similar to how Python has and interpreter that can run in multiple hardware to run python code, TensorFlow can run the graph on multiple hardware platforms, including CPU, GPU, and TPU. Here will be an example of pseudo code of a linear classification program using tf.estimator 1 2 3 4 5 6 7 8 9 10 import tensorflow as tf # Set up a linear classifier. classifier = tf.estimator.LinearClassifier(feature_columns) # Train the model on some example data. classifier.train(input_fn=train_input_fn, steps=2000) # Use it to predict. predictions = classifier.predict(input_fn=predict_input_fn) Tensor: The primary data structure in TensorFlow programs. Tensors are N-dimensional (where N could be very large) data structures, most commonly scalars, vectors, or matrices. The elements of a Tensor can hold integer, floating-point, or string values. Common hyperparameter in Machine Learning \u00b6 Many of the coding exercises contain the following hyperparameters: steps , which is the total number of training iterations. One step calculates the loss from one batch and uses that value to modify the model\u2019s weights once. batch size , which is the number of examples (chosen at random) for a single step. For example, the batch size for SGD is 1. The following formula applies: total\\ number\\ of\\ trained\\ examples = batch\\ size * steps total\\ number\\ of\\ trained\\ examples = batch\\ size * steps","title":"First Steps TensorFlow"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/First_steps_TensorFlow.html#common_hyperparameter_in_machine_learning","text":"Many of the coding exercises contain the following hyperparameters: steps , which is the total number of training iterations. One step calculates the loss from one batch and uses that value to modify the model\u2019s weights once. batch size , which is the number of examples (chosen at random) for a single step. For example, the batch size for SGD is 1. The following formula applies: total\\ number\\ of\\ trained\\ examples = batch\\ size * steps total\\ number\\ of\\ trained\\ examples = batch\\ size * steps","title":"Common hyperparameter in Machine Learning"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Framing.html","text":"Supervised machine learning: ML systems learn how to combine input to produce useful predictions on never-before-seen data. the fundamental machine learning terminology Key ML Terminology \u00b6 Label \u00b6 It is the thing we\u2019re predicting, the y y variable in a lineal regression. It can be the future price of the wheat, kind of animal shown in a picture, just about anything. Features \u00b6 A Feature is a input variable, the x x in a simple linear regression. A ML project might have several x x depending of the complexity of the project. x_1, x_2,...,x_n x_1, x_2,...,x_n Lets take a spam detector as an example, the features include the following: words in the email text sender\u2019s address time of day the email was sent email contains the phrase \u201cone weird trick\u201d Examples \u00b6 An Example is a particular instance of Data, this data can be represented as x , ( were x can be a vector), we have to categories: Labeled examples Unlabeled examples Labeled Examples \u00b6 labeled examples: {features, labels}: (x,y) use label example to train the model, for the example of the spam detector , the label example will be those mails mark as \u201cspam\u201d or \u201cnot spam\u201d. Unlabeled Examples \u00b6 unlabeled examples: {features, ?}: (x,?) once the model is trained using the label examples, we can use the model to predict the labels of the unlabeled examples. Models \u00b6 It defines the relationship between features and labels. The main phases of the model\u2019s life are: Training: means creating or learning the model, in this phase the label example are use to feed the model, so it will learn the relationship between features and labels. Inference: means apply the trained model to the unlabeled example, in other words, it is use the trained model to make a prediction, if the example are x x the prediction will be y y . Regressions vs Classifications \u00b6 Regression models are used to predict continues values, for example, given a temperature in Celsius what is the equivalent in Fahrenheit, or: What is the value of a house in California? What is the probability that a user click on this ad? Classification models are used to predict discrete values, for example: Is a given email message Spam or not Spam? Is this an image of a dog, a cat or a hamster?","title":"Framing"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Framing.html#key_ml_terminology","text":"","title":"Key ML Terminology"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Framing.html#label","text":"It is the thing we\u2019re predicting, the y y variable in a lineal regression. It can be the future price of the wheat, kind of animal shown in a picture, just about anything.","title":"Label"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Framing.html#features","text":"A Feature is a input variable, the x x in a simple linear regression. A ML project might have several x x depending of the complexity of the project. x_1, x_2,...,x_n x_1, x_2,...,x_n Lets take a spam detector as an example, the features include the following: words in the email text sender\u2019s address time of day the email was sent email contains the phrase \u201cone weird trick\u201d","title":"Features"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Framing.html#examples","text":"An Example is a particular instance of Data, this data can be represented as x , ( were x can be a vector), we have to categories: Labeled examples Unlabeled examples","title":"Examples"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Framing.html#labeled_examples","text":"labeled examples: {features, labels}: (x,y) use label example to train the model, for the example of the spam detector , the label example will be those mails mark as \u201cspam\u201d or \u201cnot spam\u201d.","title":"Labeled Examples"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Framing.html#unlabeled_examples","text":"unlabeled examples: {features, ?}: (x,?) once the model is trained using the label examples, we can use the model to predict the labels of the unlabeled examples.","title":"Unlabeled Examples"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Framing.html#models","text":"It defines the relationship between features and labels. The main phases of the model\u2019s life are: Training: means creating or learning the model, in this phase the label example are use to feed the model, so it will learn the relationship between features and labels. Inference: means apply the trained model to the unlabeled example, in other words, it is use the trained model to make a prediction, if the example are x x the prediction will be y y .","title":"Models"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Framing.html#regressions_vs_classifications","text":"Regression models are used to predict continues values, for example, given a temperature in Celsius what is the equivalent in Fahrenheit, or: What is the value of a house in California? What is the probability that a user click on this ad? Classification models are used to predict discrete values, for example: Is a given email message Spam or not Spam? Is this an image of a dog, a cat or a hamster?","title":"Regressions vs Classifications"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Intro_to_pandas.html","text":"pandas is a column-oriented dataanalysis API Basic Concepts \u00b6 first we need to import it 1 2 3 4 from __future__ import print_function import pandas as pd pd . __version__ The primary data structure in pandas are inmplemented in two classes: DataFrame , similar to a relational data table, row and columns Series , single column, DataFrame contains one or more Series and a name for each Series . for example a Series will be: 1 pd . Series ([ 'San Fransisco' , 'San Jose' , 'Sacramento' ]) And DataFrame can be created similar to dict in this case we will have the key, a String that serve as column name, and the series that will be the content, if we have more than one series and this series dont mach the length, missing values are filled with special NA/NaN 1 2 3 4 city_names = pd . Series ([ 'San Fransisco' , 'San Jose' , 'Sacramento' ]) population = pd . Series ([ 852469 , 1015785 , 485199 ]) pd . DataFrame ({ 'City name' : city_names , 'Population' : population }) we can load information from a file directly to a DataFrame 1 california_housing_dataframe = pd . read . csv ( \"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\" , sep = ',' ) Now to display this information we can use different ways: 1 2 california_housing_dataframe . describe () california_housing_dataframe . head () describe() will display some statistics about this DataFrame, and head() display few of the first records, here is what we see with head() now we can use the information that we load to the DataFrame to create a graph, we are going to use hist() : 1 california_housing_dataframe . hist ( 'housing_median_age' ) Accessing Data \u00b6 To access the data on the DataFrame we can use familiar dict/list operations: 1 2 3 4 5 6 7 8 cities = pd . DataFrame ({ 'City name' : city_names , 'Population' : population }) print ( type ( cities [ 'City name' ])) # <class 'pandas.core.series.Series'> cities [ 'Ciy name' ] # 0 San Francisco # 1 San Jose # 2 Sacramento # Name: City name, dtype: object in the code above we access the entire Series, now we can access a single item of that series 1 2 3 4 print ( type ( cities [ 'city name' ][ 1 ])) # <type 'str'> cities [ 'city name' ][ 1 ] # 'San Jose' or 1 2 3 4 5 6 print ( type ( cities [ 0 : 2 ])) # <class 'pandas.core.frame.DataFrame> cities [ 0 : 2 ] #' City name Population # 0 San Francisco 852469 # 1 San Jose 1015785 Manipulating Data \u00b6 We can use the python arithmetics to interact with the pandas data types 1 population / 1000. we can use series as arguments of a Numpy function 1 2 3 import numpy as np np . log ( population ) In similar to how in python we use map() to perform a transformation of a set of data, we can use Series.apply() and lambda as arguments 1 population . apply ( lambda val : val > 1000000 ) in this case we will get back true or false if the population value of a city is bigger than 1000000 1000000 to modify DataFrames we can do it in a easy way 1 2 3 4 5 6 7 cities [ 'Area square miles' ] = pd . Series ([ 46.87 , 176.53 , 97.92 ]) cities [ 'Population density' ] = cities [ 'Population' ] / cities [ 'Area square miles' ] cities # City name Population Area square miles Population density # 0 San Francisco 852469 46.87 18187.945381 # 1 San Jose 1015785 176.53 5754.177760 # 2 Sacramento 485199 97.92 4955.055147 Indexes \u00b6 Both Series and DataFrame objects also define an index property that assigns an identifier value to each Series item or DataFrame row. By default, at construction, pandas assigns index values that reflect the ordering of the source data. Once created, the index values are stable; that is, they do not change when data is reordered. 1 2 3 4 city_names . index # RangeIndex(start=0, stop=3, step=1) cities . index # RangeIndex(start=0, stop=3, step=1) Call DataFrame.reindex to manually reorder the rows. For example, the following has the same effect as sorting by city name: 1 cities . reindex ([ 2 , 0 , 1 ]) Reindexing is a great way to shuffle (randomize) a DataFrame. In the example below, we take the index, which is array-like, and pass it to NumPy\u2019s random.permutation function, which shuffles its values in place. 1 cities . reindex ( np . random . permutation ( cities . index ))","title":"Introduction to Pandas"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Intro_to_pandas.html#basic_concepts","text":"first we need to import it 1 2 3 4 from __future__ import print_function import pandas as pd pd . __version__ The primary data structure in pandas are inmplemented in two classes: DataFrame , similar to a relational data table, row and columns Series , single column, DataFrame contains one or more Series and a name for each Series . for example a Series will be: 1 pd . Series ([ 'San Fransisco' , 'San Jose' , 'Sacramento' ]) And DataFrame can be created similar to dict in this case we will have the key, a String that serve as column name, and the series that will be the content, if we have more than one series and this series dont mach the length, missing values are filled with special NA/NaN 1 2 3 4 city_names = pd . Series ([ 'San Fransisco' , 'San Jose' , 'Sacramento' ]) population = pd . Series ([ 852469 , 1015785 , 485199 ]) pd . DataFrame ({ 'City name' : city_names , 'Population' : population }) we can load information from a file directly to a DataFrame 1 california_housing_dataframe = pd . read . csv ( \"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\" , sep = ',' ) Now to display this information we can use different ways: 1 2 california_housing_dataframe . describe () california_housing_dataframe . head () describe() will display some statistics about this DataFrame, and head() display few of the first records, here is what we see with head() now we can use the information that we load to the DataFrame to create a graph, we are going to use hist() : 1 california_housing_dataframe . hist ( 'housing_median_age' )","title":"Basic Concepts"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Intro_to_pandas.html#accessing_data","text":"To access the data on the DataFrame we can use familiar dict/list operations: 1 2 3 4 5 6 7 8 cities = pd . DataFrame ({ 'City name' : city_names , 'Population' : population }) print ( type ( cities [ 'City name' ])) # <class 'pandas.core.series.Series'> cities [ 'Ciy name' ] # 0 San Francisco # 1 San Jose # 2 Sacramento # Name: City name, dtype: object in the code above we access the entire Series, now we can access a single item of that series 1 2 3 4 print ( type ( cities [ 'city name' ][ 1 ])) # <type 'str'> cities [ 'city name' ][ 1 ] # 'San Jose' or 1 2 3 4 5 6 print ( type ( cities [ 0 : 2 ])) # <class 'pandas.core.frame.DataFrame> cities [ 0 : 2 ] #' City name Population # 0 San Francisco 852469 # 1 San Jose 1015785","title":"Accessing Data"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Intro_to_pandas.html#manipulating_data","text":"We can use the python arithmetics to interact with the pandas data types 1 population / 1000. we can use series as arguments of a Numpy function 1 2 3 import numpy as np np . log ( population ) In similar to how in python we use map() to perform a transformation of a set of data, we can use Series.apply() and lambda as arguments 1 population . apply ( lambda val : val > 1000000 ) in this case we will get back true or false if the population value of a city is bigger than 1000000 1000000 to modify DataFrames we can do it in a easy way 1 2 3 4 5 6 7 cities [ 'Area square miles' ] = pd . Series ([ 46.87 , 176.53 , 97.92 ]) cities [ 'Population density' ] = cities [ 'Population' ] / cities [ 'Area square miles' ] cities # City name Population Area square miles Population density # 0 San Francisco 852469 46.87 18187.945381 # 1 San Jose 1015785 176.53 5754.177760 # 2 Sacramento 485199 97.92 4955.055147","title":"Manipulating Data"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Intro_to_pandas.html#indexes","text":"Both Series and DataFrame objects also define an index property that assigns an identifier value to each Series item or DataFrame row. By default, at construction, pandas assigns index values that reflect the ordering of the source data. Once created, the index values are stable; that is, they do not change when data is reordered. 1 2 3 4 city_names . index # RangeIndex(start=0, stop=3, step=1) cities . index # RangeIndex(start=0, stop=3, step=1) Call DataFrame.reindex to manually reorder the rows. For example, the following has the same effect as sorting by city name: 1 cities . reindex ([ 2 , 0 , 1 ]) Reindexing is a great way to shuffle (randomize) a DataFrame. In the example below, we take the index, which is array-like, and pass it to NumPy\u2019s random.permutation function, which shuffles its values in place. 1 cities . reindex ( np . random . permutation ( cities . index ))","title":"Indexes"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Linear regression.html","text":"Linear Regression, Training and loss \u00b6 Lineal Regression \u00b6 Linear regression is a method for finding the straight line or hyperplane that best fits a set of points. As an example, we can use the relationship between the temperature and the cripts-per-minutes of crickets As expected, the plot shows the temperature rising with the number of chirps. the relationship between chirps and temperature is linear, you could draw a single straight line like the following to approximate this relationship: the line doesn\u2019t pass through every dot, but the line does clearly show the relationship between chirps and temperature. Using the equation for a line: y = mx + b y = mx + b where: y y is the temperature Celsius - the vale the model need to predict m m is the slope of the line x x is the number of chirp per minute b b is the y-intercept in the field of machine learning the lineal regression formula change to: y' = b_1 + w_1x_1 y' = b_1 + w_1x_1 where: y' y' is the predicted label (the desired output) b b is the bias, the y-intercept (sometimes refer as w_0 w_0 ) w_1 w_1 is the weight of feature 1. x_1 x_1 is a feature (a known input) To infer (predict) the temperature y' y' for a new chirps-per-minute x_1 x_1 value, just substitute the x_1 x_1 value into this model. Although this model uses only one feature, a more advance model might rely on multiple features, each having a separate weight ( w_2 w_2 , w_3 w_3 , etc.). so a model that relies on three features might look as follows: y' = b_1 + w_1x_1 + w_2x_2 + w_3x_3 y' = b_1 + w_1x_1 + w_2x_2 + w_3x_3 Training and Loss \u00b6 Training a model simply means learning good values for all the weights and the bias from labeled examples, in this case we talk about supervised learning in this case we want to find a model that minimize loss, this is call empirical risk minimization . We can see loss like a penalty for a bad prediction, this number indicate how bad the model\u2019s prediction was on a single example. Here: the arrows represent loss. the blue line represent prediction. Squared loss: a popular loss function \u00b6 Most of the Lineal regression models will use the loss function called squared loss ( L_2 loss L_2 loss ), for the example above (the two lineal function side by side) the squared loss will be = (observation - prediction(x))^2 = (observation - prediction(x))^2 = ( y - y' )^2 = ( y - y' )^2 Mean square error (MSE) is the average squared loss per example over the whole dataset. MSE = \\frac{1}{N} \\sum_{(x,y)\\in D} (y - prediction(x))^2 MSE = \\frac{1}{N} \\sum_{(x,y)\\in D} (y - prediction(x))^2 Where: (x,y) (x,y) is an example in which: x x is the set of features that the model used to make prediction y y is the example\u2019s label prediction(x) prediction(x) is a function of the weights and bias in combination with the set of features x x D D is a dataset containing many examples and labels, which are (x,y) (x,y) pairs. N N is the number of examples in D D Important Concepts \u00b6 Bias \u00b6 An intercept or offset from an origin. Bias (also known as the bias term) is referred to as b or w0 in machine learning models. For example, bias is the b in the following formula: y' = b + w_1x_1 + w_2x_2 + ... + w_nx_n y' = b + w_1x_1 + w_2x_2 + ... + w_nx_n Inference \u00b6 In machine learning, often refers to the process of making predictions by applying the trained model to unlabeled examples. In statistics, inference refers to the process of fitting the parameters of a distribution conditioned on some observed data. Weight \u00b6 A coefficient for a feature in a linear model, or an edge in a deep network. The goal of training a linear model is to determine the ideal weight for each feature. If a weight is 0 0 , then its corresponding feature does not contribute to the model. Loss \u00b6 A measure of how far a model\u2019s predictions are from its label . Or, to phrase it more pessimistically, a measure of how bad the model is. To determine this value, a model must define a loss function. For example, linear regression models typically use mean squared error for a loss function, while logistic regression models use Log Loss .","title":"Linear Regression"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Linear regression.html#linear_regression_training_and_loss","text":"","title":"Linear Regression, Training and loss"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Linear regression.html#lineal_regression","text":"Linear regression is a method for finding the straight line or hyperplane that best fits a set of points. As an example, we can use the relationship between the temperature and the cripts-per-minutes of crickets As expected, the plot shows the temperature rising with the number of chirps. the relationship between chirps and temperature is linear, you could draw a single straight line like the following to approximate this relationship: the line doesn\u2019t pass through every dot, but the line does clearly show the relationship between chirps and temperature. Using the equation for a line: y = mx + b y = mx + b where: y y is the temperature Celsius - the vale the model need to predict m m is the slope of the line x x is the number of chirp per minute b b is the y-intercept in the field of machine learning the lineal regression formula change to: y' = b_1 + w_1x_1 y' = b_1 + w_1x_1 where: y' y' is the predicted label (the desired output) b b is the bias, the y-intercept (sometimes refer as w_0 w_0 ) w_1 w_1 is the weight of feature 1. x_1 x_1 is a feature (a known input) To infer (predict) the temperature y' y' for a new chirps-per-minute x_1 x_1 value, just substitute the x_1 x_1 value into this model. Although this model uses only one feature, a more advance model might rely on multiple features, each having a separate weight ( w_2 w_2 , w_3 w_3 , etc.). so a model that relies on three features might look as follows: y' = b_1 + w_1x_1 + w_2x_2 + w_3x_3 y' = b_1 + w_1x_1 + w_2x_2 + w_3x_3","title":"Lineal Regression"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Linear regression.html#training_and_loss","text":"Training a model simply means learning good values for all the weights and the bias from labeled examples, in this case we talk about supervised learning in this case we want to find a model that minimize loss, this is call empirical risk minimization . We can see loss like a penalty for a bad prediction, this number indicate how bad the model\u2019s prediction was on a single example. Here: the arrows represent loss. the blue line represent prediction.","title":"Training and Loss"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Linear regression.html#squared_loss_a_popular_loss_function","text":"Most of the Lineal regression models will use the loss function called squared loss ( L_2 loss L_2 loss ), for the example above (the two lineal function side by side) the squared loss will be = (observation - prediction(x))^2 = (observation - prediction(x))^2 = ( y - y' )^2 = ( y - y' )^2 Mean square error (MSE) is the average squared loss per example over the whole dataset. MSE = \\frac{1}{N} \\sum_{(x,y)\\in D} (y - prediction(x))^2 MSE = \\frac{1}{N} \\sum_{(x,y)\\in D} (y - prediction(x))^2 Where: (x,y) (x,y) is an example in which: x x is the set of features that the model used to make prediction y y is the example\u2019s label prediction(x) prediction(x) is a function of the weights and bias in combination with the set of features x x D D is a dataset containing many examples and labels, which are (x,y) (x,y) pairs. N N is the number of examples in D D","title":"Squared loss: a popular loss function"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Linear regression.html#important_concepts","text":"","title":"Important Concepts"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Linear regression.html#bias","text":"An intercept or offset from an origin. Bias (also known as the bias term) is referred to as b or w0 in machine learning models. For example, bias is the b in the following formula: y' = b + w_1x_1 + w_2x_2 + ... + w_nx_n y' = b + w_1x_1 + w_2x_2 + ... + w_nx_n","title":"Bias"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Linear regression.html#inference","text":"In machine learning, often refers to the process of making predictions by applying the trained model to unlabeled examples. In statistics, inference refers to the process of fitting the parameters of a distribution conditioned on some observed data.","title":"Inference"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Linear regression.html#weight","text":"A coefficient for a feature in a linear model, or an edge in a deep network. The goal of training a linear model is to determine the ideal weight for each feature. If a weight is 0 0 , then its corresponding feature does not contribute to the model.","title":"Weight"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Linear regression.html#loss","text":"A measure of how far a model\u2019s predictions are from its label . Or, to phrase it more pessimistically, a measure of how bad the model is. To determine this value, a model must define a loss function. For example, linear regression models typically use mean squared error for a loss function, while logistic regression models use Log Loss .","title":"Loss"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Reducing_Loss.html","text":"Previously, we mentioned the concept of loss , now we are going to see how ML model iteratively reduce the loss. the figure suggest the iterative trial-and-error process that machine learning algorithms use to train a model: This iterative approach is use in ML, and basically will work as a Trial and Error, the machine will guess a number, it will calculate the loss and will make adjustment to the wight and bias accordingly. This iterative strategy prevalent in machine learning, primary because they scale so well to large data sets. The \u201cmodel\u201d take one or more feature as input and return one prediction y' y' as output: y' = b + w_1x_1 y' = b + w_1x_1 For lineal regression problems the values to start are not important, in this example we can pick, b = 0 b = 0 , w_1 = 0 w_1 = 0 , and assuming the first feature value is 10, the prediction yields: y' = 0 + 0(10) y' = 0 y' = 0 + 0(10) y' = 0 So y' y' will be the prediction. The \u201cCompute Loss\u201d part of the diagram is the loss function that the model use. Suppose we use the squared loss function. The loss function takes in two inputs values: y' y' : The model\u2019s prediction for feature x. y y : The correct label corresponding to feature x. Finally the last part of the diagram the \u201cCompute parameter updates\u201d is here where the ML system examines the value of the loss function and generate new values for b b and w_1 w_1 , for the moment less assume that the job of this box is to provide new values for the next iteration, and the learning continue iterating until the algorithm discovers the model parameters with the lowest possible loss. Usually, we will iterate until the loss stop changing or at least changes extremely slowly. when that happen, we say that the model has converged . A Machine Learning model is trained by starting with an initial guess for the weights and bias and iteratively adjusting those guesses until learning the weights and bias with the lowest possible loss. Gradient Descent \u00b6 in the first diagram we have a box called \u201cCompute parameter updates\u201d, lets suppose we have the time to calculate all the possible values of w_1 w_1 for the type of regression exercises mentioned until now, the resulting plot of loss vs w_1 w_1 will be: For this type of bowl-shape problems is easier to find the minimum loss, in order to make efficient the process we can use a mechanism called gradient descent instead of calculate the loss function for every single w_1 w_1 point. The first step will be to pick a starting value, the starting value is not important at this point, then the algorithm calculate the gradient of the loss at that point, the result will be a derivative(slope) of the curve that will tell you how close or far is this point from the minimum loss point (or the converge point). The gradient is a vector, thus has \u201ca direction\u201d, and \u201d a Magnitude\u201d, the gradient points in the direction where the loss increase, so the gradient descent algorithm points to the direction where the loss is reduce. To determine the next point along the loss function curve, the gradient descent algorithm adds some fractions of the gradient\u2019s magnitude to the stating point. Learning Rate \u00b6 The Gradient descent algorithm multiply the gradient by a scalar knows as the Learning Rate or Step size to determinate the next point, For example, if the gradient magnitude is 2.5 and the learning rate is 0.01, then the gradient descent algorithm will pick the next point 0.025 away from the previous point. it is important to find the right Learning rate to don\u2019t overshot the minimum or a so small learning rate that will take for ever reach the converge point. Stochastic Gradient Descent \u00b6 a batch is the total number of examples you use to calculate the gradient in a single iteration, A large data set with randomly sampled examples probably contains redundant data. In fact, redundancy becomes more likely as the batch size grows. Some redundancy can be useful to smooth out noisy gradients, but enormous batches tend not to carry much more predictive value than large batches. By choosing examples at random from our data set, we could estimate a big average from a much smaller batch Stochastic gradient descent ( SGD ) takes this idea to the extreme\u2013it uses only a single example (a batch size of 1) per iteration. Given enough iterations, SGD works but is very noisy. The term \u201cstochastic\u201d indicates that the one example comprising each batch is chosen at random. Mini-batch stochastic gradient descent ( mini-batch SGD ) is a compromise between full-batch iteration and SGD. A mini-batch is typically between 10 and 1,000 examples, chosen at random. Mini-batch SGD reduces the amount of noise in SGD but is still more efficient than full-batch.","title":"Reducing Loss"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Reducing_Loss.html#gradient_descent","text":"in the first diagram we have a box called \u201cCompute parameter updates\u201d, lets suppose we have the time to calculate all the possible values of w_1 w_1 for the type of regression exercises mentioned until now, the resulting plot of loss vs w_1 w_1 will be: For this type of bowl-shape problems is easier to find the minimum loss, in order to make efficient the process we can use a mechanism called gradient descent instead of calculate the loss function for every single w_1 w_1 point. The first step will be to pick a starting value, the starting value is not important at this point, then the algorithm calculate the gradient of the loss at that point, the result will be a derivative(slope) of the curve that will tell you how close or far is this point from the minimum loss point (or the converge point). The gradient is a vector, thus has \u201ca direction\u201d, and \u201d a Magnitude\u201d, the gradient points in the direction where the loss increase, so the gradient descent algorithm points to the direction where the loss is reduce. To determine the next point along the loss function curve, the gradient descent algorithm adds some fractions of the gradient\u2019s magnitude to the stating point.","title":"Gradient Descent"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Reducing_Loss.html#learning_rate","text":"The Gradient descent algorithm multiply the gradient by a scalar knows as the Learning Rate or Step size to determinate the next point, For example, if the gradient magnitude is 2.5 and the learning rate is 0.01, then the gradient descent algorithm will pick the next point 0.025 away from the previous point. it is important to find the right Learning rate to don\u2019t overshot the minimum or a so small learning rate that will take for ever reach the converge point.","title":"Learning Rate"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Reducing_Loss.html#stochastic_gradient_descent","text":"a batch is the total number of examples you use to calculate the gradient in a single iteration, A large data set with randomly sampled examples probably contains redundant data. In fact, redundancy becomes more likely as the batch size grows. Some redundancy can be useful to smooth out noisy gradients, but enormous batches tend not to carry much more predictive value than large batches. By choosing examples at random from our data set, we could estimate a big average from a much smaller batch Stochastic gradient descent ( SGD ) takes this idea to the extreme\u2013it uses only a single example (a batch size of 1) per iteration. Given enough iterations, SGD works but is very noisy. The term \u201cstochastic\u201d indicates that the one example comprising each batch is chosen at random. Mini-batch stochastic gradient descent ( mini-batch SGD ) is a compromise between full-batch iteration and SGD. A mini-batch is typically between 10 and 1,000 examples, chosen at random. Mini-batch SGD reduces the amount of noise in SGD but is still more efficient than full-batch.","title":"Stochastic Gradient Descent"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Training_Test_sets.html","text":"In most of the ML project we will have to deal with datasets, this dataset in an ideal situation will be divided or split in: training set : a subset to train a model test set : a subset to test the trained model It is important to be sure that the test set meet the following criteria: Is large enough to wield statistically meaningful results. Is representative of the data set as a whole. do not pick a test set with different characteristics than the training set. Important to know that one never train on test data, otherwise we will get close to 100% accuracy or surprisingly good results, that are not totally true. Overfitting \u00b6 Creating a model that matches the training data so closely that the model fails to make a correct predictions on new data. Training set \u00b6 The subset pf the dataset used to train a model Test set \u00b6 the subset of the dataset use to test your model after the model has gone through initial vetting by the validation set. Validation set \u00b6 A subset of the dataset -disjoin from the training g set- used in validation An approach of Training data and test data is certainly a good idea, although not the best in some situations, It will be better if the data can be spitted in 3 different sets, the Training set, Validation Set and Test set. Using the validation set to evaluate results from the training set. Then use the test set to double-check your evaluation after the model has \u201cpassed\u201d the validation set will be a more efficient approach. few things to take in count in this model: Pick the model that does best on the validation set Double-check that model against the test set.","title":"Training and Test Sets"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Training_Test_sets.html#overfitting","text":"Creating a model that matches the training data so closely that the model fails to make a correct predictions on new data.","title":"Overfitting"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Training_Test_sets.html#training_set","text":"The subset pf the dataset used to train a model","title":"Training set"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Training_Test_sets.html#test_set","text":"the subset of the dataset use to test your model after the model has gone through initial vetting by the validation set.","title":"Test set"},{"location":"Machine Learning/Google Crash Course/ML Concepsts/Training_Test_sets.html#validation_set","text":"A subset of the dataset -disjoin from the training g set- used in validation An approach of Training data and test data is certainly a good idea, although not the best in some situations, It will be better if the data can be spitted in 3 different sets, the Training set, Validation Set and Test set. Using the validation set to evaluate results from the training set. Then use the test set to double-check your evaluation after the model has \u201cpassed\u201d the validation set will be a more efficient approach. few things to take in count in this model: Pick the model that does best on the validation set Double-check that model against the test set.","title":"Validation set"},{"location":"Machine Learning/Scripts or Projects/Twitter_sentiment_Analyzer.html","text":"There are two sources for this project: Twitter Sentiment Analysis - Learn Python for Data Science #2 by Siraj Raval How to build a twitter sentiment Analyzer using TextBlob In this case we are going to use TextBlob Now, what is sentiment analysis, it is the process of determining whether a piece of writing is positive, negative or neutral, there are several ways to do this sentiment analysis, the two more common approaches will be: Lexicon-base model Machine Learning-based Method In a really shallow description/definition, Lexicon is a method that will list the worlds as positive and negative, e.g \u2018nice\u2019,+2, \u2018good\u2019,+1,\u2019terrible\u2019,-1.5), The algorithm will find all the words and combine individual results and provide a result base in that value. Twitter API \u00b6 First, we will need to register the app in twitter in order to get the various keys associated with the API, there will be 4 keys: consumer_key consumer_key_secret access_token access_token_secret In order to continue we will need to install two packages, tweepy and textblob , we can do this using pip, 1 pip install tweepy with this package we will handle the Twitter API 1 pip install textblob and with this will make the sentiment analysis. Once we have the packages we can start the code, first we need to import the packages 1 2 import tweepy from textblob import TextBlob now, we assigned the keys to variable to pass it later in the code 1 2 3 4 consumer_key = \u2018 [ consumer_key ] \u2019 consumer_key_secret = \u2018 [ consumer_key_secret ] \u2019 access_token = \u2018 [ access_token ] \u2019 access_token_secret = \u2018 [ access_token_secret ] \u2019 Tweepy \u00b6 Tweepy support OAuth authentication, this is handle by the class tweepy.OAuthHandler , An instance of OAuthHandler must be created passing the consumer token and the secret. Next, on this instance, we will call a function set_access_token by passing the access_token and access_token_secret . Finally we create and instance of the api with the tweepy function API() 1 2 3 auth = tweepy . OAuthHandler ( consumer_key , consumer_key_secret ) auth . set_access_token ( access_token , access_token_secret ) api = tweepy . API ( auth ) the next step will be to find the public tweets related to a topic, in this case we use the method search of the API. 1 public_tweet = api . search ( 'Dogs' ) Sentiment Analysis \u00b6 TextBlob \u00b6 TextBlob is a library Natural Language Processing (NLP). Sentiment analysis \u00b6 As a result of the sentiment analysis we will receive a tuple (polarity, subjectivity). The polarity score is a float within a range of [-1.0,1.0]. The subjectivity is a float within the range [0.0,1.0] where 0.0 is very objective and 1.0 is very subjective. so now we are going to check each tweeter in our public_tweets variable 1 2 3 4 5 6 7 8 9 10 for tweet in public_tweets : print ( tweet . text ) analysis = TextBlob ( tweet . text ) print ( analysis . sentiment ) if analysis . sentiment [ 0 ] > 0 : print ( 'positive' ) elif analysis . sentiment [ 0 ] < 0 : print ( 'Negative' ) else : print ( 'Neutral' ) here the complite script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import tweepy from textblob import TextBlob consumer_key = 'xcrKGTL1FWhBB5si7xNrxpxVL' # this is a place holder not the real key consumer_key_secret = '8WrC0ukU0ejs9E5eblGlCtJTrLVMsLeXwj7cZCSMqiL3ke67SP' # this is a place holder not the real key access_token = '90495695-UOx9C6Zl1l2U2xFjw2LitDLOXu6aylWwf0p4UDQDA' # this is a place holder not the real key access_token_secret = 'WXFqNV5Da8hwwh5oIfUXhsBKb9ouvE6gQCZ3fIgQLtdm2' # this is a place holder not the real key # set the OAuth authentication auth = tweepy . OAuthHandler ( consumer_key , consumer_key_secret ) auth . set_access_token ( access_token , access_token_secret ) api = tweepy . API ( auth ) public_tweets = api . search ( 'Taiwan' ) for tweet in public_tweets : print ( tweet . text ) analysis = TextBlob ( tweet . text ) print ( analysis . sentiment ) if analysis . sentiment [ 0 ] > 0 : print ( 'Positive' ) elif analysis . sentiment [ 0 ] < 0 : print ( 'Negative' ) else : print ( 'Neutral' )","title":"Twitter Sentiment Analyzer"},{"location":"Machine Learning/Scripts or Projects/Twitter_sentiment_Analyzer.html#twitter_api","text":"First, we will need to register the app in twitter in order to get the various keys associated with the API, there will be 4 keys: consumer_key consumer_key_secret access_token access_token_secret In order to continue we will need to install two packages, tweepy and textblob , we can do this using pip, 1 pip install tweepy with this package we will handle the Twitter API 1 pip install textblob and with this will make the sentiment analysis. Once we have the packages we can start the code, first we need to import the packages 1 2 import tweepy from textblob import TextBlob now, we assigned the keys to variable to pass it later in the code 1 2 3 4 consumer_key = \u2018 [ consumer_key ] \u2019 consumer_key_secret = \u2018 [ consumer_key_secret ] \u2019 access_token = \u2018 [ access_token ] \u2019 access_token_secret = \u2018 [ access_token_secret ] \u2019","title":"Twitter API"},{"location":"Machine Learning/Scripts or Projects/Twitter_sentiment_Analyzer.html#tweepy","text":"Tweepy support OAuth authentication, this is handle by the class tweepy.OAuthHandler , An instance of OAuthHandler must be created passing the consumer token and the secret. Next, on this instance, we will call a function set_access_token by passing the access_token and access_token_secret . Finally we create and instance of the api with the tweepy function API() 1 2 3 auth = tweepy . OAuthHandler ( consumer_key , consumer_key_secret ) auth . set_access_token ( access_token , access_token_secret ) api = tweepy . API ( auth ) the next step will be to find the public tweets related to a topic, in this case we use the method search of the API. 1 public_tweet = api . search ( 'Dogs' )","title":"Tweepy"},{"location":"Machine Learning/Scripts or Projects/Twitter_sentiment_Analyzer.html#sentiment_analysis","text":"","title":"Sentiment Analysis"},{"location":"Machine Learning/Scripts or Projects/Twitter_sentiment_Analyzer.html#textblob","text":"TextBlob is a library Natural Language Processing (NLP).","title":"TextBlob"},{"location":"Machine Learning/Scripts or Projects/Twitter_sentiment_Analyzer.html#sentiment_analysis_1","text":"As a result of the sentiment analysis we will receive a tuple (polarity, subjectivity). The polarity score is a float within a range of [-1.0,1.0]. The subjectivity is a float within the range [0.0,1.0] where 0.0 is very objective and 1.0 is very subjective. so now we are going to check each tweeter in our public_tweets variable 1 2 3 4 5 6 7 8 9 10 for tweet in public_tweets : print ( tweet . text ) analysis = TextBlob ( tweet . text ) print ( analysis . sentiment ) if analysis . sentiment [ 0 ] > 0 : print ( 'positive' ) elif analysis . sentiment [ 0 ] < 0 : print ( 'Negative' ) else : print ( 'Neutral' ) here the complite script: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import tweepy from textblob import TextBlob consumer_key = 'xcrKGTL1FWhBB5si7xNrxpxVL' # this is a place holder not the real key consumer_key_secret = '8WrC0ukU0ejs9E5eblGlCtJTrLVMsLeXwj7cZCSMqiL3ke67SP' # this is a place holder not the real key access_token = '90495695-UOx9C6Zl1l2U2xFjw2LitDLOXu6aylWwf0p4UDQDA' # this is a place holder not the real key access_token_secret = 'WXFqNV5Da8hwwh5oIfUXhsBKb9ouvE6gQCZ3fIgQLtdm2' # this is a place holder not the real key # set the OAuth authentication auth = tweepy . OAuthHandler ( consumer_key , consumer_key_secret ) auth . set_access_token ( access_token , access_token_secret ) api = tweepy . API ( auth ) public_tweets = api . search ( 'Taiwan' ) for tweet in public_tweets : print ( tweet . text ) analysis = TextBlob ( tweet . text ) print ( analysis . sentiment ) if analysis . sentiment [ 0 ] > 0 : print ( 'Positive' ) elif analysis . sentiment [ 0 ] < 0 : print ( 'Negative' ) else : print ( 'Neutral' )","title":"Sentiment analysis"},{"location":"Miscellaneous/Aliases_in_Windows_command_prompt.html","text":"you may make the alias(es) persistent with the following steps: Create a .bat or .cmd file with your DOSKEY commands. Run regedit and go to HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor Add String Value entry with the name AutoRun and the full path of your .bat/.cmd file. For example, %USERPROFILE%\\alias.cmd , replacing the initial segment of the path with %USERPROFILE% is useful for syncing among multiple machines. This way, every time cmd is run, the aliases are loaded. For completeness, here is a template to illustrate the kind of aliases one may find useful. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @ echo off : : Temporary system path at cmd startup : : set PATH=%PATH%;\"C:\\Program Files\\Sublime Text 2\\\" : : Add to path by command : : DOSKEY add_python26=set PATH=%PATH%;\"C:\\Python26\\\" : : DOSKEY add_python33=set PATH=%PATH%;\"C:\\Python33\\\" : : Commands DOSKEY ls=dir /B DOSKEY notes = cd Documents/Notes/notes : : DOSKEY sublime=sublime_text $* : :sublime_text.exe is name of the executable. By adding a temporary entry to system path, we don't have to write the whole directory anymore. : : DOSKEY gsp=\"C:\\Program Files (x86)\\Sketchpad5\\GSP505en.exe\" : :DOSKEY alias=notepad %USERPROFILE%\\Dropbox\\alias.cmd : : Common directories : :DOSKEY dropbox=cd \"%USERPROFILE%\\Dropbox\\$*\" : :DOSKEY research=cd %USERPROFILE%\\Dropbox\\Research\\","title":"Aliases in Windows command prompt"},{"location":"Miscellaneous/How_install_openCV_raspberry.html","text":"There are 7 steps in this installation, and a small project that include the raspberry camera, we will expand the file system and free some space removing some programs that we are not going to use. Step 1: Expanding filesystem on the Raspberry Pi \u00b6 The first thing will be expand the filesystem on the raspberry to make use of all the available space in the micro-sd card: 1 $ sudo raspi-config we are going to access to the configuration, after the command a menu will be display and we can select the option \u201cAdvance Options\u201d next, we select the option \u201cExpand filesystem\u201d once we select we option we can go t and reboot the unit 1 $ sudo reboot after the reboot the available space will expand, this can be verify using the command df -h This might not be enough, therefore we will proceed to remove libreOffice and Wolfram engine to make more room. 1 2 3 4 $ sudo apt-get purge wolfram-engine $ sudo apt-get purge libreoffice* $ sudo apt-get clean $ sudo apt-get autoremove you can use dpkg-query -l to list all the programs installed, we can remove the following: * wolfram-engine. * bluej. * greenfoot. * nodered. * nuscratch. * scratch. * sonic-pi. * libreoffice. * claws-mail. * claws-mail-i18n. * minecraft-pi. * python-pygame. Step 2: Installing OpenCV 4 dependencies on Raspberry pi \u00b6 We start by updating and upgrading the system 1 $ sudo apt-get update && sudo apt-get upgrade this process might take some time after this, we proceed to include the developer tools on CMake 1 $ sudo apt-get install build-essential cmake unzip pkg-config Next, we are going to install libraries to work with videos and images 1 2 3 $ sudo apt-get install libjpeg-dev libpng-dev libtiff-dev $ sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev $ sudo apt-get install libxvidcore-dev libx264-dev The next step will be Install GTK and GUI backend and install a package whihc reduce the GTK warnings 1 2 $ sudo apt-get install libgtk-3-dev $ sudo apt-get install libcanberra-gtk* now two more packages, one for numerical optimization other for python development headers 1 2 $ sudo apt-get install libatlas-base-dev gfortran $ sudo apt-get install python3-dev Step 3: Download OpenCV 4 for Raspberry pi \u00b6 There are two things to download, the opencv and the opencv_contrib , we are going to download them, unzip them and later rename the directories 1 2 3 $ cd ~ $ wget -O opencv.zip https://github.com/opencv/opencv/archive/4.0.0.zip $ wget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/4.0.0.zip Resource","title":"How to Install OpenCv on Raspberry pi"},{"location":"Miscellaneous/How_install_openCV_raspberry.html#step_1_expanding_filesystem_on_the_raspberry_pi","text":"The first thing will be expand the filesystem on the raspberry to make use of all the available space in the micro-sd card: 1 $ sudo raspi-config we are going to access to the configuration, after the command a menu will be display and we can select the option \u201cAdvance Options\u201d next, we select the option \u201cExpand filesystem\u201d once we select we option we can go t and reboot the unit 1 $ sudo reboot after the reboot the available space will expand, this can be verify using the command df -h This might not be enough, therefore we will proceed to remove libreOffice and Wolfram engine to make more room. 1 2 3 4 $ sudo apt-get purge wolfram-engine $ sudo apt-get purge libreoffice* $ sudo apt-get clean $ sudo apt-get autoremove you can use dpkg-query -l to list all the programs installed, we can remove the following: * wolfram-engine. * bluej. * greenfoot. * nodered. * nuscratch. * scratch. * sonic-pi. * libreoffice. * claws-mail. * claws-mail-i18n. * minecraft-pi. * python-pygame.","title":"Step 1: Expanding filesystem on the Raspberry Pi"},{"location":"Miscellaneous/How_install_openCV_raspberry.html#step_2_installing_opencv_4_dependencies_on_raspberry_pi","text":"We start by updating and upgrading the system 1 $ sudo apt-get update && sudo apt-get upgrade this process might take some time after this, we proceed to include the developer tools on CMake 1 $ sudo apt-get install build-essential cmake unzip pkg-config Next, we are going to install libraries to work with videos and images 1 2 3 $ sudo apt-get install libjpeg-dev libpng-dev libtiff-dev $ sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev $ sudo apt-get install libxvidcore-dev libx264-dev The next step will be Install GTK and GUI backend and install a package whihc reduce the GTK warnings 1 2 $ sudo apt-get install libgtk-3-dev $ sudo apt-get install libcanberra-gtk* now two more packages, one for numerical optimization other for python development headers 1 2 $ sudo apt-get install libatlas-base-dev gfortran $ sudo apt-get install python3-dev","title":"Step 2: Installing OpenCV 4 dependencies on Raspberry pi"},{"location":"Miscellaneous/How_install_openCV_raspberry.html#step_3_download_opencv_4_for_raspberry_pi","text":"There are two things to download, the opencv and the opencv_contrib , we are going to download them, unzip them and later rename the directories 1 2 3 $ cd ~ $ wget -O opencv.zip https://github.com/opencv/opencv/archive/4.0.0.zip $ wget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/4.0.0.zip Resource","title":"Step 3: Download OpenCV 4 for Raspberry pi"},{"location":"Miscellaneous/OAuth_2.html","text":"OAuth 2 \u00b6 TODO: Introduction and a better description Now and example of how the OAuth 2 interation look like: Get (ussually) Manually client_id client_secret Sent User over OAuth Client: GET: provider.com/oauth/authorize? client_id redirect_uri Web ask for permission yes yes or no no Re-direct to the uri: GET: clietn.com/oauth_accept? code: foobahz Issue access token POST: provider.com/oauth/access_token code: foobahz client_id: foo client_secret: bar Get access token Result: Result: {'access_token' : 'bazfoobahz'} Can do alll user will do GET: /user/me/friends access_token: bazfoobahz Response Result: Result: {'friends': [{'name': ....}]}","title":"OAuth 2"},{"location":"Miscellaneous/OAuth_2.html#oauth_2","text":"TODO: Introduction and a better description Now and example of how the OAuth 2 interation look like: Get (ussually) Manually client_id client_secret Sent User over OAuth Client: GET: provider.com/oauth/authorize? client_id redirect_uri Web ask for permission yes yes or no no Re-direct to the uri: GET: clietn.com/oauth_accept? code: foobahz Issue access token POST: provider.com/oauth/access_token code: foobahz client_id: foo client_secret: bar Get access token Result: Result: {'access_token' : 'bazfoobahz'} Can do alll user will do GET: /user/me/friends access_token: bazfoobahz Response Result: Result: {'friends': [{'name': ....}]}","title":"OAuth 2"},{"location":"Miscellaneous/Random Link.html","text":"link to the tutorial of fader Lightroom Quick Tips - The Fader lightroom Preset Django tutorial Django documentation","title":"Random"},{"location":"Miscellaneous/potential activities.html","text":"Potential activities Outside Taipei \u00b6 Paragliding Flight Experience - 2 days course The Adventure of River Trekking in Hualien Taipei Scuba Diving Experience at Longdong Bay Tree Climbing Experience at Sky Peak Valley Taiwan Wuyanjiao Kayak Experience Penghu Taiwan Parasailing Experience Driving ATV at South bay beach, Kenting Potential activities Inside Taipei \u00b6 Taipei The Shu\u2019s Pottery DIY Class (Hubox) Handmade vintage LOFT lamp Potential activities Overseas \u00b6","title":"Potential activities"},{"location":"Miscellaneous/potential activities.html#potential_activities_outside_taipei","text":"Paragliding Flight Experience - 2 days course The Adventure of River Trekking in Hualien Taipei Scuba Diving Experience at Longdong Bay Tree Climbing Experience at Sky Peak Valley Taiwan Wuyanjiao Kayak Experience Penghu Taiwan Parasailing Experience Driving ATV at South bay beach, Kenting","title":"Potential activities Outside Taipei"},{"location":"Miscellaneous/potential activities.html#potential_activities_inside_taipei","text":"Taipei The Shu\u2019s Pottery DIY Class (Hubox) Handmade vintage LOFT lamp","title":"Potential activities Inside Taipei"},{"location":"Miscellaneous/potential activities.html#potential_activities_overseas","text":"","title":"Potential activities Overseas"},{"location":"Miscellaneous/setting_openmediavault.html","text":"This is the Guide to install and sett up the Open Media vault on the Raspberry pi ( in this case raspberry pi 3B) From the Open media vault website What is openmediavault? openmediavault is the next generation network attached storage (NAS) solution based on Debian Linux. It contains services like SSH, (S)FTP, SMB/CIFS, DAAP media server, RSync, BitTorrent client and many more. Thanks to the modular design of the framework it can be enhanced via plugins. Download Image \u00b6 In this case we need to download the images for the Raspberry We can use balenaEtcher to flash the image, after this we can put it in the RAspberry pi and boot it up, the next steps i did it connecting a keyboard to the Raspberry and a screen, this might be possible connecting to the raspberry by SSH but i didn\u2019t try. Configuration \u00b6 Let the raspberry boot up. Connect using the default user and password, this will be root and openmediavault . Set a new password. Run reboot Once rebooted run sudo /etc/network/interfaces In this document add this line at the end dns-nameservers 8.8.4.4 8.8.8.8 Run reboot Run sudo apt-get update Run sudo apt-get upgrade After it finish, run reboot Run omv-firstaid Previous command will display a menu. Select from the menu clean apt . Select Clear local upload package repository . Select Clear web control panel cache . Run reboot Run omv-firstaid Select Configure network interface , select or just go OK\u2026OK until finish. Run omv-firstaid Select Configure web control panel the default will be 80. Run sudo apt-get update Run sudo apt-get upgrade Now we can access to the web configuration using the IP address mention in the raspberry screen, the user and password will be admin and openmediavault source: configure OMV on raspberry","title":"Setting Open Media Vault"},{"location":"Miscellaneous/setting_openmediavault.html#download_image","text":"In this case we need to download the images for the Raspberry We can use balenaEtcher to flash the image, after this we can put it in the RAspberry pi and boot it up, the next steps i did it connecting a keyboard to the Raspberry and a screen, this might be possible connecting to the raspberry by SSH but i didn\u2019t try.","title":"Download Image"},{"location":"Miscellaneous/setting_openmediavault.html#configuration","text":"Let the raspberry boot up. Connect using the default user and password, this will be root and openmediavault . Set a new password. Run reboot Once rebooted run sudo /etc/network/interfaces In this document add this line at the end dns-nameservers 8.8.4.4 8.8.8.8 Run reboot Run sudo apt-get update Run sudo apt-get upgrade After it finish, run reboot Run omv-firstaid Previous command will display a menu. Select from the menu clean apt . Select Clear local upload package repository . Select Clear web control panel cache . Run reboot Run omv-firstaid Select Configure network interface , select or just go OK\u2026OK until finish. Run omv-firstaid Select Configure web control panel the default will be 80. Run sudo apt-get update Run sudo apt-get upgrade Now we can access to the web configuration using the IP address mention in the raspberry screen, the user and password will be admin and openmediavault source: configure OMV on raspberry","title":"Configuration"}]}